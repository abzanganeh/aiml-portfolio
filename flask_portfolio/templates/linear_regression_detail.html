{% extends "tutorial_detail.html" %}

{% block tutorial_content %}
<div class="tutorial-section">
    <h2>Introduction to Linear Regression</h2>
    <p>
        Linear regression is a fundamental statistical method used to model the relationship between a 
        dependent variable and one or more independent variables. It assumes a linear relationship and 
        is widely used for prediction, inference, and understanding variable relationships.
    </p>
    
    <h3>Types of Linear Regression</h3>
    <ul>
        <li><strong>Simple Linear Regression:</strong> One independent variable</li>
        <li><strong>Multiple Linear Regression:</strong> Multiple independent variables</li>
        <li><strong>Polynomial Regression:</strong> Non-linear relationships using polynomial terms</li>
        <li><strong>Ridge Regression:</strong> L2 regularization to prevent overfitting</li>
        <li><strong>Lasso Regression:</strong> L1 regularization for feature selection</li>
    </ul>
</div>

<div class="tutorial-section">
    <h2>Mathematical Foundation</h2>
    
    <h3>Simple Linear Regression</h3>
    <div class="formula-box">
        y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œµ
    </div>
    <p>Where:</p>
    <ul>
        <li><strong>y:</strong> Dependent variable (target)</li>
        <li><strong>x:</strong> Independent variable (feature)</li>
        <li><strong>Œ≤‚ÇÄ:</strong> Intercept (y-intercept)</li>
        <li><strong>Œ≤‚ÇÅ:</strong> Slope coefficient</li>
        <li><strong>Œµ:</strong> Error term</li>
    </ul>

    <h3>Multiple Linear Regression</h3>
    <div class="formula-box">
        y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô + Œµ
    </div>

    <h3>Matrix Form</h3>
    <div class="formula-box">
        Y = XŒ≤ + Œµ<br>
        Œ≤ = (X'X)‚Åª¬πX'Y (Normal Equation)
    </div>
</div>

<div class="tutorial-section">
    <h2>Model Assumptions</h2>
    <ul>
        <li><strong>Linearity:</strong> Linear relationship between variables</li>
        <li><strong>Independence:</strong> Observations are independent</li>
        <li><strong>Homoscedasticity:</strong> Constant variance of residuals</li>
        <li><strong>Normality:</strong> Residuals are normally distributed</li>
        <li><strong>No Multicollinearity:</strong> Independent variables are not highly correlated</li>
    </ul>
</div>

<div class="tutorial-section">
    <h2>Implementation Example</h2>
    <p>Here's a linear regression implementation using scikit-learn:</p>
    
    <div class="code-example">import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Prepare data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features (important for regularized regression)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Linear Regression
lr_model = LinearRegression()
lr_model.fit(X_train_scaled, y_train)

# Ridge Regression (L2 regularization)
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train_scaled, y_train)

# Lasso Regression (L1 regularization)
lasso_model = Lasso(alpha=1.0)
lasso_model.fit(X_train_scaled, y_train)

# Make predictions
y_pred_lr = lr_model.predict(X_test_scaled)
y_pred_ridge = ridge_model.predict(X_test_scaled)
y_pred_lasso = lasso_model.predict(X_test_scaled)

# Evaluate models
print("Linear Regression:")
print(f"MSE: {mean_squared_error(y_test, y_pred_lr):.4f}")
print(f"R¬≤: {r2_score(y_test, y_pred_lr):.4f}")

print("\nRidge Regression:")
print(f"MSE: {mean_squared_error(y_test, y_pred_ridge):.4f}")
print(f"R¬≤: {r2_score(y_test, y_pred_ridge):.4f}")

print("\nLasso Regression:")
print(f"MSE: {mean_squared_error(y_test, y_pred_lasso):.4f}")
print(f"R¬≤: {r2_score(y_test, y_pred_lasso):.4f}")</div>
</div>

<div class="tutorial-section">
    <h2>Model Evaluation</h2>
    
    <h3>Common Metrics</h3>
    <ul>
        <li><strong>R¬≤ (Coefficient of Determination):</strong> Proportion of variance explained</li>
        <li><strong>MSE (Mean Squared Error):</strong> Average squared differences</li>
        <li><strong>RMSE (Root Mean Squared Error):</strong> Square root of MSE</li>
        <li><strong>MAE (Mean Absolute Error):</strong> Average absolute differences</li>
    </ul>

    <div class="formula-box">
        R¬≤ = 1 - (SSres / SStot)<br>
        MSE = Œ£(y·µ¢ - ≈∑·µ¢)¬≤ / n<br>
        RMSE = ‚àöMSE<br>
        MAE = Œ£|y·µ¢ - ≈∑·µ¢| / n
    </div>

    <h3>Residual Analysis</h3>
    <ul>
        <li>Plot residuals vs fitted values</li>
        <li>Check for patterns (non-linearity, heteroscedasticity)</li>
        <li>Q-Q plot for normality of residuals</li>
        <li>Cook's distance for influential observations</li>
    </ul>
</div>

<div class="tutorial-section">
    <h2>Regularization Techniques</h2>
    
    <h3>Ridge Regression (L2)</h3>
    <div class="formula-box">
        Cost = MSE + Œ± √ó Œ£Œ≤·µ¢¬≤
    </div>
    <p>Shrinks coefficients but doesn't eliminate them. Good when all features are relevant.</p>

    <h3>Lasso Regression (L1)</h3>
    <div class="formula-box">
        Cost = MSE + Œ± √ó Œ£|Œ≤·µ¢|
    </div>
    <p>Can eliminate features by setting coefficients to zero. Performs automatic feature selection.</p>

    <h3>Elastic Net</h3>
    <div class="formula-box">
        Cost = MSE + Œ±‚ÇÅ √ó Œ£|Œ≤·µ¢| + Œ±‚ÇÇ √ó Œ£Œ≤·µ¢¬≤
    </div>
    <p>Combines both L1 and L2 regularization. Balances feature selection and coefficient shrinkage.</p>
</div>

<div class="tutorial-section">
    <h2>üöÄ Next Steps</h2>
    <div class="example-box">
        <h3>Continue Your Learning Journey</h3>
        <ul>
            <li><strong>Practice Implementation:</strong> Try different regression techniques on real datasets</li>
            <li><strong>Feature Engineering:</strong> Learn advanced preprocessing techniques</li>
            <li><strong>Model Selection:</strong> Study cross-validation and hyperparameter tuning</li>
            <li><strong>Advanced Topics:</strong> Explore polynomial features and interaction terms</li>
        </ul>
        
        <div style="margin-top: 1.5rem;">
            <a href="{{ url_for('tutorial_detail', slug='clustering') }}" class="btn btn-primary" style="margin-right: 1rem;">Next: Clustering</a>
            <a href="{{ url_for('tutorials_list') }}" class="btn btn-secondary">All Tutorials</a>
        </div>
    </div>
</div>
{% endblock %}
