{% extends "tutorial_detail.html" %}

{% block tutorial_content %}
<div class="tutorial-section">
    <h2>Introduction to Clustering</h2>
    <p>
        Clustering is an unsupervised machine learning technique that groups similar data points together 
        without prior knowledge of group labels. It's used to discover hidden patterns, structures, 
        and relationships in data, making it valuable for exploratory data analysis and data mining.
    </p>
    
    <div class="highlight-box">
        <h3>üéØ Why Clustering?</h3>
        <ul>
            <li><strong>Pattern Discovery:</strong> Identify hidden structures in data</li>
            <li><strong>Data Exploration:</strong> Understand data distribution and relationships</li>
            <li><strong>Customer Segmentation:</strong> Group customers by behavior or preferences</li>
            <li><strong>Anomaly Detection:</strong> Identify outliers and unusual patterns</li>
            <li><strong>Dimensionality Reduction:</strong> Simplify complex datasets</li>
        </ul>
    </div>
</div>

<div class="tutorial-section">
    <h2>Types of Clustering</h2>
    
    <div class="clustering-types-grid">
        <div class="clustering-type">
            <h3>üéØ Centroid-Based</h3>
            <p><strong>K-Means, K-Medoids</strong></p>
            <ul>
                <li>Clusters organized around central points</li>
                <li>Works well with spherical clusters</li>
                <li>Requires predefined number of clusters</li>
            </ul>
        </div>
        
        <div class="clustering-type">
            <h3>üåä Density-Based</h3>
            <p><strong>DBSCAN, OPTICS</strong></p>
            <ul>
                <li>Groups areas of high density</li>
                <li>Handles clusters of arbitrary shape</li>
                <li>Automatically determines cluster count</li>
            </ul>
        </div>
        
        <div class="clustering-type">
            <h3>üå≥ Hierarchical</h3>
            <p><strong>Agglomerative, Divisive</strong></p>
            <ul>
                <li>Creates tree-like cluster structures</li>
                <li>Provides multiple granularity levels</li>
                <li>No need to predefine cluster count</li>
            </ul>
        </div>
        
        <div class="clustering-type">
            <h3>üìä Distribution-Based</h3>
            <p><strong>Gaussian Mixture Models</strong></p>
            <ul>
                <li>Assumes clusters follow statistical distributions</li>
                <li>Provides probabilistic cluster membership</li>
                <li>Handles overlapping clusters well</li>
            </ul>
        </div>
    </div>
</div>

<div class="tutorial-section">
    <h2>K-Means Clustering</h2>
    <p>K-Means is the most popular clustering algorithm that partitions data into k clusters.</p>
    
    <h3>Algorithm Steps</h3>
    <ol>
        <li><strong>Initialize:</strong> Choose k cluster centers randomly</li>
        <li><strong>Assign:</strong> Assign each point to nearest cluster center</li>
        <li><strong>Update:</strong> Move cluster centers to mean of assigned points</li>
        <li><strong>Repeat:</strong> Continue until convergence</li>
    </ol>

    <h3>Mathematical Foundation</h3>
    <div class="formula-box">
        <strong>Objective Function:</strong> J = Œ£·µ¢ Œ£‚Çì‚ààC·µ¢ ||x - Œº·µ¢||¬≤<br>
        <strong>Cluster Center Update:</strong> Œº·µ¢ = (1/|C·µ¢|) Œ£‚Çì‚ààC·µ¢ x<br>
        <strong>Distance Metric:</strong> d(x, Œº) = ‚àö(Œ£(x‚±º - Œº‚±º)¬≤)
    </div>
</div>

<div class="tutorial-section">
    <h2>Implementation Example</h2>
    <p>Here's how to implement various clustering algorithms using scikit-learn:</p>
    
    <div class="code-example">import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt
import seaborn as sns

# Generate sample data
X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, 
                      n_features=2, random_state=42)

# Standardize features (important for distance-based algorithms)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# K-Means Clustering
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
y_kmeans = kmeans.fit_predict(X_scaled)

# DBSCAN Clustering
dbscan = DBSCAN(eps=0.5, min_samples=5)
y_dbscan = dbscan.fit_predict(X_scaled)

# Hierarchical Clustering
hierarchical = AgglomerativeClustering(n_clusters=4)
y_hierarchical = hierarchical.fit_predict(X_scaled)

# Gaussian Mixture Model
gmm = GaussianMixture(n_components=4, random_state=42)
y_gmm = gmm.fit_predict(X_scaled)

# Evaluate clustering performance
print("Clustering Evaluation:")
print(f"K-Means Silhouette Score: {silhouette_score(X_scaled, y_kmeans):.4f}")
print(f"DBSCAN Silhouette Score: {silhouette_score(X_scaled, y_dbscan):.4f}")
print(f"Hierarchical Silhouette Score: {silhouette_score(X_scaled, y_hierarchical):.4f}")
print(f"GMM Silhouette Score: {silhouette_score(X_scaled, y_gmm):.4f}")

# Plot results
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
algorithms = [
    ("K-Means", y_kmeans),
    ("DBSCAN", y_dbscan),
    ("Hierarchical", y_hierarchical),
    ("Gaussian Mixture", y_gmm)
]

for idx, (name, labels) in enumerate(algorithms):
    row, col = idx // 2, idx % 2
    scatter = axes[row, col].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
    axes[row, col].set_title(f'{name} Clustering')
    axes[row, col].set_xlabel('Feature 1')
    axes[row, col].set_ylabel('Feature 2')

plt.tight_layout()
plt.show()</div>
</div>

<div class="tutorial-section">
    <h2>Choosing the Right Number of Clusters</h2>
    
    <h3>Elbow Method</h3>
    <p>Plot the within-cluster sum of squares (WCSS) for different k values and look for the "elbow":</p>
    
    <div class="code-example"># Elbow Method for K-Means
wcss = []
k_range = range(1, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(10, 6))
plt.plot(k_range, wcss, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.xticks(k_range)
plt.grid(True)
plt.show()</div>

    <h3>Silhouette Analysis</h3>
    <p>Measure how similar objects are to their own cluster compared to other clusters:</p>
    
    <div class="code-example"># Silhouette Analysis
silhouette_scores = []

for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    cluster_labels = kmeans.fit_predict(X_scaled)
    silhouette_avg = silhouette_score(X_scaled, cluster_labels)
    silhouette_scores.append(silhouette_avg)

plt.figure(figsize=(10, 6))
plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.title('Silhouette Analysis for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Average Silhouette Score')
plt.xticks(range(2, 11))
plt.grid(True)
plt.show()</div>
</div>

<div class="tutorial-section">
    <h2>Advanced Clustering Techniques</h2>
    
    <div class="advanced-techniques-grid">
        <div class="technique">
            <h4>üîç DBSCAN</h4>
            <p><strong>Density-Based Spatial Clustering</strong></p>
            <ul>
                <li>Finds clusters of varying densities</li>
                <li>Robust to outliers</li>
                <li>Automatically determines cluster count</li>
                <li>Parameters: eps (neighborhood size), min_samples</li>
            </ul>
        </div>
        
        <div class="technique">
            <h4>üåä Mean Shift</h4>
            <p><strong>Mode-Seeking Algorithm</strong></p>
            <ul>
                <li>Finds dense areas in feature space</li>
                <li>No need to specify cluster count</li>
                <li>Can find arbitrary shaped clusters</li>
                <li>Computationally expensive for large datasets</li>
            </ul>
        </div>
        
        <div class="technique">
            <h4>üé≠ Spectral Clustering</h4>
            <p><strong>Graph-Based Clustering</strong></p>
            <ul>
                <li>Uses eigenvalues of similarity matrix</li>
                <li>Works well with non-convex clusters</li>
                <li>Effective for image segmentation</li>
                <li>Requires similarity matrix construction</li>
            </ul>
        </div>
        
        <div class="technique">
            <h4>üìä Gaussian Mixture Models</h4>
            <p><strong>Probabilistic Clustering</strong></p>
            <ul>
                <li>Soft clustering with membership probabilities</li>
                <li>Handles overlapping clusters</li>
                <li>Assumes Gaussian distributions</li>
                <li>Provides cluster uncertainty estimates</li>
            </ul>
        </div>
    </div>
</div>

<div class="tutorial-section">
    <h2>Clustering Evaluation Metrics</h2>
    
    <h3>Internal Metrics (No Ground Truth)</h3>
    <div class="formula-box">
        <strong>Silhouette Score:</strong> s(i) = (b(i) - a(i)) / max(a(i), b(i))<br>
        <strong>Calinski-Harabasz Index:</strong> CH = (SSB / (k-1)) / (SSW / (n-k))<br>
        <strong>Davies-Bouldin Index:</strong> DB = (1/k) Œ£·µ¢ max(R·µ¢‚±º)
    </div>

    <h3>External Metrics (With Ground Truth)</h3>
    <div class="formula-box">
        <strong>Adjusted Rand Index:</strong> ARI = (RI - E[RI]) / (max(RI) - E[RI])<br>
        <strong>Normalized Mutual Information:</strong> NMI = MI(X,Y) / ‚àö(H(X)H(Y))<br>
        <strong>V-Measure:</strong> V = (2 √ó precision √ó recall) / (precision + recall)
    </div>
</div>

<div class="tutorial-section">
    <h2>Real-World Applications</h2>
    
    <div class="applications-grid">
        <div class="application">
            <h4>üõí Customer Segmentation</h4>
            <p>Group customers by purchasing behavior, demographics, or preferences for targeted marketing</p>
        </div>
        
        <div class="application">
            <h4>üß¨ Gene Expression Analysis</h4>
            <p>Identify groups of genes with similar expression patterns across different conditions</p>
        </div>
        
        <div class="application">
            <h4>üñºÔ∏è Image Segmentation</h4>
            <p>Partition images into meaningful regions for computer vision and medical imaging</p>
        </div>
        
        <div class="application">
            <h4>üì∞ Document Clustering</h4>
            <p>Organize large document collections by topic or theme for information retrieval</p>
        </div>
        
        <div class="application">
            <h4>üåê Social Network Analysis</h4>
            <p>Identify communities and groups within social networks and recommendation systems</p>
        </div>
        
        <div class="application">
            <h4>üö® Anomaly Detection</h4>
            <p>Detect fraud, network intrusions, or unusual patterns by identifying outliers</p>
        </div>
    </div>
</div>

<div class="tutorial-section">
    <h2>üöÄ Next Steps</h2>
    <div class="example-box">
        <h3>Master Unsupervised Learning</h3>
        <ul>
            <li><strong>Practice:</strong> Apply clustering to real datasets from different domains</li>
            <li><strong>Dimensionality Reduction:</strong> Learn PCA, t-SNE, and UMAP</li>
            <li><strong>Advanced Techniques:</strong> Explore ensemble clustering and deep clustering</li>
            <li><strong>Evaluation:</strong> Master different clustering validation techniques</li>
        </ul>
        
        <div style="margin-top: 1.5rem;">
            <a href="{{ url_for('tutorial_detail', slug='feature-engineering') }}" class="btn btn-primary" style="margin-right: 1rem;">Next: Feature Engineering</a>
            <a href="{{ url_for('tutorials_list') }}" class="btn btn-secondary">All Tutorials</a>
        </div>
    </div>
</div>
{% endblock %}
