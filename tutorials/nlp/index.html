<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Complete Interactive NLP Course - Alireza Barzin Zanganeh</title>
    <link rel="stylesheet" href="../../assets/css/style.css">
</head>
<body class="nlp-course"> 
    <div style="
        background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%);
        color: white;
        padding: 12px 20px;
        text-align: center;
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        z-index: 1000;
        box-shadow: 0 2px 10px rgba(0,0,0,0.2);
        font-size: 14px;
        border-bottom: 2px solid rgba(255,255,255,0.2);
    ">
        <span style="margin-right: 10px;">üöß</span>
        <strong>Website Under Development:</strong> 
        We experienced some downtime recently. Some links may be broken and topics incomplete - working to fix everything gradually. 
        <span style="margin-left: 10px;">Thank you for your patience! üôè</span>
        <button onclick="this.parentElement.style.display='none'" style="
            background: rgba(255,255,255,0.2);
            border: none;
            color: white;
            margin-left: 15px;
            padding: 4px 8px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 12px;
        ">‚úï</button>
    </div>

    <header class="azbn-header" style="top: 50px;">
        <div class="azbn-container">
            <h1><a href="../../" style="text-decoration: none; color: #4f46e5;">Alireza Barzin Zanganeh</a></h1>
            <nav>
                <a href="../../#home">Home</a>
                <a href="../">Tutorials</a>
                <a href="../../#projects">Projects</a>
            </nav>
        </div>
    </header>
    <main style="padding-top: 150px;">
        <section class="azbn-section">
            <div class="azbn-container">
                <div class="container">
                    <div class="header">
                        <h1>ü§ñ Complete Interactive NLP Course</h1>
                        <p>Master Natural Language Processing from fundamentals to advanced Transformers</p>
                        <div class="progress-bar">
                            <div class="progress-fill" id="progressFill"></div>
                        </div>
                    </div>
                
                    <div class="course-nav">
                        <button class="nav-btn active" onclick="showSection('intro', event)">Introduction</button>
                        <button class="nav-btn" onclick="showSection('text-repr', event)">Text Representation</button>
                        <button class="nav-btn" onclick="showSection('embeddings', event)">Word Embeddings</button>
                        <button class="nav-btn" onclick="showSection('sentiment', event)">Sentiment Analysis</button>
                        <button class="nav-btn" onclick="showSection('seq2seq', event)">Seq2Seq Models</button>
                        <button class="nav-btn" onclick="showSection('transformers', event)">Transformers</button>
                        <button class="nav-btn" onclick="showSection('attention', event)">Self-Attention</button>
                        <button class="nav-btn" onclick="showSection('applications', event)">Applications</button>
                    </div>

        <!-- Introduction Section -->
        <div id="intro" class="section active">
            <h2>üëã Welcome to the NLP Course!</h2>
            <h2>üöÄ Introduction to Natural Language Processing</h2>
            
            <p>Natural Language Processing (NLP) is a machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language. It involves reading, deciphering, understanding, and making sense of human languages.</p>

            <p>In this course, you will learn the fundamentals of NLP, from basic text representation techniques to advanced transformer models like BERT and GPT. Each section includes interactive demos, quizzes, and practical applications.</p>

            <h3>üåü Key Topics Covered</h3>
            <ul>
                <li>Text Representation Techniques</li>
                <li>Word Embeddings</li>
                <li>Sentiment Analysis</li>
                <li>Seq2Seq Models</li>
                <li>Transformers and Self-Attention</li>
                <li>Applications in Real-World Scenarios</li>
            </ul>

            <h3>üë®‚Äçüè´ Who This Course is For</h3>
            <p>This course is designed for anyone interested in learning about NLP, from beginners to advanced practitioners. No prior experience with machine learning is required, but familiarity with Python is recommended.</p>

            <div class="interactive-demo">
                <h3>üéØ Try NLP in Action!</h3>
                <p>Enter some text to see basic NLP preprocessing:</p>
                <input type="text" class="demo-input" id="nlpInput" placeholder="Enter your text here..." value="Hello! This is a GREAT example of NLP preprocessing.">
                <button class="demo-btn" onclick="preprocessText()">Process Text</button>
                <div class="demo-output" id="nlpOutput"></div>
            </div>

            <h3>üåü Key Applications of NLP</h3>
            <div class="pros-cons">
                <div class="pros">
                    <h4>üìß Communication</h4>
                    <ul>
                        <li>Spam Filters (Gmail)</li>
                        <li>Email Classification</li>
                        <li>Chatbots & Virtual Assistants</li>
                        <li>Language Translation</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>üìä Business Intelligence</h4>
                    <ul>
                        <li>Sentiment Analysis</li>
                        <li>Market Research</li>
                        <li>Algorithmic Trading</li>
                        <li>Document Summarization</li>
                    </ul>
                </div>
            </div>

            <div class="quiz-container">
                <div class="quiz-question">üìù Quick Quiz: Which of these is NOT a typical NLP application?</div>
                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Email spam detection</div>
                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Language translation</div>
                <div class="quiz-option" onclick="checkAnswer(this, true)">C) Image object detection</div>
                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Sentiment analysis</div>
            </div>
        </div>

        <!-- Text Representation Section -->
        <div id="text-repr" class="section">
            <h2>üìù Text Representation Techniques</h2>
            
            <h3>1. Bag of Words (BoW)</h3>
            <p>BoW represents text by the frequency of words within a document, ignoring grammar and word order.</p>

            <div class="interactive-demo">
                <h3>üéØ Bag of Words Demo</h3>
                <input type="text" class="demo-input" id="bowInput" placeholder="Enter sentences separated by | (e.g., I love NLP | NLP is amazing)" value="I love machine learning | Machine learning is powerful">
                <button class="demo-btn" onclick="demonstrateBOW()">Create BoW</button>
                <div class="demo-output" id="bowOutput"></div>
            </div>

            <div class="pros-cons">
                <div class="pros">
                    <h4>‚úÖ Advantages</h4>
                    <ul>
                        <li>Simple and easy to implement</li>
                        <li>Works well for text classification</li>
                        <li>Computationally efficient</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>‚ùå Disadvantages</h4>
                    <ul>
                        <li>High dimensionality</li>
                        <li>Sparse features</li>
                        <li>Treats synonyms differently</li>
                        <li>Ignores word order</li>
                    </ul>
                </div>
            </div>

            <h3>2. TF-IDF (Term Frequency-Inverse Document Frequency)</h3>
            <p>TF-IDF reflects the importance of a word in a document relative to a collection of documents.</p>

            <div class="example-box">
                <strong>Formula:</strong><br>
                TF-IDF(t,d) = TF(t,d) √ó IDF(t)<br>
                Where:<br>
                ‚Ä¢ TF = (Number of times term appears in document) / (Total number of terms in document)<br>
                ‚Ä¢ IDF = log(Total number of documents / Number of documents containing the term)
            </div>

            <div class="interactive-demo">
                <h3>üéØ TF-IDF Demo</h3>
                <input type="text" class="demo-input" id="tfidfInput" placeholder="Enter documents separated by |" value="The cat sat on the mat | The dog ran in the park | Cats and dogs are pets">
                <button class="demo-btn" onclick="demonstrateTFIDF()">Calculate TF-IDF</button>
                <div class="demo-output" id="tfidfOutput"></div>
            </div>
        </div>

        <!-- Word Embeddings Section -->
        <div id="embeddings" class="section">
            <h2>üß† Word Embeddings</h2>
            
            <p>Word embeddings are dense vector representations of words that capture their semantic meaning. Unlike BoW and TF-IDF, embeddings consider the context in which words appear.</p>

            <div class="example-box">
                <strong>Famous Example:</strong><br>
                king - man + woman = queen<br>
                This demonstrates how embeddings capture semantic relationships!
            </div>

            <h3>1. Word2Vec</h3>
            <p>Word2Vec uses neural networks to learn word associations from a large corpus of text.</p>

            <div class="architecture-diagram">
                <div class="layer">Input Layer</div>
                <span class="arrow">‚Üí</span>
                <div class="layer">Hidden Layer (Embeddings)</div>
                <span class="arrow">‚Üí</span>
                <div class="layer">Output Layer</div>
            </div>

            <div class="interactive-demo">
                <h3>üéØ Word Similarity Demo</h3>
                <p>Enter two words to see their conceptual similarity:</p>
                <input type="text" class="demo-input" id="word1" placeholder="First word" value="king">
                <input type="text" class="demo-input" id="word2" placeholder="Second word" value="queen">
                <button class="demo-btn" onclick="calculateWordSimilarity()">Calculate Similarity</button>
                <div class="demo-output" id="similarityOutput"></div>
            </div>

            <h3>Word2Vec Variants</h3>
            <div class="pros-cons">
                <div class="pros">
                    <h4>üéØ CBOW (Continuous Bag of Words)</h4>
                    <ul>
                        <li>Predicts target word from context</li>
                        <li>Faster training</li>
                        <li>Better for frequent words</li>
                        <li>Good for large datasets</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>üéØ Skip-gram</h4>
                    <ul>
                        <li>Predicts context from target word</li>
                        <li>Better for rare words</li>
                        <li>Higher accuracy</li>
                        <li>Good for small datasets</li>
                    </ul>
                </div>
            </div>

            <h3>2. GloVe (Global Vectors)</h3>
            <p>GloVe generates word vectors based on co-occurrence statistics in a large corpus.</p>

            <div class="interactive-demo">
                <h3>üéØ Co-occurrence Matrix Demo</h3>
                <input type="text" class="demo-input" id="gloveInput" placeholder="Enter a sentence" value="The quick brown fox jumps over the lazy dog">
                <button class="demo-btn" onclick="demonstrateCooccurrence()">Generate Co-occurrence</button>
                <div class="demo-output" id="gloveOutput"></div>
            </div>

            <h3>3. FastText</h3>
            <p>FastText extends Word2Vec by using subword representations (character n-grams), making it excellent for handling out-of-vocabulary words.</p>

            <div class="example-box">
                <strong>FastText Advantage:</strong><br>
                Even if "unhappiness" wasn't in training data, FastText can understand it through subwords:<br>
                "un-", "-happy-", "-ness", "unhappy", "happiness", etc.
            </div>
        </div>

        <!-- Sentiment Analysis Section -->
        <div id="sentiment" class="section">
            <h2>üòä Sentiment Analysis</h2>
            
            <p>Sentiment analysis determines the emotional tone behind words, helping understand opinions, attitudes, and emotions expressed in text.</p>

            <div class="interactive-demo">
                <h3>üéØ Live Sentiment Analysis</h3>
                <input type="text" class="demo-input" id="sentimentInput" placeholder="Enter text to analyze sentiment..." value="I love this amazing product! It works perfectly.">
                <button class="demo-btn" onclick="analyzeSentiment()">Analyze Sentiment</button>
                <div class="demo-output" id="sentimentOutput"></div>
            </div>

            <h3>üìä Sentiment Analysis Workflow</h3>
            <div class="architecture-diagram">
                <div class="layer">Data Collection</div>
                <span class="arrow">‚Üí</span>
                <div class="layer">Preprocessing</div>
                <span class="arrow">‚Üí</span>
                <div class="layer">Feature Extraction</div>
                <span class="arrow">‚Üí</span>
                <div class="layer">Model Training</div>
                <span class="arrow">‚Üí</span>
                <div class="layer">Evaluation</div>
            </div>

            <h3>üè¢ Applications</h3>
            <div class="pros-cons">
                <div class="pros">
                    <h4>üõçÔ∏è Business Applications</h4>
                    <ul>
                        <li>Brand reputation monitoring</li>
                        <li>Product review analysis</li>
                        <li>Customer feedback processing</li>
                        <li>Market research</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>üèõÔ∏è Social & Political</h4>
                    <ul>
                        <li>Social media monitoring</li>
                        <li>Political opinion tracking</li>
                        <li>Public sentiment analysis</li>
                        <li>Crisis management</li>
                    </ul>
                </div>
            </div>

            <h3>‚ö†Ô∏è Challenges in Sentiment Analysis</h3>
            <ul>
                <li><strong>Sarcasm Detection:</strong> "Great job!" might be sarcastic</li>
                <li><strong>Context Dependency:</strong> Same word, different sentiments</li>
                <li><strong>Imbalanced Datasets:</strong> More positive than negative examples</li>
                <li><strong>Domain Specificity:</strong> Movie reviews vs. product reviews</li>
            </ul>

            <div class="quiz-container">
                <div class="quiz-question">üìù Quiz: Which is the biggest challenge in sentiment analysis?</div>
                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Processing speed</div>
                <div class="quiz-option" onclick="checkAnswer(this, true)">B) Understanding context and sarcasm</div>
                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Memory requirements</div>
                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Data storage</div>
            </div>
        </div>

        <!-- Seq2Seq Section -->
        <div id="seq2seq" class="section">
            <h2>üîÑ Sequence-to-Sequence Models</h2>
            
            <p>Seq2Seq models are specialized neural network architectures designed to handle sequences as both input and output. They're perfect for tasks like translation, summarization, and chatbots.</p>

            <div class="architecture-diagram">
                <h3>üèóÔ∏è Seq2Seq Architecture</h3>
                <div class="layer">Encoder</div>
                <span class="arrow">‚Üí</span>
                <div class="layer">Context Vector</div>
                <span class="arrow">‚Üí</span>
                <div class="layer">Decoder</div>
            </div>

            <div class="interactive-demo">
                <h3>üéØ Translation Demo (Conceptual)</h3>
                <input type="text" class="demo-input" id="translateInput" placeholder="Enter English text..." value="Hello how are you today">
                <select class="demo-input" id="targetLang">
                    <option value="spanish">Spanish</option>
                    <option value="french">French</option>
                    <option value="german">German</option>
                </select>
                <button class="demo-btn" onclick="demonstrateTranslation()">Translate</button>
                <div class="demo-output" id="translateOutput"></div>
            </div>

            <h3>üß© Key Components</h3>
            
            <div class="example-box">
                <h4>üì• Encoder</h4>
                <p>Processes each token in the input sequence and creates a fixed-length context vector that encapsulates the meaning of the entire input sequence.</p>
            </div>

            <div class="example-box">
                <h4>üéØ Context Vector</h4>
                <p>The final internal state of the encoder - a dense representation that captures the essence of the input sequence.</p>
            </div>

            <div class="example-box">
                <h4>üì§ Decoder</h4>
                <p>Reads the context vector and generates the target sequence token by token, using the context and previously generated tokens.</p>
            </div>

            <h3>üîß Types of Seq2Seq Models</h3>
            <ul>
                <li><strong>Many-to-One:</strong> Sentiment analysis (sequence ‚Üí single label)</li>
                <li><strong>One-to-Many:</strong> Image captioning (image ‚Üí sequence of words)</li>
                <li><strong>Many-to-Many:</strong> Machine translation (sequence ‚Üí sequence)</li>
                <li><strong>Synchronized:</strong> Video classification (frame by frame)</li>
            </ul>

            <h3>‚ö†Ô∏è Limitations</h3>
            <div class="pros-cons">
                <div class="cons">
                    <h4>üöß RNN/LSTM Based Seq2Seq Issues</h4>
                    <ul>
                        <li>Vanishing gradient problems</li>
                        <li>Sequential processing (no parallelization)</li>
                        <li>Information bottleneck in context vector</li>
                        <li>Difficulty with long sequences</li>
                    </ul>
                </div>
                <div class="pros">
                    <h4>‚ú® Solutions</h4>
                    <ul>
                        <li>Attention mechanisms</li>
                        <li>Transformer architecture</li>
                        <li>Better initialization techniques</li>
                        <li>Advanced optimization methods</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Transformers Section -->
        <div id="transformers" class="section">
            <h2>ü§ñ Transformers: The Revolution</h2>
            
            <p>Transformers revolutionized NLP by introducing the "Attention is All You Need" paradigm, eliminating the need for recurrent connections while achieving superior performance.</p>

            <div class="example-box">
                <h4>üéØ Key Innovation: Self-Attention</h4>
                <p>Instead of processing sequences step-by-step, Transformers look at all positions simultaneously and learn which parts are most relevant to each other.</p>
            </div>

            <div class="interactive-demo">
                <h3>üéØ Transformer Components Explorer</h3>
                <select class="demo-input" id="transformerComponent">
                    <option value="overview">Architecture Overview</option>
                    <option value="encoder">Encoder Stack</option>
                    <option value="decoder">Decoder Stack</option>
                    <option value="attention">Multi-Head Attention</option>
                </select>
                <button class="demo-btn" onclick="exploreTransformer()">Explore Component</button>
                <div class="demo-output" id="transformerOutput"></div>
            </div>

            <h3>üèóÔ∏è Transformer Architecture</h3>
            <div class="architecture-diagram">
                <div style="display: flex; justify-content: space-between; width: 100%;">
                    <div style="text-align: center;">
                        <h4>üì• Encoder</h4>
                        <div class="layer">Multi-Head Attention</div>
                        <div class="layer">Add & Norm</div>
                        <div class="layer">Feed Forward</div>
                        <div class="layer">Add & Norm</div>
                        <p>√ó6 layers</p>
                    </div>
                    <div style="text-align: center;">
                        <h4>üì§ Decoder</h4>
                        <div class="layer">Masked Multi-Head Attention</div>
                        <div class="layer">Add & Norm</div>
                        <div class="layer">Multi-Head Attention</div>
                        <div class="layer">Add & Norm</div>
                        <div class="layer">Feed Forward</div>
                        <div class="layer">Add & Norm</div>
                        <p>√ó6 layers</p>
                    </div>
                </div>
            </div>

            <h3>‚ö° Why Transformers?</h3>
            <div class="pros-cons">
                <div class="pros">
                    <h4>‚úÖ Advantages</h4>
                    <ul>
                        <li><strong>Parallelization:</strong> Process entire sequences simultaneously</li>
                        <li><strong>Long-term Dependencies:</strong> Better at capturing relationships</li>
                        <li><strong>Scalability:</strong> Easy to scale to larger datasets</li>
                        <li><strong>Transfer Learning:</strong> Pre-trained models work across tasks</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>‚ö†Ô∏è Limitations</h4>
                    <ul>
                        <li><strong>Computational Cost:</strong> Quadratic complexity with sequence length</li>
                        <li><strong>Data Hungry:</strong> Requires large amounts of training data</li>
                        <li><strong>Memory Requirements:</strong> High memory usage</li>
                        <li><strong>Overfitting:</strong> Prone to overfitting on small datasets</li>
                    </ul>
                </div>
            </div>

            <h3>üåü Famous Transformer Models</h3>
            <ul>
                <li><strong>BERT:</strong> Bidirectional Encoder Representations from Transformers</li>
                <li><strong>GPT:</strong> Generative Pre-trained Transformer</li>
                <li><strong>T5:</strong> Text-to-Text Transfer Transformer</li>
                <li><strong>RoBERTa:</strong> Robustly Optimized BERT Pretraining Approach</li>
            </ul>
        </div>

        <!-- Self-Attention Section -->
        <div id="attention" class="section">
            <h2>üéØ Self-Attention Mechanism</h2>
            
            <p>Self-attention is the core innovation of Transformers. It allows each position in a sequence to attend to all positions in the same sequence to compute a representation.</p>

            <div class="interactive-demo">
                <h3>üéØ Attention Visualization</h3>
                <input type="text" class="demo-input" id="attentionInput" placeholder="Enter a sentence to visualize attention..." value="The cat sat on the mat">
                <button class="demo-btn" onclick="visualizeAttention()">Visualize Attention</button>
                <div class="demo-output" id="attentionOutput"></div>
            </div>

            <h3>üîç How Self-Attention Works</h3>
            
            <div class="example-box">
                <h4>üóùÔ∏è Key Components</h4>
                <ul>
                    <li><strong>Query (Q):</strong> What information are we looking for?</li>
                    <li><strong>Key (K):</strong> What information does each position offer?</li>
                    <li><strong>Value (V):</strong> The actual information to be retrieved</li>
                </ul>
            </div>

            <div class="code-block">
Attention(Q, K, V) = softmax(QK^T / ‚àöd_k)V

Where:
- Q, K, V are matrices of queries, keys, and values
- d_k is the dimension of the key vectors
- ‚àöd_k is used for scaling to prevent extremely small gradients
            </div>

            <div class="interactive-demo">
                <h3>üéØ Step-by-Step Attention Calculation</h3>
                <button class="demo-btn" onclick="demonstrateAttentionSteps()">Show Attention Steps</button>
                <div class="demo-output" id="attentionStepsOutput"></div>
            </div>

            <h3>üé≠ Multi-Head Attention</h3>
            <p>Instead of performing a single attention function, multi-head attention runs multiple attention "heads" in parallel, each focusing on different types of relationships.</p>

            <div class="architecture-diagram">
                <div style="display: flex; justify-content: space-around; flex-wrap: wrap;">
                    <div class="layer">Head 1</div>
                    <div class="layer">Head 2</div>
                    <div class="layer">Head 3</div>
                    <div class="layer">...</div>
                    <div class="layer">Head 8</div>
                </div>
                <div style="text-align: center; margin: 20px 0;">
                    <span class="arrow">‚Üì</span>
                </div>
                <div class="layer">Concatenate & Linear</div>
            </div>

            <div class="interactive-demo">
                <h3>üéØ Multi-Head Attention Demo</h3>
                <input type="number" class="demo-input" id="numHeads" min="1" max="8" value="4" placeholder="Number of attention heads">
                <button class="demo-btn" onclick="demonstrateMultiHead()">Simulate Multi-Head</button>
                <div class="demo-output" id="multiHeadOutput"></div>
            </div>

            <div class="quiz-container">
                <div class="quiz-question">üìù Quiz: What is the main advantage of multi-head attention?</div>
                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Faster computation</div>
                <div class="quiz-option" onclick="checkAnswer(this, true)">B) Captures different types of relationships simultaneously</div>
                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Uses less memory</div>
                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Simpler to implement</div>
            </div>
        </div>

        <!-- Applications Section -->
        <div id="applications" class="section">
            <h2>üöÄ Modern NLP Applications</h2>
            
            <p>Modern NLP has enabled countless applications that we use daily. Let's explore some cutting-edge applications and try them out!</p>

            <div class="interactive-demo">
                <h3>üéØ Text Summarization</h3>
                <textarea class="demo-input" id="summaryInput" rows="5" placeholder="Enter a long text to summarize...">Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics, in its pursuit to fill the gap between human communication and computer understanding. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages.</textarea>
                <button class="demo-btn" onclick="summarizeText()">Summarize</button>
                <div class="demo-output" id="summaryOutput"></div>
            </div>

            <div class="interactive-demo">
                <h3>üéØ Named Entity Recognition (NER)</h3>
                <input type="text" class="demo-input" id="nerInput" placeholder="Enter text with names, places, organizations..." value="Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.">
                <button class="demo-btn" onclick="performNER()">Extract Entities</button>
                <div class="demo-output" id="nerOutput"></div>
            </div>

            <div class="interactive-demo">
                <h3>üéØ Question Answering</h3>
                <textarea class="demo-input" id="qaContext" rows="3" placeholder="Enter context...">The Transformer is a deep learning model introduced in 2017, used primarily in the field of natural language processing. Like recurrent neural networks, transformers are designed to handle sequential input data, such as natural language, for tasks such as translation and text summarization.</textarea>
                <input type="text" class="demo-input" id="qaQuestion" placeholder="Ask a question about the context..." value="When was the Transformer model introduced?">
                <button class="demo-btn" onclick="answerQuestion()">Answer Question</button>
                <div class="demo-output" id="qaOutput"></div>
            </div>

            <h3>üè¢ Industry Applications</h3>
            <div class="pros-cons">
                <div class="pros">
                    <h4>üè• Healthcare</h4>
                    <ul>
                        <li>Medical record analysis</li>
                        <li>Drug discovery assistance</li>
                        <li>Clinical decision support</li>
                        <li>Patient interaction chatbots</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>üí∞ Finance</h4>
                    <ul>
                        <li>Fraud detection</li>
                        <li>Risk assessment</li>
                        <li>Algorithmic trading</li>
                        <li>Customer service automation</li>
                    </ul>
                </div>
            </div>

            <div class="pros-cons">
                <div class="pros">
                    <h4>üéì Education</h4>
                    <ul>
                        <li>Automated essay scoring</li>
                        <li>Personalized learning</li>
                        <li>Language learning apps</li>
                        <li>Research assistance</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>üõí E-commerce</h4>
                    <ul>
                        <li>Product recommendations</li>
                        <li>Review analysis</li>
                        <li>Customer support</li>
                        <li>Search optimization</li>
                    </ul>
                </div>
            </div>

            <h3>üîÆ Future of NLP</h3>
            <ul>
                <li><strong>Multimodal Models:</strong> Combining text, images, and audio</li>
                <li><strong>Few-shot Learning:</strong> Learning from minimal examples</li>
                <li><strong>Efficient Models:</strong> Smaller, faster models for mobile devices</li>
                <li><strong>Ethical AI:</strong> Reducing bias and improving fairness</li>
                <li><strong>Specialized Models:</strong> Domain-specific fine-tuned models</li>
            </ul>

            <div class="example-box">
                <h4>üéä Congratulations!</h4>
                <p>You've completed the comprehensive NLP course! You now understand the fundamental concepts from basic text representation to advanced Transformer architectures. Keep practicing and exploring to master these powerful techniques!</p>
            </div>
        </div>
            </div>
        </section>
    </main>
    <script>
        let currentSection = 'intro';
        const sections = ['intro', 'text-repr', 'embeddings', 'sentiment', 'seq2seq', 'transformers', 'attention', 'applications'];
        
        function showSection(sectionId, evt) {
            // Hide all sections
            document.querySelectorAll('.section').forEach(section => {
                section.classList.remove('active');
            });
            
            // Show target section
            document.getElementById(sectionId).classList.add('active');
            
            // Update navigation
            document.querySelectorAll('.nav-btn').forEach(btn => {
                btn.classList.remove('active');
            });
            if (evt && evt.target) {
                evt.target.classList.add('active');
            }
            
            // Update progress
            const progress = (sections.indexOf(sectionId) + 1) / sections.length * 100;
            document.getElementById('progressFill').style.width = progress + '%';
            
            currentSection = sectionId;
        }

        function preprocessText() {
            const input = document.getElementById('nlpInput').value;
            const output = document.getElementById('nlpOutput');
            
            // Simple preprocessing simulation
            const steps = [
                `Original: "${input}"`,
                `Lowercased: "${input.toLowerCase()}"`,
                `Tokens: [${input.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/).map(w => `"${w}"`).join(', ')}]`,
                `Filtered (no stopwords): [${input.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/).filter(w => !['the', 'is', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'].includes(w)).map(w => `"${w}"`).join(', ')}]`
            ];
            
            output.innerHTML = steps.join('<br>');
        }

        function demonstrateBOW() {
            const input = document.getElementById('bowInput').value;
            const output = document.getElementById('bowOutput');
            
            const documents = input.split('|').map(doc => doc.trim());
            const allWords = new Set();
            
            // Collect all unique words
            documents.forEach(doc => {
                const words = doc.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
                words.forEach(word => allWords.add(word));
            });
            
            const vocabulary = Array.from(allWords).sort();
            
            // Create BoW vectors
            let result = '<strong>Vocabulary:</strong> [' + vocabulary.join(', ') + ']<br><br>';
            
            documents.forEach((doc, i) => {
                const words = doc.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
                const vector = vocabulary.map(word => words.includes(word) ? 1 : 0);
                result += `<strong>Document ${i + 1}:</strong> "${doc}"<br>`;
                result += `<strong>BoW Vector:</strong> [${vector.join(', ')}]<br><br>`;
            });
            
            output.innerHTML = result;
        }

        function demonstrateTFIDF() {
            const input = document.getElementById('tfidfInput').value;
            const output = document.getElementById('tfidfOutput');
            
            const documents = input.split('|').map(doc => doc.trim());
            const allWords = new Set();
            
            // Collect all unique words
            documents.forEach(doc => {
                const words = doc.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
                words.forEach(word => allWords.add(word));
            });
            
            const vocabulary = Array.from(allWords).sort();
            
            let result = '<strong>TF-IDF Calculation:</strong><br><br>';
            
            documents.forEach((doc, i) => {
                const words = doc.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
                const wordCount = {};
                words.forEach(word => wordCount[word] = (wordCount[word] || 0) + 1);
                
                result += `<strong>Document ${i + 1}:</strong> "${doc}"<br>`;
                
                vocabulary.forEach(word => {
                    const tf = (wordCount[word] || 0) / words.length;
                    const docsWithWord = documents.filter(d => 
                        d.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/).includes(word)
                    ).length;
                    const idf = Math.log(documents.length / docsWithWord);
                    const tfidf = tf * idf;
                    
                    if (tfidf > 0) {
                        result += `&nbsp;&nbsp;${word}: TF=${tf.toFixed(3)}, IDF=${idf.toFixed(3)}, TF-IDF=${tfidf.toFixed(3)}<br>`;
                    }
                });
                result += '<br>';
            });
            
            output.innerHTML = result;
        }

        function calculateWordSimilarity() {
            const word1 = document.getElementById('word1').value.toLowerCase();
            const word2 = document.getElementById('word2').value.toLowerCase();
            const output = document.getElementById('similarityOutput');
            
            // Simulated similarity based on conceptual relationships
            const similarities = {
                'king,queen': 0.85, 'queen,king': 0.85,
                'man,woman': 0.75, 'woman,man': 0.75,
                'dog,cat': 0.70, 'cat,dog': 0.70,
                'happy,joy': 0.80, 'joy,happy': 0.80,
                'computer,laptop': 0.85, 'laptop,computer': 0.85,
                'car,vehicle': 0.80, 'vehicle,car': 0.80
            };
            
            const key = `${word1},${word2}`;
            const similarity = similarities[key] || (Math.random() * 0.5 + 0.2); // Random similarity for demo
            
            output.innerHTML = `
                <strong>Similarity Analysis:</strong><br>
                Word 1: "${word1}"<br>
                Word 2: "${word2}"<br>
                <strong>Cosine Similarity: ${similarity.toFixed(3)}</strong><br>
                <em>Note: This is a simplified demonstration. Real embeddings require trained models.</em>
            `;
        }

        function demonstrateCooccurrence() {
            const input = document.getElementById('gloveInput').value;
            const output = document.getElementById('gloveOutput');
            
            const words = input.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
            const windowSize = 2;
            const cooccurrence = {};
            
            // Initialize co-occurrence matrix
            words.forEach(word => {
                cooccurrence[word] = {};
                words.forEach(otherWord => {
                    cooccurrence[word][otherWord] = 0;
                });
            });
            
            // Calculate co-occurrences
            for (let i = 0; i < words.length; i++) {
                for (let j = Math.max(0, i - windowSize); j <= Math.min(words.length - 1, i + windowSize); j++) {
                    if (i !== j) {
                        cooccurrence[words[i]][words[j]]++;
                    }
                }
            }
            
            let result = '<strong>Co-occurrence Matrix (window size: 2):</strong><br><br>';
            result += '<table border="1" style="border-collapse: collapse; margin: 10px 0;"><tr><th></th>';
            
            words.forEach(word => {
                result += `<th>${word}</th>`;
            });
            result += '</tr>';
            
            words.forEach(word1 => {
                result += `<tr><th>${word1}</th>`;
                words.forEach(word2 => {
                    result += `<td style="padding: 5px; text-align: center;">${cooccurrence[word1][word2]}</td>`;
                });
                result += '</tr>';
            });
            result += '</table>';
            
            output.innerHTML = result;
        }

        function analyzeSentiment() {
            const input = document.getElementById('sentimentInput').value;
            const output = document.getElementById('sentimentOutput');
            
            // Simple rule-based sentiment analysis for demo
            const positiveWords = ['love', 'amazing', 'great', 'excellent', 'wonderful', 'fantastic', 'good', 'perfect', 'awesome', 'brilliant'];
            const negativeWords = ['hate', 'terrible', 'awful', 'bad', 'horrible', 'disgusting', 'worst', 'disappointing', 'poor', 'useless'];
            
            const words = input.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
            
            let positiveScore = 0;
            let negativeScore = 0;
            
            words.forEach(word => {
                if (positiveWords.includes(word)) positiveScore++;
                if (negativeWords.includes(word)) negativeScore++;
            });
            
            let sentiment, confidence, emoji;
            if (positiveScore > negativeScore) {
                sentiment = 'Positive';
                confidence = ((positiveScore / (positiveScore + negativeScore + 1)) * 100).toFixed(1);
                emoji = 'üòä';
            } else if (negativeScore > positiveScore) {
                sentiment = 'Negative';
                confidence = ((negativeScore / (positiveScore + negativeScore + 1)) * 100).toFixed(1);
                emoji = 'üòû';
            } else {
                sentiment = 'Neutral';
                confidence = '50.0';
                emoji = 'üòê';
            }
            
            output.innerHTML = `
                <div style="text-align: center; font-size: 2em; margin: 10px 0;">${emoji}</div>
                <strong>Sentiment:</strong> ${sentiment}<br>
                <strong>Confidence:</strong> ${confidence}%<br>
                <strong>Analysis:</strong><br>
                ‚Ä¢ Positive words found: ${positiveScore}<br>
                ‚Ä¢ Negative words found: ${negativeScore}<br>
                <em>Note: This is a simplified demonstration using rule-based analysis.</em>
            `;
        }

        function demonstrateTranslation() {
            const input = document.getElementById('translateInput').value;
            const targetLang = document.getElementById('targetLang').value;
            const output = document.getElementById('translateOutput');
            
            // Simple mock translations for demo
            const translations = {
                'spanish': {
                    'hello': 'hola',
                    'how': 'c√≥mo',
                    'are': 'est√°s',
                    'you': 't√∫',
                    'today': 'hoy',
                    'good': 'bueno',
                    'morning': 'ma√±ana',
                    'thank': 'gracias',
                    'please': 'por favor'
                },
                'french': {
                    'hello': 'bonjour',
                    'how': 'comment',
                    'are': '√™tes',
                    'you': 'vous',
                    'today': 'aujourd\'hui',
                    'good': 'bon',
                    'morning': 'matin',
                    'thank': 'merci',
                    'please': 's\'il vous pla√Æt'
                },
                'german': {
                    'hello': 'hallo',
                    'how': 'wie',
                    'are': 'sind',
                    'you': 'sie',
                    'today': 'heute',
                    'good': 'gut',
                    'morning': 'morgen',
                    'thank': 'danke',
                    'please': 'bitte'
                }
            };
            
            const words = input.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
            const translatedWords = words.map(word => 
                translations[targetLang][word] || word
            );
            
            output.innerHTML = `
                <strong>Original (English):</strong> ${input}<br>
                <strong>Translated (${targetLang.charAt(0).toUpperCase() + targetLang.slice(1)}):</strong> ${translatedWords.join(' ')}<br>
                <em>Note: This is a simplified word-by-word translation for demonstration.</em>
            `;
        }

        function exploreTransformer() {
            const component = document.getElementById('transformerComponent').value;
            const output = document.getElementById('transformerOutput');
            
            const explanations = {
                'overview': `
                    <strong>üèóÔ∏è Transformer Architecture Overview:</strong><br><br>
                    The Transformer consists of:<br>
                    ‚Ä¢ <strong>Encoder Stack:</strong> 6 identical layers processing input<br>
                    ‚Ä¢ <strong>Decoder Stack:</strong> 6 identical layers generating output<br>
                    ‚Ä¢ <strong>Input/Output Embeddings:</strong> Convert tokens to vectors<br>
                    ‚Ä¢ <strong>Positional Encoding:</strong> Add position information<br><br>
                    <em>Key Innovation: No recurrence, only attention!</em>
                `,
                'encoder': `
                    <strong>üì• Encoder Stack Details:</strong><br><br>
                    Each encoder layer contains:<br>
                    1. <strong>Multi-Head Self-Attention:</strong> Looks at other positions in input<br>
                    2. <strong>Add & Norm:</strong> Residual connection + layer normalization<br>
                    3. <strong>Feed-Forward Network:</strong> Two linear transformations with ReLU<br>
                    4. <strong>Add & Norm:</strong> Another residual connection + normalization<br><br>
                    <em>Output: Rich contextual representations of input sequence</em>
                `,
                'decoder': `
                    <strong>üì§ Decoder Stack Details:</strong><br><br>
                    Each decoder layer contains:<br>
                    1. <strong>Masked Multi-Head Attention:</strong> Prevents looking at future tokens<br>
                    2. <strong>Add & Norm:</strong> Residual connection + normalization<br>
                    3. <strong>Encoder-Decoder Attention:</strong> Attends to encoder output<br>
                    4. <strong>Add & Norm:</strong> Another residual connection<br>
                    5. <strong>Feed-Forward Network:</strong> Final processing<br>
                    6. <strong>Add & Norm:</strong> Final residual connection<br><br>
                    <em>Output: Generated sequence (e.g., translation)</em>
                `,
                'attention': `
                    <strong>üéØ Multi-Head Attention:</strong><br><br>
                    Instead of single attention:<br>
                    ‚Ä¢ <strong>8 parallel attention heads</strong> (typically)<br>
                    ‚Ä¢ Each head learns different relationships<br>
                    ‚Ä¢ Results are concatenated and linearly transformed<br><br>
                    Benefits:<br>
                    ‚Ä¢ Head 1: Syntactic relationships<br>
                    ‚Ä¢ Head 2: Semantic relationships<br>
                    ‚Ä¢ Head 3: Long-distance dependencies<br>
                    ‚Ä¢ ... and so on<br><br>
                    <em>Like having multiple experts examining the same data!</em>
                `
            };
            
            output.innerHTML = explanations[component];
        }

        function visualizeAttention() {
            const input = document.getElementById('attentionInput').value;
            const output = document.getElementById('attentionOutput');
            
            const words = input.split(/\s+/);
            if (words.length === 0) return;
            
            // Simple attention simulation - word focuses most on nearby words
            let visualization = '<div class="attention-visualization">';
            
            words.forEach((word, i) => {
                // Generate random attention weights favoring nearby words
                const weights = words.map((_, j) => {
                    const distance = Math.abs(i - j);
                    return Math.max(0.1, 1 - distance * 0.3 + Math.random() * 0.2);
                });
                
                // Normalize weights
                const sum = weights.reduce((a, b) => a + b, 0);
                const normalizedWeights = weights.map(w => w / sum);
                
                // Find max weight for color intensity
                const maxWeight = Math.max(...normalizedWeights);
                const intensity = Math.floor((normalizedWeights[i] / maxWeight) * 255);
                const color = `rgb(${255-intensity}, ${255-intensity}, 255)`;
                
                visualization += `<div class="attention-word" style="background-color: ${color}; border: 2px solid #667eea;">${word}</div>`;
            });
            
            visualization += '</div>';
            visualization += '<p><em>Color intensity represents attention weight (darker = higher attention)</em></p>';
            
            output.innerHTML = visualization;
        }

        function demonstrateAttentionSteps() {
            const output = document.getElementById('attentionStepsOutput');
            
            output.innerHTML = `
                <strong>üìù Step-by-Step Attention Calculation:</strong><br><br>
                
                <strong>Step 1: Create Q, K, V matrices</strong><br>
                ‚Ä¢ Query (Q) = Input √ó W_Q<br>
                ‚Ä¢ Key (K) = Input √ó W_K<br>
                ‚Ä¢ Value (V) = Input √ó W_V<br><br>
                
                <strong>Step 2: Calculate attention scores</strong><br>
                ‚Ä¢ Scores = Q √ó K^T<br>
                ‚Ä¢ Example: [0.8, 0.2, 0.1, 0.9]<br><br>
                
                <strong>Step 3: Scale by ‚àöd_k</strong><br>
                ‚Ä¢ Scaled = Scores / ‚àö64 = Scores / 8<br>
                ‚Ä¢ Prevents extremely small gradients<br><br>
                
                <strong>Step 4: Apply softmax</strong><br>
                ‚Ä¢ Weights = softmax([0.1, 0.025, 0.0125, 0.1125])<br>
                ‚Ä¢ Weights = [0.28, 0.23, 0.22, 0.27]<br><br>
                
                <strong>Step 5: Weighted sum of values</strong><br>
                ‚Ä¢ Output = Weights √ó V<br>
                ‚Ä¢ Final contextualized representation!<br><br>
                
                <em>This happens for every position in parallel!</em>
            `;
        }

        function demonstrateMultiHead() {
            const numHeads = document.getElementById('numHeads').value;
            const output = document.getElementById('multiHeadOutput');
            
            let result = `<strong>üé≠ Multi-Head Attention with ${numHeads} heads:</strong><br><br>`;
            
            for (let i = 1; i <= numHeads; i++) {
                const focus = ['syntactic relations', 'semantic meaning', 'long-distance deps', 'local context', 'coreference', 'temporal relations', 'causal relations', 'thematic roles'][i-1] || 'specialized patterns';
                result += `<strong>Head ${i}:</strong> Focuses on ${focus}<br>`;
            }
            
            result += `<br><strong>Process:</strong><br>`;
            result += `1. Each head computes its own Q, K, V matrices<br>`;
            result += `2. Each head produces attention output independently<br>`;
            result += `3. All head outputs are concatenated<br>`;
            result += `4. Final linear transformation combines information<br><br>`;
            result += `<em>Result: Rich, multi-faceted understanding of relationships!</em>`;
            
            output.innerHTML = result;
        }

        function summarizeText() {
            const input = document.getElementById('summaryInput').value;
            const output = document.getElementById('summaryOutput');
            
            // Simple extractive summarization simulation
            const sentences = input.split(/[.!?]+/).filter(s => s.trim().length > 0);
            
            if (sentences.length <= 2) {
                output.innerHTML = '<em>Text is already concise!</em>';
                return;
            }
            
            // Score sentences based on keyword frequency (simplified)
            const words = input.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
            const wordFreq = {};
            words.forEach(word => {
                if (word.length > 3) { // Ignore short words
                    wordFreq[word] = (wordFreq[word] || 0) + 1;
                }
            });
            
            const sentenceScores = sentences.map(sentence => {
                const sentWords = sentence.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
                const score = sentWords.reduce((sum, word) => sum + (wordFreq[word] || 0), 0);
                return { sentence: sentence.trim() + '.', score: score / sentWords.length };
            });
            
            // Take top 2 sentences
            const topSentences = sentenceScores
                .sort((a, b) => b.score - a.score)
                .slice(0, 2)
                .map(item => item.sentence);
            
            output.innerHTML = `
                <strong>üìù Summary:</strong><br>
                ${topSentences.join(' ')}<br><br>
                <em>Note: This is a simplified extractive summarization for demonstration.</em>
            `;
        }

        function performNER() {
            const input = document.getElementById('nerInput').value;
            const output = document.getElementById('nerOutput');
            
            // Simple named entity recognition patterns
            const patterns = [
                { regex: /\b[A-Z][a-z]+ Inc\.\b/g, type: 'ORGANIZATION', color: '#ff6b6b' },
                { regex: /\b[A-Z][a-z]+ [A-Z][a-z]+\b/g, type: 'PERSON', color: '#4ecdc4' },
                { regex: /\b[A-Z][a-z]+(?:, [A-Z][a-z]+)?\b/g, type: 'LOCATION', color: '#45b7d1' },
                { regex: /\b\d{4}\b/g, type: 'DATE', color: '#96ceb4' }
            ];
            
            let processedText = input;
            const entities = [];
            
            patterns.forEach(pattern => {
                const matches = input.match(pattern.regex);
                if (matches) {
                    matches.forEach(match => {
                        entities.push({ text: match, type: pattern.type, color: pattern.color });
                        processedText = processedText.replace(match, 
                            `<span style="background-color: ${pattern.color}; padding: 2px 4px; border-radius: 3px; color: white; font-weight: bold;">${match} (${pattern.type})</span>`
                        );
                    });
                }
            });
            
            let result = `<strong>üè∑Ô∏è Named Entities Found:</strong><br><br>`;
            result += processedText + '<br><br>';
            
            if (entities.length > 0) {
                result += '<strong>Entity List:</strong><br>';
                entities.forEach(entity => {
                    result += `‚Ä¢ <span style="color: ${entity.color}; font-weight: bold;">${entity.text}</span> - ${entity.type}<br>`;
                });
            } else {
                result += '<em>No named entities detected with current patterns.</em>';
            }
            
            output.innerHTML = result;
        }

        function answerQuestion() {
            const context = document.getElementById('qaContext').value;
            const question = document.getElementById('qaQuestion').value;
            const output = document.getElementById('qaOutput');
            
            // Simple keyword-based question answering simulation
            const contextLower = context.toLowerCase();
            const questionLower = question.toLowerCase();
            
            let answer = "I couldn't find a specific answer in the context.";
            
            // Simple pattern matching for demo
            if (questionLower.includes('when') && questionLower.includes('introduced')) {
                const yearMatch = context.match(/\b(19|20)\d{2}\b/);
                if (yearMatch) {
                    answer = `The Transformer model was introduced in ${yearMatch[0]}.`;
                }
            } else if (questionLower.includes('what') && questionLower.includes('transformer')) {
                const sentences = context.split(/[.!?]+/);
                const relevantSentence = sentences.find(s => s.toLowerCase().includes('transformer'));
                if (relevantSentence) {
                    answer = relevantSentence.trim() + '.';
                }
            } else if (questionLower.includes('used for') || questionLower.includes('tasks')) {
                const tasks = context.match(/(?:tasks such as|for tasks|including) ([^.]+)/i);
                if (tasks) {
                    answer = `Transformers are used for ${tasks[1]}.`;
                }
            }
            
            output.innerHTML = `
                <strong>‚ùì Question:</strong> ${question}<br>
                <strong>üí° Answer:</strong> ${answer}<br><br>
                <em>Note: This is a simplified keyword-based QA system for demonstration.</em>
            `;
        }

        function checkAnswer(element, isCorrect) {
            // Store original text if not already stored
            if (!element.dataset.originalText) {
                element.dataset.originalText = element.textContent.trim();
            }
            
            // Remove previous selections and reset text
            element.parentNode.querySelectorAll('.quiz-option').forEach(option => {
                option.classList.remove('correct', 'incorrect');
                if (option.dataset.originalText) {
                    option.textContent = option.dataset.originalText;
                }
            });
            
            // Mark the selected answer
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent = element.dataset.originalText + ' ‚úÖ Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent = element.dataset.originalText + ' ‚ùå Incorrect';
                
                // Show correct answer
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(option => {
                        const onclickStr = option.getAttribute('onclick') || '';
                        return onclickStr.includes('true');
                    });
                if (correctOption) {
                    correctOption.classList.add('correct');
                    correctOption.textContent = correctOption.dataset.originalText + ' ‚úÖ Correct Answer';
                }
            }
        }

            // Also add this initialization function to store original text
        document.addEventListener('DOMContentLoaded', function() {
            // Store original text for all quiz options
            document.querySelectorAll('.quiz-option').forEach(option => {
                option.dataset.originalText = option.textContent.trim();
            });
            
            // Your existing DOMContentLoaded code continues here...
            showSection('intro');
            
            // Add some interactive enhancements
            const demoInputs = document.querySelectorAll('.demo-input');
            demoInputs.forEach(input => {
                input.addEventListener('keypress', function(e) {
                    if (e.key === 'Enter') {
                        const button = this.parentNode.querySelector('.demo-btn');
                        if (button) button.click();
                    }
                });
            });
            
            // Add smooth scrolling for better UX
            document.querySelectorAll('.nav-btn').forEach(btn => {
                btn.addEventListener('click', function() {
                    document.querySelector('.container').scrollIntoView({ 
                        behavior: 'smooth',
                        block: 'start'
                    });
                });
            });
        });

        document.addEventListener('DOMContentLoaded', function() {
            // Add some easter egg interactions
            let clickCount = 0;
            const headerH1 = document.querySelector('.header h1');
            if (headerH1) {
                headerH1.addEventListener('click', function() {
                    clickCount++;
                    if (clickCount === 5) {
                        this.innerHTML = 'ü§ñ You found the easter egg! Welcome to Advanced NLP! üéâ';
                        setTimeout(() => {
                            this.innerHTML = 'ü§ñ Complete Interactive NLP Course';
                            clickCount = 0;
                        }, 3000);
                    }
                });
            }

            // Add keyboard navigation
            document.addEventListener('keydown', function(e) {
                if (e.key === 'ArrowRight') {
                    const currentIndex = sections.indexOf(currentSection);
                    if (currentIndex < sections.length - 1) {
                        const nextSection = sections[currentIndex + 1];
                        showSection(nextSection);
                        document.querySelector(`[onclick="showSection('${nextSection}')"]`).classList.add('active');
                    }
                } else if (e.key === 'ArrowLeft') {
                    const currentIndex = sections.indexOf(currentSection);
                    if (currentIndex > 0) {
                        const prevSection = sections[currentIndex - 1];
                        showSection(prevSection);
                        document.querySelector(`[onclick="showSection('${prevSection}')"]`).classList.add('active');
                    }
                }
            });

            // Add tooltips for technical terms
            const technicalTerms = {
                'transformer': 'A neural network architecture that relies entirely on attention mechanisms',
                'attention': 'A mechanism that allows the model to focus on relevant parts of the input',
                'embedding': 'Dense vector representations of words that capture semantic meaning',
                'encoder': 'The part of the model that processes and understands the input',
                'decoder': 'The part of the model that generates the output sequence'
            };

            Object.keys(technicalTerms).forEach(term => {
                const regex = new RegExp(`\\b${term}\\b`, 'gi');
                document.querySelectorAll('.section p, .section li').forEach(element => {
                    element.innerHTML = element.innerHTML.replace(regex, 
                        `<span class="tooltip" title="${technicalTerms[term]}">${term}</span>`
                    );
                });
            });

            // Add CSS for tooltips
            const tooltipStyle = document.createElement('style');
            tooltipStyle.textContent = `
                .tooltip {
                    position: relative;
                    border-bottom: 1px dotted #667eea;
                    cursor: help;
                }
                
                .tooltip:hover::after {
                    content: attr(title);
                    position: absolute;
                    bottom: 100%;
                    left: 50%;
                    transform: translateX(-50%);
                    background: #333;
                    color: white;
                    padding: 5px 10px;
                    border-radius: 4px;
                    font-size: 12px;
                    white-space: nowrap;
                    z-index: 1000;
                    box-shadow: 0 2px 8px rgba(0,0,0,0.2);
                }
                
                .tooltip:hover::before {
                    content: '';
                    position: absolute;
                    bottom: 94%;
                    left: 50%;
                    transform: translateX(-50%);
                    border: 5px solid transparent;
                    border-top-color: #333;
                    z-index: 1000;
                }
            `;
            document.head.appendChild(tooltipStyle);

            // Performance optimization: Lazy load heavy content
            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        const section = entry.target;
                        if (!section.dataset.loaded) {
                            // Load heavy content here if needed
                            section.dataset.loaded = 'true';
                        }
                    }
                });
            });

            document.querySelectorAll('.section').forEach(section => {
                observer.observe(section);
            });

            // Add completion tracking
            const completedSections = new Set();

            function markSectionComplete(sectionId) {
                completedSections.add(sectionId);
                const button = document.querySelector(`[onclick="showSection('${sectionId}')"]`);
                if (button && !button.querySelector('.checkmark')) {
                    button.innerHTML += ' ‚úÖ';
                    button.querySelector('.checkmark')?.classList.add('checkmark');
                }
                
                // Update overall progress
                const overallProgress = (completedSections.size / sections.length) * 100;
                document.getElementById('progressFill').style.width = overallProgress + '%';
                
                // Congratulations message when all sections completed
                if (completedSections.size === sections.length) {
                    setTimeout(() => {
                        alert('üéâ Congratulations! You have completed the entire NLP course! üéä');
                    }, 500);
                }
            }

            // Auto-mark sections as complete when user spends time on them
            let sectionStartTime = Date.now();
            let timeThreshold = 30000; // 30 seconds

            function trackSectionTime() {
                const timeSpent = Date.now() - sectionStartTime;
                if (timeSpent > timeThreshold) {
                    markSectionComplete(currentSection);
                }
            }

            // Track time spent on each section
            setInterval(trackSectionTime, 5000);

            // Reset timer when section changes
            const originalShowSection = showSection;
            showSection = function(sectionId) {
                sectionStartTime = Date.now();
                originalShowSection(sectionId);
            };

            // Add final interactive summary
            function generateCourseSummary() {
                return `
                    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0;">
                        <h3>üéì Course Summary</h3>
                        <p>You've learned about:</p>
                        <ul>
                            <li>üìù Text representation techniques (BoW, TF-IDF)</li>
                            <li>üß† Word embeddings (Word2Vec, GloVe, FastText)</li>
                            <li>üòä Sentiment analysis applications and challenges</li>
                            <li>üîÑ Sequence-to-sequence model architectures</li>
                            <li>ü§ñ Transformer models and their innovations</li>
                            <li>üéØ Self-attention mechanisms and multi-head attention</li>
                            <li>üöÄ Modern NLP applications and future directions</li>
                        </ul>
                        <p><strong>Next Steps:</strong> Practice with real datasets, explore frameworks like Hugging Face Transformers, and stay updated with the latest research!</p>
                    </div>
                `;
            }

            // Add the summary to the applications section
            const applicationsSection = document.getElementById('applications');
            if (applicationsSection) {
                applicationsSection.innerHTML += generateCourseSummary();
            }
        });

        // Add dynamic content loading simulation
        function simulateLoading(outputElement, content, delay = 50) {
            outputElement.innerHTML = 'Loading...';
            setTimeout(() => {
                outputElement.innerHTML = content;
            }, delay);
        }

        // Enhanced attention visualization with animation
        function animateAttention() {
            const words = ['The', 'cat', 'sat', 'on', 'the', 'mat'];
            const attentionDiv = document.createElement('div');
            attentionDiv.className = 'attention-visualization';
            
            words.forEach((word, i) => {
                const wordDiv = document.createElement('div');
                wordDiv.className = 'attention-word';
                wordDiv.textContent = word;
                wordDiv.style.animationDelay = `${i * 0.2}s`;
                attentionDiv.appendChild(wordDiv);
            });
            
            return attentionDiv;
        }
    </script>
</body>
</html>