<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Complete Introduction to Machine Learning - Ali Zanganeh</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: white;
            margin-top: 20px;
            margin-bottom: 20px;
            border-radius: 15px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
        }
        
        .header {
            text-align: center;
            padding: 40px 0;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 15px;
            margin-bottom: 30px;
        }
        
        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        
        .header p {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .section {
            margin: 40px 0;
            padding: 30px;
            background: #f8f9fa;
            border-radius: 12px;
            border-left: 5px solid #667eea;
        }
        
        .card {
            background: white;
            padding: 25px;
            border-radius: 12px;
            margin: 20px 0;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        
        .card:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 15px rgba(0,0,0,0.15);
        }
        
        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 25px;
            margin: 25px 0;
        }
        
        .code-block {
            background: #1e1e1e;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            overflow-x: auto;
            margin: 15px 0;
            position: relative;
        }
        
        .code-block::before {
            content: "Python Code";
            position: absolute;
            top: -10px;
            right: 10px;
            background: #667eea;
            color: white;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.7em;
        }
        
        .highlight-box {
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid;
        }
        
        .info { background: #e3f2fd; border-left-color: #2196f3; }
        .tip { background: #e8f5e8; border-left-color: #4caf50; }
        .warning { background: #fff3e0; border-left-color: #ff9800; }
        .danger { background: #ffebee; border-left-color: #f44336; }
        
        .interactive-demo {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 12px;
            margin: 25px 0;
        }
        
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        
        .comparison-table th, .comparison-table td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        
        .comparison-table th {
            background: #667eea;
            color: white;
        }
        
        .flowchart {
            display: flex;
            flex-direction: column;
            gap: 15px;
            margin: 25px 0;
        }
        
        .flow-step {
            display: flex;
            align-items: center;
            padding: 15px;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .step-number {
            background: #667eea;
            color: white;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-right: 20px;
            font-weight: bold;
        }
        
        .tabs {
            display: flex;
            margin-bottom: 20px;
        }
        
        .tab {
            padding: 10px 20px;
            background: #e0e0e0;
            border: none;
            cursor: pointer;
            border-radius: 8px 8px 0 0;
            margin-right: 2px;
        }
        
        .tab.active {
            background: #667eea;
            color: white;
        }
        
        .tab-content {
            display: none;
            padding: 20px;
            background: white;
            border-radius: 0 8px 8px 8px;
        }
        
        .tab-content.active {
            display: block;
        }
        
        h2 {
            color: #667eea;
            margin: 30px 0 20px 0;
            font-size: 2em;
        }
        
        h3 {
            color: #764ba2;
            margin: 25px 0 15px 0;
        }
        
        .emoji {
            font-size: 1.2em;
            margin-right: 8px;
        }
        
        .progress-bar {
            width: 100%;
            height: 20px;
            background: #e0e0e0;
            border-radius: 10px;
            overflow: hidden;
            margin: 10px 0;
        }
        
        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #667eea, #764ba2);
            transition: width 0.3s ease;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1><span class="emoji">ü§ñ</span>Machine Learning Masterclass</h1>
            <p>Chapter 1: From Zero to ML Hero - Complete Interactive Guide</p>
            <div class="progress-bar">
                <div class="progress-fill" style="width: 100%"></div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üéØ</span>What You'll Master Today</h2>
            <div class="grid">
                <div class="card">
                    <h3>üß† Core Concepts</h3>
                    <ul>
                        <li>Deep understanding of ML types and when to use each</li>
                        <li>Mathematical intuition behind algorithms</li>
                        <li>Real-world problem identification</li>
                        <li>Data quality assessment</li>
                    </ul>
                </div>
                <div class="card">
                    <h3>üíª Technical Skills</h3>
                    <ul>
                        <li>Complete Python ML toolkit mastery</li>
                        <li>Advanced data preprocessing techniques</li>
                        <li>Model evaluation and interpretation</li>
                        <li>Production-ready code practices</li>
                    </ul>
                </div>
                <div class="card">
                    <h3>üöÄ Practical Projects</h3>
                    <ul>
                        <li>End-to-end Iris classification</li>
                        <li>Real estate price prediction</li>
                        <li>Customer segmentation analysis</li>
                        <li>Time series forecasting basics</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">ü§ñ</span>Machine Learning: The Complete Picture</h2>
            
            <div class="highlight-box info">
                <h3>üéì The Fundamental Definition</h3>
                <p><strong>Machine Learning</strong> is the science of programming computers to learn patterns from data and make predictions or decisions without being explicitly programmed for every possible scenario.</p>
            </div>

            <div class="card">
                <h3>üîç The Paradigm Shift</h3>
                <div class="grid">
                    <div style="background: #ffebee; padding: 20px; border-radius: 8px;">
                        <h4>‚ùå Traditional Programming</h4>
                        <p><strong>Input:</strong> Data + Rules (Code)</p>
                        <p><strong>Output:</strong> Answers</p>
                        <p><strong>Problem:</strong> We must anticipate every scenario and write explicit rules for each case.</p>
                    </div>
                    <div style="background: #e8f5e8; padding: 20px; border-radius: 8px;">
                        <h4>‚úÖ Machine Learning</h4>
                        <p><strong>Input:</strong> Data + Answers (Examples)</p>
                        <p><strong>Output:</strong> Rules (Model)</p>
                        <p><strong>Advantage:</strong> The system discovers patterns and rules automatically from examples.</p>
                    </div>
                </div>
            </div>

            <div class="interactive-demo" style="color: #fff;"></div>
                <h3>üåü Real-World Impact Stories</h3>
                <div class="tabs">
                    <button class="tab active" onclick="showTab('healthcare')">üè• Healthcare</button>
                    <button class="tab" onclick="showTab('finance')">üí∞ Finance</button>
                    <button class="tab" onclick="showTab('tech')">üì± Technology</button>
                    <button class="tab" onclick="showTab('transport')">üöó Transportation</button>
                </div>
                
                <div id="healthcare" class="tab-content active">
                    <h4>Revolutionary Medical Diagnostics</h4>
                    <p><strong>Problem:</strong> Radiologists can miss up to 30% of early-stage cancers in mammograms.</p>
                    <p><strong>ML Solution:</strong> Google's AI system can detect breast cancer with 94.5% accuracy, reducing false negatives by 9.4%.</p>
                    <p><strong>Impact:</strong> Earlier detection saves thousands of lives annually and reduces healthcare costs by billions.</p>
                </div>
                
                <div id="finance" class="tab-content">
                    <h4>Fraud Detection & Risk Assessment</h4>
                    <p><strong>Problem:</strong> Credit card fraud costs $28 billion annually worldwide.</p>
                    <p><strong>ML Solution:</strong> Real-time transaction analysis using ensemble methods and neural networks.</p>
                    <p><strong>Impact:</strong> Fraud detection accuracy improved from 60% to 99.9%, with false positives down 70%.</p>
                </div>
                
                <div id="tech" class="tab-content">
                    <h4>Personalized User Experiences</h4>
                    <p><strong>Problem:</strong> Information overload - Netflix has 15,000+ titles, YouTube uploads 500 hours/minute.</p>
                    <p><strong>ML Solution:</strong> Collaborative filtering and deep learning recommendation systems.</p>
                    <p><strong>Impact:</strong> 80% of Netflix viewing comes from recommendations, saving users 1+ hour daily in search time.</p>
                </div>
                
                <div id="transport" class="tab-content">
                    <h4>Autonomous Vehicle Safety</h4>
                    <p><strong>Problem:</strong> Human error causes 94% of serious traffic crashes (1.35M deaths annually).</p>
                    <p><strong>ML Solution:</strong> Computer vision, sensor fusion, and reinforcement learning for navigation.</p>
                    <p><strong>Impact:</strong> Waymo's self-driving cars are 2.3x safer than human drivers per mile driven.</p>
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üìä</span>The Three Pillars of Machine Learning</h2>
            
            <div class="card">
                <h3><span class="emoji">üßë‚Äçüè´</span>Supervised Learning: Learning with a Teacher</h3>
                
                <div class="highlight-box info">
                    <h4>üéØ Core Concept</h4>
                    <p>Like a student learning with a teacher who provides correct answers. The algorithm learns by studying input-output pairs (labeled examples) and then makes predictions on new, unseen data.</p>
                </div>

                <div class="grid">
                    <div class="card">
                        <h4>üî¢ Regression: Predicting Numbers</h4>
                        <p><strong>Goal:</strong> Predict continuous numerical values</p>
                        <p><strong>Mathematical Foundation:</strong> Finding the best function f(x) that maps inputs to continuous outputs</p>
                        
                        <h5>Real Examples with Business Impact:</h5>
                        <ul>
                            <li><strong>Real Estate:</strong> Zillow's Zestimate uses 100+ features to predict home values (¬±1.9% median error)</li>
                            <li><strong>Stock Trading:</strong> Renaissance Technologies' Medallion Fund uses ML for 66% annual returns</li>
                            <li><strong>Energy:</strong> Google reduced data center cooling costs by 40% using ML temperature prediction</li>
                            <li><strong>Retail:</strong> Amazon's demand forecasting prevents $1.1B in overstock annually</li>
                        </ul>

                        <div class="highlight-box tip">
                            <strong>üí° Key Insight:</strong> Regression answers "How much?" or "How many?" questions. If you can measure it with a ruler, scale, or stopwatch, it's likely a regression problem.
                        </div>
                    </div>

                    <div class="card">
                        <h4>üè∑Ô∏è Classification: Predicting Categories</h4>
                        <p><strong>Goal:</strong> Predict discrete categories or classes</p>
                        <p><strong>Mathematical Foundation:</strong> Finding decision boundaries that separate different classes in feature space</p>

                        <h5>Classification Types:</h5>
                        <div class="grid" style="grid-template-columns: 1fr 1fr;">
                            <div style="background: #e3f2fd; padding: 15px; border-radius: 8px;">
                                <h6>Binary Classification</h6>
                                <p>Two possible outcomes (Yes/No, Spam/Ham, Fraud/Legitimate)</p>
                                <ul>
                                    <li>Email spam detection (99.9% accuracy)</li>
                                    <li>Medical diagnosis (Cancer/No Cancer)</li>
                                    <li>Credit approval (Approve/Deny)</li>
                                </ul>
                            </div>
                            <div style="background: #fff3e0; padding: 15px; border-radius: 8px;">
                                <h6>Multi-class Classification</h6>
                                <p>Multiple possible outcomes (A, B, C...)</p>
                                <ul>
                                    <li>Image classification (Cat/Dog/Bird)</li>
                                    <li>Handwriting recognition (0-9 digits)</li>
                                    <li>Sentiment analysis (Positive/Neutral/Negative)</li>
                                </ul>
                                <p>Multiple possible outcomes (A, B, C, D...)</p>
                                <ul>
                                    <li>Image recognition (1000+ object classes)</li>
                                    <li>Language detection (100+ languages)</li>
                                    <li>Product categorization (Electronics, Clothing, Books...)</li>
                                </ul>
                            </div>
                        </div>

                        <div class="highlight-box warning">
                            <strong>‚ö†Ô∏è Common Pitfall:</strong> Class imbalance! If 99% of emails are legitimate and 1% are spam, a naive model could achieve 99% accuracy by always predicting "legitimate" - but it would be useless!
                        </div>
                    </div>
                </div>

                <div class="code-block">
<pre><code class="language-python">
# Supervised Learning Intuition: House Price Prediction
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Let's understand how supervised learning works step by step
# Imagine we're a real estate agent trying to price houses

# Historical data (our "teacher")
house_sizes = np.array([1000, 1500, 2000, 2500, 3000, 3500]).reshape(-1, 1)  # sq ft
house_prices = np.array([200000, 300000, 400000, 500000, 600000, 700000])    # dollars

print("üè† Historical Data (What we learn from):")
for size, price in zip(house_sizes.flatten(), house_prices):
    print(f"   {size:,} sq ft ‚Üí ${price:,}")

# Create and train our model (learning phase)
model = LinearRegression()
model.fit(house_sizes, house_prices)

# The model has learned: Price ‚âà $200/sq ft (the pattern!)
print(f"\nüß† What the model learned:")
print(f"   Price per sq ft: ${model.coef_[0]:.2f}")
print(f"   Base price: ${model.intercept_:,.2f}")

# Now predict prices for new houses (applying what we learned)
new_houses = np.array([[1800], [2200], [4000]])
predicted_prices = model.predict(new_houses)

print("\nüîÆ Predictions for new houses:")
for size, price in zip(new_houses.flatten(), predicted_prices):
    print(f"   {size:,} sq ft ‚Üí ${price:,.2f}")

# This is the essence of supervised learning:
# 1. Learn patterns from labeled examples
# 2. Apply those patterns to make predictions
</code></pre>
                </div>
            </div>

            <div class="card">
                <h3><span class="emoji">üîç</span>Unsupervised Learning: Finding Hidden Patterns</h3>
                
                <div class="highlight-box info">
                    <h4>üéØ Core Concept</h4>
                    <p>Like an explorer discovering hidden treasures. No teacher, no right answers - just data with hidden structures waiting to be uncovered. The algorithm must find patterns, groupings, and relationships on its own.</p>
                </div>

                <div class="grid">
                    <div class="card">
                        <h4>üéØ Clustering: Finding Natural Groups</h4>
                        <p><strong>Question it answers:</strong> "What natural groups exist in my data?"</p>
                        
                        <h5>Business Applications:</h5>
                        <ul>
                            <li><strong>Customer Segmentation:</strong> Starbucks identified 5 customer types, increasing targeted marketing ROI by 300%</li>
                            <li><strong>Market Research:</strong> Netflix discovered 76,897 unique viewer "taste clusters" for personalization</li>
                            <li><strong>Anomaly Detection:</strong> Banks detect unusual transaction patterns (fraud prevention)</li>
                            <li><strong>Gene Analysis:</strong> Identify disease subtypes for personalized medicine</li>
                        </ul>

                        <div class="highlight-box tip">
                            <strong>üí° Real Example:</strong> Spotify's "Discover Weekly" uses clustering to group similar songs and find music you might like based on listening patterns of users similar to you.
                        </div>
                    </div>

                    <div class="card">
                        <h4>üìè Dimensionality Reduction: Simplifying Complexity</h4>
                        <p><strong>Question it answers:</strong> "What are the most important patterns in my high-dimensional data?"</p>
                        
                        <h5>Why It Matters:</h5>
                        <ul>
                            <li><strong>Visualization:</strong> Plot 1000-dimensional data in 2D/3D</li>
                            <li><strong>Storage:</strong> Compress data while preserving important information</li>
                            <li><strong>Speed:</strong> Faster algorithms with fewer dimensions</li>
                            <li><strong>Noise Reduction:</strong> Remove irrelevant features</li>
                        </ul>

                        <div class="highlight-box warning">
                            <strong>‚ö†Ô∏è The Curse of Dimensionality:</strong> As dimensions increase, data becomes sparse. In 1000 dimensions, most points are equally far apart, making patterns hard to find!
                        </div>
                    </div>
                </div>

                <div class="code-block">
<pre><code class="language-python">
# Unsupervised Learning Example: Customer Segmentation
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# Simulate customer data: spending habits and visit frequency
np.random.seed(42)
# Generate synthetic customer data
customer_spending = np.random.normal(100, 30, 200)  # Average spending
visit_frequency = np.random.normal(5, 2, 200)       # Visits per month

# Combine into feature matrix
customer_data = np.column_stack([customer_spending, visit_frequency])

print("üõçÔ∏è Customer Behavior Analysis (Unsupervised Learning)")
print("We have customer data but don't know what groups exist...")

# Apply K-means clustering to find customer segments
kmeans = KMeans(n_clusters=3, random_state=42)
customer_segments = kmeans.fit_predict(customer_data)

# Analyze the discovered segments
print("\nüéØ Discovered Customer Segments:")
for i in range(3):
    segment_data = customer_data[customer_segments == i]
    avg_spending = segment_data[:, 0].mean()
    avg_visits = segment_data[:, 1].mean()
    size = len(segment_data)
    
    if avg_spending > 120 and avg_visits > 6:
        segment_name = "üíé VIP Customers"
    elif avg_spending > 100:
        segment_name = "üéØ Regular Customers"
    else:
        segment_name = "üå± Budget-Conscious"
    
    print(f"   Segment {i+1} - {segment_name}:")
    print(f"     ‚Ä¢ Size: {size} customers ({size/200*100:.1f}%)")
    print(f"     ‚Ä¢ Avg Spending: ${avg_spending:.2f}")
    print(f"     ‚Ä¢ Avg Visits: {avg_visits:.1f}/month")
    print()

print("üí° Business Insight: Now we can create targeted marketing campaigns!")
print("   ‚Ä¢ VIP: Exclusive offers and premium services")
print("   ‚Ä¢ Regular: Loyalty programs and upselling")
print("   ‚Ä¢ Budget: Discounts and value propositions")
</code></pre>
                </div>
            </div>

            <div class="card">
                <h3><span class="emoji">üéÆ</span>Reinforcement Learning: Learning Through Trial and Error</h3>
                
                <div class="highlight-box info">
                    <h4>üéØ Core Concept</h4>
                    <p>Like training a pet or learning to ride a bike. An agent learns by interacting with an environment, receiving rewards for good actions and penalties for bad ones. Over time, it discovers the optimal strategy.</p>
                </div>

                <div class="grid">
                    <div class="card">
                        <h4>üèóÔ∏è Key Components</h4>
                        <ul>
                            <li><strong>Agent:</strong> The decision maker (AI player, robot, trading algorithm)</li>
                            <li><strong>Environment:</strong> The world/context (game board, real world, stock market)</li>
                            <li><strong>Actions:</strong> What the agent can do (move, buy/sell, accelerate)</li>
                            <li><strong>States:</strong> Current situation (game position, sensor readings, portfolio value)</li>
                            <li><strong>Rewards:</strong> Feedback signal (+1 for winning, -1 for losing, profit/loss)</li>
                            <li><strong>Policy:</strong> Strategy for choosing actions (the "brain" of the agent)</li>
                        </ul>
                    </div>

                    <div class="card">
                        <h4>üåü Breakthrough Applications</h4>
                        <ul>
                            <li><strong>Gaming:</strong> AlphaGo defeated world Go champion (10^170 possible games!)</li>
                            <li><strong>Robotics:</strong> Boston Dynamics' robots learn to walk and recover from falls</li>
                            <li><strong>Autonomous Vehicles:</strong> Learning optimal driving policies in simulation</li>
                            <li><strong>Finance:</strong> Algorithmic trading that adapts to market conditions</li>
                            <li><strong>Resource Management:</strong> Google's data centers use 40% less energy</li>
                            <li><strong>Personalization:</strong> Dynamic content recommendation (YouTube, TikTok)</li>
                        </ul>
                    </div>
                </div>

                <div class="highlight-box danger">
                    <strong>üöÄ Advanced Warning:</strong> RL is the most complex ML type. It requires significant computational resources, careful reward design, and extensive testing. Start with supervised learning first!
                </div>

                <div class="code-block">
<pre><code class="language-python">
# Reinforcement Learning Concept: Simple Trading Agent
import numpy as np
import random

class SimpleTrader:
    """A basic RL trading agent that learns to buy/sell stocks"""
    
    def __init__(self):
        self.portfolio_value = 10000  # Starting money
        self.stock_shares = 0
        self.actions = ['buy', 'sell', 'hold']
        self.q_table = {}  # Stores learned values for state-action pairs
        self.learning_rate = 0.1
        self.epsilon = 0.1  # Exploration rate
        
    def get_state(self, price_trend):
        """Simplified state: just the price trend"""
        if price_trend > 0.05:
            return "rising"
        elif price_trend < -0.05:
            return "falling"
        else:
            return "stable"
    
    def choose_action(self, state):
        """Choose action using epsilon-greedy strategy"""
        if random.random() < self.epsilon:
            return random.choice(self.actions)  # Explore
        else:
            # Exploit: choose best known action for this state
            if state not in self.q_table:
                self.q_table[state] = {action: 0 for action in self.actions}
            return max(self.q_table[state], key=self.q_table[state].get)
    
    def update_q_value(self, state, action, reward, next_state):
        """Update Q-value based on experience"""
        if state not in self.q_table:
            self.q_table[state] = {action: 0 for action in self.actions}
        if next_state not in self.q_table:
            self.q_table[next_state] = {action: 0 for action in self.actions}
        
        # Q-learning update rule
        best_next_action = max(self.q_table[next_state].values())
        current_q = self.q_table[state][action]
        
        # Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]
        self.q_table[state][action] = current_q + self.learning_rate * (
            reward + 0.9 * best_next_action - current_q
        )

print("ü§ñ Reinforcement Learning Trading Agent")
print("Learning to trade through experience...")

# Simulate the learning process
trader = SimpleTrader()
prices = [100, 102, 98, 105, 103, 110, 95, 108, 112, 98]  # Sample price data

for day, price in enumerate(prices[:-1]):
    # Calculate price trend
    next_price = prices[day + 1]
    trend = (next_price - price) / price
    current_state = trader.get_state(trend)
    
    # Agent chooses action
    action = trader.choose_action(current_state)
    
    # Execute action and calculate reward
    if action == 'buy' and trader.portfolio_value >= price:
        trader.stock_shares += 1
        trader.portfolio_value -= price
        reward = (next_price - price)  # Profit/loss from this trade
    elif action == 'sell' and trader.stock_shares > 0:
        trader.stock_shares -= 1
        trader.portfolio_value += price
        reward = 10  # Small reward for selling
    else:
        reward = 0  # No action taken
    
    # Learn from this experience
    next_state = trader.get_state((prices[day + 2] - next_price) / next_price if day + 2 < len(prices) else 0)
    trader.update_q_value(current_state, action, reward, next_state)
    
    print(f"Day {day+1}: Price=${price:.2f}, State={current_state}, Action={action}, Reward={reward:.2f}")

final_value = trader.portfolio_value + trader.stock_shares * prices[-1]
print(f"\nüìä Final Portfolio Value: ${final_value:.2f}")
print(f"üìà Return: {(final_value - 10000) / 10000 * 100:.1f}%")
print("\nüí° The agent learned which actions work best in different market conditions!")
</code></pre>
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üêç</span>Python ML Ecosystem: Your Complete Toolkit</h2>
            
            <div class="highlight-box info">
                <h4>üéØ Why Python Dominates ML</h4>
                <p>Python powers 57% of ML projects because of its simplicity, extensive libraries, and strong community. It's the lingua franca of data science!</p>
            </div>

            <div class="grid">
                <div class="card">
                    <h3>üî¢ NumPy: The Foundation</h3>
                    <p><strong>What it does:</strong> Efficient numerical computing with N-dimensional arrays</p>
                    
                    <h4>Why NumPy is Essential:</h4>
                    <ul>
                        <li><strong>Speed:</strong> 50x faster than pure Python (C implementation)</li>
                        <li><strong>Memory:</strong> Uses 80% less memory than Python lists</li>
                        <li><strong>Vectorization:</strong> Apply operations to entire arrays at once</li>
                        <li><strong>Broadcasting:</strong> Perform operations on arrays of different shapes</li>
                    </ul>

                    <div class="code-block">
                        <pre>
<code class="language-python">
# NumPy Deep Dive: Why it's the ML Foundation
import numpy as np
import time

print("üî¢ NumPy vs Pure Python Performance Test")

# Test 1: Speed comparison
python_list = list(range(1000000))
numpy_array = np.arange(1000000)

# Pure Python multiplication
start = time.time()
python_result = [x * 2 for x in python_list]
python_time = time.time() - start

# NumPy vectorized multiplication
start = time.time()
numpy_result = numpy_array * 2
numpy_time = time.time() - start

print(f"Python list time: {python_time:.4f} seconds")
print(f"NumPy array time: {numpy_time:.4f} seconds")
print(f"NumPy is {python_time/numpy_time:.1f}x faster!")

# Test 2: Advanced operations that make ML possible
print("\nüßÆ Advanced NumPy Operations for ML:")

# Create sample data (like features in ML)
data = np.random.random((1000, 10))  # 1000 samples, 10 features
print(f"Data shape: {data.shape}")

# Statistical operations (crucial for ML)
print(f"Mean per feature: {data.mean(axis=0)[:3]}... (first 3)")
print(f"Standard deviation: {data.std(axis=0)[:3]}... (first 3)")

# Matrix operations (heart of ML algorithms)
weights = np.random.random((10, 1))
predictions = data @ weights  # Matrix multiplication
print(f"Predictions shape: {predictions.shape}")

# Broadcasting example (different sized arrays)
bias = np.array([0.5])  # Single value
predictions_with_bias = predictions + bias  # Broadcasts to all elements
print(f"Broadcasting bias to {predictions.shape} array: ‚úÖ")

# Boolean indexing (data filtering)
high_predictions = data[predictions.flatten() > 0.5]
print(f"Samples with high predictions: {len(high_predictions)}")
                        </code>
                        </pre>      
                    </div>
                </div>

                <div class="card">
                    <h3>üêº Pandas: Data Manipulation Master</h3>
                    <p><strong>What it does:</strong> Structured data analysis and manipulation</p>
                    
                    <h4>Core Capabilities:</h4>
                    <ul>
                        <li><strong>DataFrames:</strong> Excel-like data structures in code</li>
                        <li><strong>Data Cleaning:</strong> Handle missing values, duplicates, outliers</li>
                        <li><strong>Data Transformation:</strong> Group, pivot, merge, reshape</li>
                        <li><strong>I/O Operations:</strong> Read/write CSV, Excel, JSON, SQL, APIs</li>
                    </ul>

                    <div class="code-block">
<pre><code class="language-python">
# Pandas Deep Dive: Real-World Data Manipulation
import pandas as pd
import numpy as np

print("üêº Pandas for Real-World Data Analysis")

# Create realistic sample dataset
np.random.seed(42)
n_customers = 1000

# Simulate e-commerce customer data
data = {
    'customer_id': range(1, n_customers + 1),
    'age': np.random.normal(35, 12, n_customers).astype(int),
    'annual_income': np.random.normal(50000, 15000, n_customers),
    'purchase_frequency': np.random.poisson(8, n_customers),
    'total_spent': np.random.exponential(500, n_customers),
    'membership_tier': np.random.choice(['Bronze', 'Silver', 'Gold'], n_customers, p=[0.6, 0.3, 0.1]),
    'last_purchase_days': np.random.exponential(30, n_customers).astype(int),
    'customer_satisfaction': np.random.normal(3.5, 0.8, n_customers)
}

# Add some missing values (realistic scenario)
missing_indices = np.random.choice(n_customers, size=50, replace=False)
data['customer_satisfaction'][missing_indices] = np.nan

df = pd.DataFrame(data)

print("üìä Dataset Overview:")
print(f"Shape: {df.shape}")
print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB")
print("\nFirst 3 rows:")
print(df.head(3))

print("\nüîç Data Quality Assessment:")
print("Missing values per column:")
print(df.isnull().sum())

print("\nüìà Statistical Summary:")
print(df.describe())

print("\nüéØ Advanced Pandas Operations:")

# 1. Customer segmentation using groupby
print("\n1. Customer Analysis by Membership Tier:")
tier_analysis = df.groupby('membership_tier').agg({
    'annual_income': ['mean', 'median'],
    'total_spent': ['mean', 'sum'],
    'customer_satisfaction': 'mean',
    'customer_id': 'count'
}).round(2)
print(tier_analysis)

# 2. High-value customer identification
high_value_threshold = df['total_spent'].quantile(0.8)
df['is_high_value'] = df['total_spent'] > high_value_threshold

print(f"\n2. High-Value Customers (top 20%):")
print(f"Threshold: ${high_value_threshold:.2f}")
print(f"Count: {df['is_high_value'].sum()} customers")

# 3. Customer lifecycle analysis
def categorize_recency(days):
    if days <= 7:
        return 'Active'
    elif days <= 30:
        return 'Recent'
    elif days <= 90:
        return 'At Risk'
    else:
        return 'Inactive'

df['customer_status'] = df['last_purchase_days'].apply(categorize_recency)

print("\n3. Customer Status Distribution:")
print(df['customer_status'].value_counts())

# 4. Data cleaning and preprocessing
print("\n4. Data Preprocessing:")

# Handle missing values
df['customer_satisfaction'].fillna(df['customer_satisfaction'].mean(), inplace=True)
print(f"Missing values after imputation: {df.isnull().sum().sum()}")

# Feature engineering
df['spending_per_purchase'] = df['total_spent'] / df['purchase_frequency']
df['income_spending_ratio'] = df['total_spent'] / df['annual_income']

print("New engineered features created: spending_per_purchase, income_spending_ratio")

# 5. Advanced filtering and selection
premium_customers = df[
    (df['membership_tier'] == 'Gold') & 
    (df['customer_satisfaction'] > 4.0) &
    (df['is_high_value'] == True)
]

print(f"\n5. Premium Customer Segment: {len(premium_customers)} customers")
print(f"Average annual value: ${premium_customers['total_spent'].mean():.2f}")
                    </code></pre>
                    </div>  
                   </div>
                </div>

                <div class="card">
                    <h3>üìä Matplotlib & Seaborn: Data Visualization</h3>
                    <p><strong>What they do:</strong> Create insightful visualizations to understand data patterns</p>
                    
                    <h4>Why Visualization Matters in ML:</h4>
                    <ul>
                        <li><strong>Pattern Recognition:</strong> Spot trends humans might miss</li>
                        <li><strong>Outlier Detection:</strong> Identify data quality issues</li>
                        <li><strong>Feature Relationships:</strong> Understand correlations</li>
                        <li><strong>Model Evaluation:</strong> Visualize performance metrics</li>
                    </ul>

                    <div class="code-block">
                        <pre><code class="language-python">
# Data Visualization: The Art of Making Data Speak
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

print("üìä Data Visualization for Machine Learning")

# Set up the plotting style
plt.style.use('default')
sns.set_palette("husl")

# Generate sample data for demonstration
np.random.seed(42)
n_samples = 500

# Create correlated features (like real ML data)
feature1 = np.random.normal(100, 15, n_samples)
feature2 = feature1 * 0.8 + np.random.normal(0, 10, n_samples)  # Correlated
feature3 = np.random.exponential(20, n_samples)  # Different distribution
target = (feature1 * 0.3 + feature2 * 0.2 + np.random.normal(0, 5, n_samples)) > 85

# Create DataFrame
viz_data = pd.DataFrame({
    'feature1': feature1,
    'feature2': feature2, 
    'feature3': feature3,
    'target': target,
    'category': np.random.choice(['A', 'B', 'C'], n_samples, p=[0.5, 0.3, 0.2])
})

print("üé® Creating Essential ML Visualizations:")

# 1. Distribution Analysis
print("\n1. Feature Distributions (crucial for preprocessing)")
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Histogram with multiple features
axes[0,0].hist([viz_data['feature1'], viz_data['feature2']], 
               bins=30, alpha=0.7, label=['Feature1', 'Feature2'])
axes[0,0].set_title('Feature Distributions')
axes[0,0].legend()

# Box plot for outlier detection
viz_data[['feature1', 'feature2', 'feature3']].boxplot(ax=axes[0,1])
axes[0,1].set_title('Outlier Detection')

# Correlation heatmap
correlation_matrix = viz_data[['feature1', 'feature2', 'feature3']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1,0])
axes[1,0].set_title('Feature Correlations')

# Scatter plot with target coloring
scatter = axes[1,1].scatter(viz_data['feature1'], viz_data['feature2'], 
                           c=viz_data['target'], alpha=0.6, cmap='viridis')
axes[1,1].set_xlabel('Feature 1')
axes[1,1].set_ylabel('Feature 2')
axes[1,1].set_title('Feature Relationship by Target')

plt.tight_layout()
# In a real notebook, you'd see: plt.show()

print("‚úÖ Visualization insights:")
print(f"   ‚Ä¢ Feature1 & Feature2 correlation: {correlation_matrix.loc['feature1', 'feature2']:.3f}")
print(f"   ‚Ä¢ Feature3 appears to follow exponential distribution")
print(f"   ‚Ä¢ Clear separation visible between target classes")

# 2. Advanced Statistical Plots
print("\n2. Advanced Analysis Plots:")

# Pair plot for comprehensive relationship analysis
print("   ‚Ä¢ Pair plot shows all feature relationships simultaneously")
print("   ‚Ä¢ Diagonal shows distributions, off-diagonal shows relationships")

# Distribution by category
print("   ‚Ä¢ Category-wise analysis reveals segment differences:")
for cat in viz_data['category'].unique():
    cat_data = viz_data[viz_data['category'] == cat]
    mean_f1 = cat_data['feature1'].mean()
    print(f"     Category {cat}: Feature1 mean = {mean_f1:.1f}")

# 3. Model Evaluation Visualizations (preview)
print("\n3. ML Model Evaluation Plots (you'll learn these later):")
print("   ‚Ä¢ Confusion Matrix: Shows classification accuracy")
print("   ‚Ä¢ ROC Curve: Plots true vs false positive rates")
print("   ‚Ä¢ Learning Curves: Show model performance vs training size")
print("   ‚Ä¢ Feature Importance: Shows which features matter most")

print("\nüí° Visualization Best Practices:")
print("   ‚úÖ Always explore data visually before modeling")
print("   ‚úÖ Use appropriate plot types for data types")
print("   ‚úÖ Color-code by target variable when possible")
print("   ‚úÖ Look for outliers, missing values, and distributions")
print("   ‚úÖ Check correlations to avoid redundant features")
                        </code></pre>   
                    </div>
                </div>

                <div class="card">
                    <h3>ü§ñ Scikit-learn: The ML Swiss Army Knife</h3>
                    <p><strong>What it does:</strong> Comprehensive machine learning algorithms and tools</p>
                    
                    <h4>Core Components:</h4>
                    <ul>
                        <li><strong>Algorithms:</strong> 50+ ML algorithms ready to use</li>
                        <li><strong>Preprocessing:</strong> Data cleaning and transformation</li>
                        <li><strong>Model Selection:</strong> Cross-validation and hyperparameter tuning</li>
                        <li><strong>Evaluation:</strong> Comprehensive metrics for all ML tasks</li>
                    </ul>

                    <div class="code-block">
<pre><code class="language-python"> 
# Scikit-learn: Complete ML Pipeline
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression
from sklearn.metrics import classification_report, mean_squared_error, r2_score
import numpy as np

print("ü§ñ Scikit-learn: Complete ML Workflow")

# 1. Generate realistic datasets
print("\n1. Creating Realistic ML Datasets:")

# Classification dataset (predicting customer churn)
X_class, y_class = make_classification(
    n_samples=1000, n_features=10, n_informative=7, n_redundant=2,
    n_classes=2, class_sep=0.8, random_state=42
)

# Regression dataset (predicting house prices)
X_reg, y_reg = make_regression(
    n_samples=1000, n_features=8, n_informative=6, noise=0.1,
    random_state=42
)

print(f"Classification dataset: {X_class.shape} features, {len(np.unique(y_class))} classes")
print(f"Regression dataset: {X_reg.shape} features, continuous target")

# 2. Data Preprocessing Pipeline
print("\n2. Data Preprocessing (Critical for Success):")

# Split the data first (ALWAYS do this before preprocessing!)
X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class
)

X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)

# Feature scaling (essential for many algorithms)
scaler_c = StandardScaler()
X_train_c_scaled = scaler_c.fit_transform(X_train_c)
X_test_c_scaled = scaler_c.transform(X_test_c)  # Never fit on test data!

scaler_r = StandardScaler()
X_train_r_scaled = scaler_r.fit_transform(X_train_r)
X_test_r_scaled = scaler_r.transform(X_test_r)

print("‚úÖ Data preprocessing completed:")
print(f"   ‚Ä¢ Training set: {X_train_c.shape[0]} samples")
print(f"   ‚Ä¢ Test set: {X_test_c.shape[0]} samples")
print(f"   ‚Ä¢ Features scaled to mean=0, std=1")

# 3. Model Training and Evaluation
print("\n3. Model Training & Evaluation:")

# Classification Model
print("\nüìä Classification Model (Customer Churn Prediction):")
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_c_scaled, y_train_c)

# Make predictions
y_pred_c = clf.predict(X_test_c_scaled)
y_pred_proba_c = clf.predict_proba(X_test_c_scaled)

# Evaluate classification
print(f"Accuracy: {clf.score(X_test_c_scaled, y_test_c):.3f}")
print("\nDetailed Classification Report:")
print(classification_report(y_test_c, y_pred_c))

# Cross-validation for robust evaluation
cv_scores = cross_val_score(clf, X_train_c_scaled, y_train_c, cv=5)
print(f"Cross-validation scores: {cv_scores}")
print(f"Mean CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")

# Regression Model
print("\nüè† Regression Model (House Price Prediction):")
reg = LinearRegression()
reg.fit(X_train_r_scaled, y_train_r)

# Make predictions
y_pred_r = reg.predict(X_test_r_scaled)

# Evaluate regression
mse = mean_squared_error(y_test_r, y_pred_r)
r2 = r2_score(y_test_r, y_pred_r)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R¬≤ Score: {r2:.3f} (higher = better, 1.0 = perfect)")
print(f"Root MSE: {np.sqrt(mse):.2f}")

# 4. Feature Importance Analysis
print("\n4. Feature Importance (Model Interpretability):")
feature_importance = clf.feature_importances_
print("Top 5 most important features for classification:")
for i, importance in enumerate(feature_importance[:5]):
    print(f"   Feature {i}: {importance:.3f}")

# 5. Model Comparison
print("\n5. Quick Model Comparison:")
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC

models = {
    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, random_state=42),
    'SVM': SVC(random_state=42)
}

print("Model performance comparison (5-fold CV):")
for name, model in models.items():
    scores = cross_val_score(model, X_train_c_scaled, y_train_c, cv=5)
    print(f"   {name}: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")

print(f"\nüéØ Key Scikit-learn Advantages:")
print("   ‚úÖ Consistent API across all algorithms")
print("   ‚úÖ Excellent documentation and examples")
print("   ‚úÖ Built-in preprocessing and evaluation tools")
print("   ‚úÖ Efficient implementations of classic algorithms")
print("   ‚úÖ Great for learning and production use")
                        </code></pre>   
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üõ†Ô∏è</span>Data Preprocessing: The Make-or-Break Phase</h2>
            
            <div class="highlight-box danger">
                <h4>üéØ The 80/20 Rule of Machine Learning</h4>
                <p><strong>80% of ML success comes from data quality and preprocessing, only 20% from algorithm selection!</strong> This is why data scientists spend most of their time on data preparation, not modeling.</p>
            </div>

            <div class="card">
                <h3>üîç Understanding Your Data Quality Issues</h3>
                
                <div class="flowchart">
                    <div class="flow-step">
                        <div class="step-number">1</div>
                        <div>
                            <h4>Data Collection Issues</h4>
                            <p>Sensor failures, human errors, system outages, incomplete surveys</p>
                        </div>
                    </div>
                    <div class="flow-step">
                        <div class="step-number">2</div>
                        <div>
                            <h4>Integration Problems</h4>
                            <p>Different formats, units, time zones, encoding standards</p>
                        </div>
                    </div>
                    <div class="flow-step">
                        <div class="step-number">3</div>
                        <div>
                            <h4>Natural Variations</h4>
                            <p>Seasonal patterns, human behavior changes, market fluctuations</p>
                        </div>
                    </div>
                    <div class="flow-step">
                        <div class="step-number">4</div>
                        <div>
                            <h4>Processing Artifacts</h4>
                            <p>Rounding errors, compression losses, sampling biases</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="card">
                <h3>üßπ Advanced Missing Value Strategies</h3>
                
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Strategy</th>
                            <th>When to Use</th>
                            <th>Pros</th>
                            <th>Cons</th>
                            <th>Example</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Deletion</strong></td>
                            <td>&lt;5% missing, random pattern</td>
                            <td>Simple, no bias if random</td>
                            <td>Lose data, may introduce bias</td>
                            <td>Remove incomplete survey responses</td>
                        </tr>
                        <tr>
                            <td><strong>Mean/Median</strong></td>
                            <td>Numerical, normal distribution</td>
                            <td>Fast, preserves sample size</td>
                            <td>Reduces variance, ignores relationships</td>
                            <td>Fill missing ages with average age</td>
                        </tr>
                        <tr>
                            <td><strong>Mode</strong></td>
                            <td>Categorical data</td>
                            <td>Preserves most common pattern</td>
                            <td>May overrepresent majority class</td>
                            <td>Fill missing gender with most common</td>
                        </tr>
                        <tr>
                            <td><strong>Forward/Backward Fill</strong></td>
                            <td>Time series data</td>
                            <td>Preserves trends</td>
                            <td>May propagate old values too far</td>
                            <td>Fill missing stock prices with last known</td>
                        </tr>
                        <tr>
                            <td><strong>Interpolation</strong></td>
                            <td>Ordered data with trends</td>
                            <td>Smooth, logical values</td>
                            <td>Assumes linear relationships</td>
                            <td>Estimate missing temperature readings</td>
                        </tr>
                        <tr>
                            <td><strong>ML Imputation</strong></td>
                            <td>Complex patterns, &gt;10% missing</td>
                            <td>Uses all available information</td>
                            <td>Computationally expensive</td>
                            <td>Predict missing income from other features</td>
                        </tr>
                    </tbody>
                </table>

                <div class="code-block">
<pre><code class="language-python">
# Data Preprocessing: Advanced Missing Value Strategies
# Advanced Missing Value Handling: Real-World Scenarios
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

print("üßπ Advanced Missing Value Handling")

# Create realistic dataset with different missing patterns
np.random.seed(42)
n_samples = 1000

# Simulate customer dataset with realistic missing patterns
data = {
    'customer_id': range(1, n_samples + 1),
    'age': np.random.normal(35, 12, n_samples),
    'income': np.random.normal(50000, 15000, n_samples),
    'credit_score': np.random.normal(650, 100, n_samples),
    'years_employed': np.random.exponential(5, n_samples),
    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 
                                 n_samples, p=[0.4, 0.4, 0.15, 0.05]),
    'loan_approved': np.random.choice([0, 1], n_samples, p=[0.3, 0.7])
}

df = pd.DataFrame(data)

# Introduce realistic missing patterns
print("Introducing realistic missing value patterns:")

# 1. Missing Completely at Random (MCAR) - equipment failure
mcar_indices = np.random.choice(n_samples, size=50, replace=False)
df.loc[mcar_indices, 'credit_score'] = np.nan
print(f"1. MCAR: {len(mcar_indices)} random credit scores missing (equipment failure)")

# 2. Missing at Random (MAR) - older people less likely to report income
older_customers = df[df['age'] > 60].index
income_missing = np.random.choice(older_customers, 
                                 size=int(len(older_customers) * 0.3), 
                                 replace=False)
df.loc[income_missing, 'income'] = np.nan
print(f"2. MAR: {len(income_missing)} income values missing (older customers)")

# 3. Missing Not at Random (MNAR) - people with low education don't report it
low_education = df[df['education'] == 'High School'].index
education_missing = np.random.choice(low_education, 
                                   size=int(len(low_education) * 0.2), 
                                   replace=False)
df.loc[education_missing, 'education'] = np.nan
print(f"3. MNAR: {len(education_missing)} education values missing (systematic)")

print(f"\nMissing value summary:")
print(df.isnull().sum())

# Strategy 1: Simple Imputation
print("\nüîß Strategy 1: Simple Imputation")
simple_imputer_num = SimpleImputer(strategy='median')
simple_imputer_cat = SimpleImputer(strategy='most_frequent')

df_simple = df.copy()
numerical_cols = ['age', 'income', 'credit_score', 'years_employed']
df_simple[numerical_cols] = simple_imputer_num.fit_transform(df_simple[numerical_cols])
df_simple['education'] = simple_imputer_cat.fit_transform(df_simple[['education']]).ravel()

print(f"Simple imputation completed. Missing values: {df_simple.isnull().sum().sum()}")

# Strategy 2: KNN Imputation (uses similar customers)
print("\nü§ñ Strategy 2: KNN Imputation (using similar customers)")
knn_imputer = KNNImputer(n_neighbors=5)

# Prepare data for KNN (encode categorical)
df_knn = df.copy()
education_encoded = pd.get_dummies(df_knn['education'], prefix='education')
df_knn = pd.concat([df_knn.drop('education', axis=1), education_encoded], axis=1)

# Apply KNN imputation
df_knn_imputed = pd.DataFrame(
    knn_imputer.fit_transform(df_knn.select_dtypes(include=[np.number])),
    columns=df_knn.select_dtypes(include=[np.number]).columns
)

print(f"KNN imputation completed. Missing values: {df_knn_imputed.isnull().sum().sum()}")

# Strategy 3: Iterative Imputation (MICE - Multiple Imputation by Chained Equations)
print("\nüî¨ Strategy 3: Iterative Imputation (MICE)")
iterative_imputer = IterativeImputer(random_state=42, max_iter=10)

df_mice = df.copy()
# Encode categorical for MICE
df_mice['education_encoded'] = pd.Categorical(df_mice['education']).codes
df_mice['education_encoded'] = df_mice['education_encoded'].replace(-1, np.nan)

numerical_cols_mice = ['age', 'income', 'credit_score', 'years_employed', 'education_encoded']
df_mice[numerical_cols_mice] = iterative_imputer.fit_transform(df_mice[numerical_cols_mice])

print(f"MICE imputation completed. Missing values: {df_mice[numerical_cols_mice].isnull().sum().sum()}")

# Compare imputation quality
print("\nüìä Imputation Quality Comparison:")
original_income_mean = df['income'].mean()
print(f"Original income mean: ${original_income_mean:,.2f}")
print(f"Simple imputation: ${df_simple['income'].mean():,.2f}")
print(f"KNN imputation: ${df_knn_imputed['income'].mean():,.2f}")
print(f"MICE imputation: ${df_mice['income'].mean():,.2f}")

# Best practices
print("\nüí° Missing Value Best Practices:")
print("‚úÖ Always analyze WHY data is missing")
print("‚úÖ Document your imputation strategy")
print("‚úÖ Consider creating 'missing' indicator variables")
print("‚úÖ Validate imputation quality on a subset with known values")
print("‚úÖ Use domain knowledge to choose appropriate strategies")
print("‚úÖ Consider multiple imputation for critical decisions")
                        </code></pre>   
                </div>
            </div>

            <div class="card">
                <h3>‚öñÔ∏è Feature Scaling: Making Features Play Nice</h3>
                
                <div class="highlight-box warning">
                    <h4>üéØ Why Scaling Matters</h4>
                    <p>Imagine comparing running speed (measured in mph) with salary (measured in dollars). Without scaling, the salary feature would dominate simply because its numbers are larger, even if speed is more predictive!</p>
                </div>

                <div class="grid">
                    <div class="card">
                        <h4>üìè StandardScaler (Z-score normalization)</h4>
                        <p><strong>Formula:</strong> (x - mean) / standard_deviation</p>
                        <p><strong>Result:</strong> Mean = 0, Standard Deviation = 1</p>
                        <p><strong>Best for:</strong> Normal distributions, algorithms sensitive to variance (SVM, Neural Networks)</p>
                        <ul>
                            <li>Preserves relationships between features</li>
                            <li>Works well with normally distributed data</li>
                            <li>Most commonly used scaling method</li>
                        </ul>
                    </div>

                    <div class="card">
                        <h4>üéØ MinMaxScaler</h4>
                        <p><strong>Formula:</strong> (x - min) / (max - min)</p>
                        <p><strong>Result:</strong> Values between 0 and 1</p>
                        <p><strong>Best for:</strong> Bounded algorithms (Neural Networks), when you need [0,1] range</p>
                        <ul>
                            <li>Preserves zero values</li>
                            <li>Sensitive to outliers</li>
                            <li>Good for image data (pixels)</li>
                        </ul>
                    </div>

                    <div class="card">
                        <h4>üõ°Ô∏è RobustScaler</h4>
                        <p><strong>Formula:</strong> (x - median) / IQR</p>
                        <p><strong>Result:</strong> Median = 0, robust to outliers</p>
                        <p><strong>Best for:</strong> Data with many outliers</p>
                        <ul>
                            <li>Uses median instead of mean</li>
                            <li>Uses Interquartile Range instead of std</li>
                            <li>Less affected by extreme values</li>
                        </ul>
                    </div>
                </div>

                <div class="code-block">
<pre><code class="language-python">
# Feature Scaling Deep Dive: When and How to Scale
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

print("‚öñÔ∏è Feature Scaling: The Science Behind the Magic")

# Create realistic dataset with different scales
np.random.seed(42)
n_samples = 1000

# Simulate employee data with vastly different scales
data = {
    'age': np.random.normal(35, 8, n_samples),                    # 20-60 range
    'salary': np.random.normal(75000, 25000, n_samples),         # 20k-150k range
    'years_experience': np.random.exponential(5, n_samples),     # 0-30 range
    'performance_score': np.random.normal(3.5, 0.8, n_samples), # 1-5 range
    'hours_per_week': np.random.normal(42, 8, n_samples)        # 20-60 range
}

# Add some outliers (realistic scenario)
outlier_indices = np.random.choice(n_samples, size=20, replace=False)
data['salary'][outlier_indices] *= 3  # Some very high earners

df = pd.DataFrame(data)

# Create target variable (promotion eligibility)
promotion_score = (
    df['performance_score'] * 0.4 + 
    df['years_experience'] * 0.1 + 
    (df['salary'] / 100000) * 0.3 +
    np.random.normal(0, 0.3, n_samples)
)
df['promotion_eligible'] = (promotion_score > promotion_score.median()).astype(int)

print("üìä Original Data Statistics:")
print(df.describe().round(2))

print(f"\nüéØ Feature Scale Comparison:")
for col in ['age', 'salary', 'years_experience', 'performance_score']:
    print(f"{col:20}: Range = {df[col].min():.0f} to {df[col].max():.0f}")

# Split data
X = df.drop('promotion_eligible', axis=1)
y = df['promotion_eligible']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Test model performance WITHOUT scaling
print(f"\nüö´ Model Performance WITHOUT Scaling:")
model_unscaled = LogisticRegression(random_state=42, max_iter=1000)
model_unscaled.fit(X_train, y_train)
accuracy_unscaled = accuracy_score(y_test, model_unscaled.predict(X_test))
print(f"Accuracy: {accuracy_unscaled:.3f}")

# Show feature coefficients (to see scale bias)
print("Feature coefficients (shows scale bias):")
for feature, coef in zip(X.columns, model_unscaled.coef_[0]):
    print(f"  {feature:20}: {coef:.6f}")

# Apply different scaling methods
scalers = {
    'StandardScaler': StandardScaler(),
    'MinMaxScaler': MinMaxScaler(),
    'RobustScaler': RobustScaler()
}

results = {}

for scaler_name, scaler in scalers.items():
    print(f"\nüîß Testing {scaler_name}:")
    
    # Fit scaler on training data only!
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Show scaling effect
    scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)
    print(f"After scaling - Mean: {scaled_df.mean().round(3).values}")
    print(f"After scaling - Std:  {scaled_df.std().round(3).values}")
    
    # Train model on scaled data
    model = LogisticRegression(random_state=42, max_iter=1000)
    model.fit(X_train_scaled, y_train)
    
    # Evaluate
    accuracy = accuracy_score(y_test, model.predict(X_test_scaled))
    results[scaler_name] = accuracy
    print(f"Accuracy: {accuracy:.3f}")

# Compare all results
print(f"\nüìà Scaling Method Comparison:")
print(f"No Scaling:     {accuracy_unscaled:.3f}")
for method, acc in results.items():
    improvement = acc - accuracy_unscaled
    print(f"{method:15}: {acc:.3f} ({improvement:+.3f})")

# Demonstrate why scaling matters with a visual example
print(f"\nüëÅÔ∏è Visual Example: Why Salary Dominates Without Scaling")
sample_customer = X_test.iloc[0]
print("Sample employee features:")
for feature, value in sample_customer.items():
    print(f"  {feature:20}: {value:.1f}")

print(f"\nWithout scaling, salary ({sample_customer['salary']:.0f}) dominates age ({sample_customer['age']:.0f})")
print("The algorithm treats salary as 2000x more important just due to scale!")

# Advanced scaling considerations
print(f"\nüéì Advanced Scaling Considerations:")

print(f"\n1. When to use each scaler:")
print("   StandardScaler: Normal distributions, most ML algorithms")
print("   MinMaxScaler: Bounded outputs needed, neural networks")
print("   RobustScaler: Many outliers present, skewed distributions")

print(f"\n2. Features that typically DON'T need scaling:")
print("   ‚Ä¢ Already scaled features (percentages, ratios)")
print("   ‚Ä¢ Tree-based algorithms (Random Forest, XGBoost)")
print("   ‚Ä¢ Categorical encoded features")

print(f"\n3. Critical scaling rules:")
print("   ‚úÖ Always fit scaler on training data only")
print("   ‚úÖ Apply same scaling to train, validation, and test sets")
print("   ‚úÖ Save scaler for production predictions")
print("   ‚úÖ Scale features before dimensionality reduction")

# Demonstrate the cardinal sin: fitting scaler on test data
print(f"\n‚ùå NEVER DO THIS - Common Scaling Mistake:")
print("# Wrong way - fits on all data!")
print("scaler.fit(X)  # This includes test data!")
print("X_train_scaled = scaler.transform(X_train)")
print("X_test_scaled = scaler.transform(X_test)")
print("")
print("‚úÖ Correct way:")
print("scaler.fit(X_train)  # Only training data!")
print("X_train_scaled = scaler.transform(X_train)")
print("X_test_scaled = scaler.transform(X_test)")
                        </code></pre>   
                </div>
            </div>

            <div class="card">
                <h3>üî§ Categorical Encoding: From Text to Numbers</h3>
                
                <div class="highlight-box info">
                    <h4>üéØ The Challenge</h4>
                    <p>Machine learning algorithms work with numbers, not text. Converting categorical data (like "Red", "Blue", "Green") into numerical format while preserving meaningful relationships is both an art and science.</p>
                </div>

                <div class="grid">
                    <div class="card">
                        <h4>üè∑Ô∏è Label Encoding</h4>
                        <p><strong>Method:</strong> Assign each category a unique integer</p>
                        <p><strong>Example:</strong> Red=0, Blue=1, Green=2</p>
                        <p><strong>Pros:</strong> Memory efficient, simple</p>
                        <p><strong>Cons:</strong> Implies ordering (Blue > Red)</p>
                        <p><strong>Use for:</strong> Ordinal data, tree-based algorithms</p>
                    </div>

                    <div class="card">
                        <h4>üéØ One-Hot Encoding</h4>
                        <p><strong>Method:</strong> Create binary column for each category</p>
                        <p><strong>Example:</strong> Red=[1,0,0], Blue=[0,1,0], Green=[0,0,1]</p>
                        <p><strong>Pros:</strong> No artificial ordering</p>
                        <p><strong>Cons:</strong> High dimensionality, sparse data</p>
                        <p><strong>Use for:</strong> Nominal data, linear algorithms</p>
                    </div>

                    <div class="card">
                        <h4>üìä Target Encoding</h4>
                        <p><strong>Method:</strong> Replace category with target statistic</p>
                        <p><strong>Example:</strong> City ‚Üí Average income in that city</p>
                        <p><strong>Pros:</strong> Captures predictive power</p>
                        <p><strong>Cons:</strong> Risk of overfitting</p>
                        <p><strong>Use for:</strong> High cardinality categorical features</p>
                    </div>
                </div>

                <div class="code-block">
<pre><code class="language-python">
# Categorical Encoding: Comprehensive Guide
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import category_encoders as ce  # pip install category_encoders

print("üî§ Categorical Encoding: From Text to Meaningful Numbers")

# Create realistic dataset with different categorical types
np.random.seed(42)
n_samples = 1000

# Simulate car sales data
brands = ['Toyota', 'Honda', 'Ford', 'BMW', 'Audi', 'Mercedes', 'Hyundai', 'Kia']
colors = ['White', 'Black', 'Silver', 'Red', 'Blue']
sizes = ['Compact', 'Mid-size', 'Full-size', 'SUV']
fuel_types = ['Gasoline', 'Hybrid', 'Electric']

# Different types of categorical variables
data = {
    'brand': np.random.choice(brands, n_samples, p=[0.15, 0.15, 0.12, 0.08, 0.08, 0.07, 0.175, 0.175]),
    'color': np.random.choice(colors, n_samples, p=[0.3, 0.25, 0.2, 0.15, 0.1]),
    'size': np.random.choice(sizes, n_samples, p=[0.3, 0.3, 0.2, 0.2]),
    'fuel_type': np.random.choice(fuel_types, n_samples, p=[0.7, 0.25, 0.05]),
    'year': np.random.randint(2015, 2024, n_samples),
    'mileage': np.random.exponential(30000, n_samples),
    'price': np.random.normal(25000, 8000, n_samples)
}

df = pd.DataFrame(data)

# Create target: high-value car (above median price)
df['high_value'] = (df['price'] > df['price'].median()).astype(int)

print("üöó Car Sales Dataset:")
print(f"Shape: {df.shape}")
print(f"\nCategorical features:")
categorical_features = ['brand', 'color', 'size', 'fuel_type']
for col in categorical_features:
    unique_count = df[col].nunique()
    print(f"  {col:12}: {unique_count:2d} unique values")
    if unique_count <= 5:
        print(f"    Values: {list(df[col].unique())}")

# Analyze categorical relationships with target
print(f"\nüìä Categorical Feature Analysis:")
for col in categorical_features:
    high_value_rate = df.groupby(col)['high_value'].mean().sort_values(ascending=False)
    print(f"\n{col} - High value rate by category:")
    for category, rate in high_value_rate.head(3).items():
        print(f"  {category:12}: {rate:.1%}")

# Prepare data for encoding experiments
X = df[categorical_features + ['year', 'mileage']]
y = df['high_value']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Method 1: Label Encoding
print(f"\nüè∑Ô∏è Method 1: Label Encoding")
X_train_label = X_train.copy()
X_test_label = X_test.copy()

label_encoders = {}
for col in categorical_features:
    le = LabelEncoder()
    X_train_label[col] = le.fit_transform(X_train[col])
    X_test_label[col] = le.transform(X_test[col])
    label_encoders[col] = le
    
    print(f"  {col}: {le.classes_[:3]}... ‚Üí {[0, 1, 2]}...")

# Test with tree-based model (works well with label encoding)
rf_label = RandomForestClassifier(n_estimators=100, random_state=42)
rf_label.fit(X_train_label, y_train)
accuracy_label_rf = accuracy_score(y_test, rf_label.predict(X_test_label))
print(f"Random Forest accuracy: {accuracy_label_rf:.3f}")

# Method 2: One-Hot Encoding
print(f"\nüéØ Method 2: One-Hot Encoding")
X_train_onehot = pd.get_dummies(X_train, columns=categorical_features, prefix=categorical_features)
X_test_onehot = pd.get_dummies(X_test, columns=categorical_features, prefix=categorical_features)

# Ensure same columns in train and test
missing_cols = set(X_train_onehot.columns) - set(X_test_onehot.columns)
for col in missing_cols:
    X_test_onehot[col] = 0
X_test_onehot = X_test_onehot[X_train_onehot.columns]

print(f"  Original features: {len(categorical_features)}")
print(f"  After one-hot: {X_train_onehot.shape[1]} features")
print(f"  Memory increase: {X_train_onehot.memory_usage(deep=True).sum() / X_train.memory_usage(deep=True).sum():.1f}x")

# Test with linear model (benefits from one-hot)
lr_onehot = LogisticRegression(random_state=42, max_iter=1000)
lr_onehot.fit(X_train_onehot, y_train)
accuracy_onehot_lr = accuracy_score(y_test, lr_onehot.predict(X_test_onehot))
print(f"Logistic Regression accuracy: {accuracy_onehot_lr:.3f}")

# Method 3: Target Encoding (Mean Encoding)
print(f"\nüìä Method 3: Target Encoding")
X_train_target = X_train.copy()
X_test_target = X_test.copy()

# Calculate target means for each category (with smoothing)
target_encoders = {}
for col in categorical_features:
    # Calculate mean target value for each category
    target_means = X_train.groupby(col)[y_train.reset_index(drop=True)].mean()
    
    # Add smoothing to prevent overfitting (using global mean)
    global_mean = y_train.mean()
    category_counts = X_train[col].value_counts()
    
    # Smoothed encoding: (category_mean * count + global_mean * smooth) / (count + smooth)
    smooth_factor = 10
    smoothed_means = (target_means * category_counts + global_mean * smooth_factor) / (category_counts + smooth_factor)
    
    target_encoders[col] = smoothed_means
    
    # Apply encoding
    X_train_target[col] = X_train[col].map(smoothed_means)
    X_test_target[col] = X_test[col].map(smoothed_means).fillna(global_mean)
    
    print(f"  {col} encoding samples:")
    for category, encoded_value in smoothed_means.head(3).items():
        print(f"    {category:12} ‚Üí {encoded_value:.3f}")

# Test target encoding
rf_target = RandomForestClassifier(n_estimators=100, random_state=42)
rf_target.fit(X_train_target, y_train)
accuracy_target_rf = accuracy_score(y_test, rf_target.predict(X_test_target))
print(f"Random Forest accuracy: {accuracy_target_rf:.3f}")

# Method 4: Advanced Encoders (using category_encoders library)
print(f"\nüöÄ Method 4: Advanced Encoders")

# Binary Encoding (combines benefits of label and one-hot)
binary_encoder = ce.BinaryEncoder(cols=categorical_features)
X_train_binary = binary_encoder.fit_transform(X_train, y_train)
X_test_binary = binary_encoder.transform(X_test)

print(f"Binary encoding: {X_train.shape[1]} ‚Üí {X_train_binary.shape[1]} features")

# Test binary encoding
rf_binary = RandomForestClassifier(n_estimators=100, random_state=42)
rf_binary.fit(X_train_binary, y_train)
accuracy_binary_rf = accuracy_score(y_test, rf_binary.predict(X_test_binary))
print(f"Random Forest accuracy: {accuracy_binary_rf:.3f}")

# Compare all methods
print(f"\nüìà Encoding Method Comparison:")
results = {
    'Label Encoding (RF)': accuracy_label_rf,
    'One-Hot Encoding (LR)': accuracy_onehot_lr,
    'Target Encoding (RF)': accuracy_target_rf,
    'Binary Encoding (RF)': accuracy_binary_rf
}

for method, accuracy in sorted(results.items(), key=lambda x: x[1], reverse=True):
    print(f"  {method:25}: {accuracy:.3f}")

# Encoding selection guide
print(f"\nüéì Encoding Selection Guide:")
print(f"\nüìä High Cardinality (many categories):")
print("  ‚úÖ Target Encoding - captures predictive power")
print("  ‚úÖ Binary Encoding - compact representation")
print("  ‚ùå One-Hot - creates too many features")

print(f"\nüìè Low Cardinality (few categories):")
print("  ‚úÖ One-Hot - for linear models")
print("  ‚úÖ Label Encoding - for tree models")
print("  ‚ö†Ô∏è Target Encoding - may overfit")

print(f"\nüéØ Ordinal Data (natural order):")
print("  ‚úÖ Ordinal Encoding - preserves order")
print("  ‚ùå One-Hot - loses ordering information")

print(f"\n‚ö†Ô∏è Critical Encoding Rules:")
print("  1. Always fit encoders on training data only")
print("  2. Handle unseen categories in test data")
print("  3. Consider target leakage with target encoding")
print("  4. Validate encoding choice with cross-validation")
print("  5. Document encoding decisions for production")
                        </code></pre>       
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üîÑ</span>The Complete ML Workflow: From Problem to Production</h2>
            
            <div class="highlight-box info">
                <h4>üéØ The Iterative Nature of ML</h4>
                <p>ML isn't a linear process‚Äîit's a cycle of experimentation, learning, and refinement. Each step informs the others, and you'll often loop back to earlier stages as you gain insights.</p>
            </div>

            <div class="flowchart">
                <div class="flow-step" style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);">
                    <div class="step-number">1</div>
                    <div>
                        <h4>üéØ Problem Definition & Business Understanding</h4>
                        <p><strong>Questions to ask:</strong> What business problem are we solving? What does success look like? What's the impact of being wrong?</p>
                        <ul style="margin-top: 10px;">
                            <li>Define success metrics (accuracy, ROI, user satisfaction)</li>
                            <li>Understand business constraints (time, budget, interpretability)</li>
                            <li>Identify stakeholders and their needs</li>
                            <li>Determine if ML is the right solution</li>
                        </ul>
                    </div>
                </div>

                <div class="flow-step" style="background: linear-gradient(135deg, #e8f5e8 0%, #c8e6c9 100%);">
                    <div class="step-number">2</div>
                    <div>
                        <h4>üìä Data Collection & Understanding</h4>
                        <p><strong>Data audit:</strong> What data do we have? What's missing? How reliable is it?</p>
                        <ul style="margin-top: 10px;">
                            <li>Inventory available data sources</li>
                            <li>Assess data quality and completeness</li>
                            <li>Understand data collection process</li>
                            <li>Identify potential biases and limitations</li>
                        </ul>
                    </div>
                </div>

                <div class="flow-step" style="background: linear-gradient(135deg, #fff8e1 0%, #ffecb3 100%);">
                    <div class="step-number">3</div>
                    <div>
                        <h4>üîç Exploratory Data Analysis (EDA)</h4>
                        <p><strong>Detective work:</strong> What patterns exist? What insights can guide our modeling?</p>
                        <ul style="margin-top: 10px;">
                            <li>Visualize distributions and relationships</li>
                            <li>Identify outliers and anomalies</li>
                            <li>Discover feature correlations</li>
                            <li>Generate hypotheses for modeling</li>
                        </ul>
                    </div>
                </div>

                <div class="flow-step" style="background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);">
                    <div class="step-number">4</div>
                    <div>
                        <h4>üõ†Ô∏è Data Preprocessing & Feature Engineering</h4>
                        <p><strong>Data preparation:</strong> Transform raw data into ML-ready format</p>
                        <ul style="margin-top: 10px;">
                            <li>Handle missing values and outliers</li>
                            <li>Scale and encode features</li>
                            <li>Create new meaningful features</li>
                            <li>Split data into train/validation/test sets</li>
                        </ul>
                    </div>
                </div>

                <div class="flow-step" style="background: linear-gradient(135deg, #ffebee 0%, #ffcdd2 100%);">
                    <div class="step-number">5</div>
                    <div>
                        <h4>ü§ñ Model Selection & Training</h4>
                        <p><strong>Algorithm choice:</strong> Which ML approach best fits our problem and data?</p>
                        <ul style="margin-top: 10px;">
                            <li>Select appropriate algorithm family</li>
                            <li>Implement baseline models</li>
                            <li>Train multiple candidate models</li>
                            <li>Tune hyperparameters systematically</li>
                        </ul>
                    </div>
                </div>

                <div class="flow-step" style="background: linear-gradient(135deg, #e0f2f1 0%, #b2dfdb 100%);">
                    <div class="step-number">6</div>
                    <div>
                        <h4>üìà Model Evaluation & Validation</h4>
                        <p><strong>Performance assessment:</strong> How well does our model generalize to new data?</p>
                        <ul style="margin-top: 10px;">
                            <li>Evaluate using appropriate metrics</li>
                            <li>Perform cross-validation</li>
                            <li>Test on held-out data</li>
                            <li>Analyze errors and edge cases</li>
                        </ul>
                    </div>
                </div>

                <div class="flow-step" style="background: linear-gradient(135deg, #fce4ec 0%, #f8bbd9 100%);">
                    <div class="step-number">7</div>
                    <div>
                        <h4>üöÄ Deployment & Monitoring</h4>
                        <p><strong>Production deployment:</strong> Make the model available for real-world use</p>
                        <ul style="margin-top: 10px;">
                            <li>Deploy model to production environment</li>
                            <li>Monitor performance and data drift</li>
                            <li>Set up alerts and feedback loops</li>
                            <li>Plan for model updates and maintenance</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="code-block">
<pre><code class="language-python"> 
# Complete ML Workflow: End-to-End Project
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

print("üîÑ Complete ML Workflow: Customer Churn Prediction")
print("="*60)

# Step 1: Problem Definition
print("1Ô∏è‚É£ PROBLEM DEFINITION")
print("Business Goal: Predict which customers will churn (cancel subscription)")
print("Success Metric: ROC-AUC > 0.85 (balanced for precision/recall)")
print("Business Impact: Reduce churn by 20% through targeted retention")
print("Constraints: Model must be interpretable for business stakeholders")

# Step 2: Data Collection (Simulated)
print("\n2Ô∏è‚É£ DATA COLLECTION")
np.random.seed(42)
n_customers = 5000

# Simulate realistic customer data
data = {
    'customer_id': range(1, n_customers + 1),
    'age': np.random.normal(40, 15, n_customers).clip(18, 80),
    'tenure_months': np.random.exponential(24, n_customers).clip(1, 120),
    'monthly_charges': np.random.normal(65, 20, n_customers).clip(20, 150),
    'total_charges': lambda: None,  # Will calculate based on tenure and monthly
    'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], 
                                     n_customers, p=[0.5, 0.3, 0.2]),
    'payment_method': np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'],
                                      n_customers, p=[0.35, 0.2, 0.25, 0.2]),
    'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], 
                                        n_customers, p=[0.4, 0.45, 0.15]),
    'online_security': np.random.choice(['Yes', 'No', 'No internet service'], 
                                       n_customers, p=[0.3, 0.55, 0.15]),
    'tech_support': np.random.choice(['Yes', 'No', 'No internet service'], 
                                    n_customers, p=[0.35, 0.5, 0.15]),
    'streaming_tv': np.random.choice(['Yes', 'No', 'No internet service'], 
                                    n_customers, p=[0.4, 0.45, 0.15]),
    'paperless_billing': np.random.choice(['Yes', 'No'], n_customers, p=[0.6, 0.4]),
    'senior_citizen': np.random.choice([0, 1], n_customers, p=[0.85, 0.15])
}

df = pd.DataFrame(data)

# Calculate total charges
df['total_charges'] = df['tenure_months'] * df['monthly_charges'] + np.random.normal(0, 100, n_customers)
df['total_charges'] = df['total_charges'].clip(0, None)

# Create realistic churn target
churn_probability = (
    0.3 * (df['contract_type'] == 'Month-to-month') +
    0.2 * (df['payment_method'] == 'Electronic check') +
    0.15 * (df['monthly_charges'] > 80) +
    0.1 * (df['tenure_months'] < 12) +
    0.1 * (df['senior_citizen'] == 1) +
    0.05 * (df['online_security'] == 'No') +
    np.random.normal(0, 0.1, n_customers)
)
df['churn'] = (churn_probability > 0.5).astype(int)

print(f"Dataset created: {df.shape[0]} customers, {df.shape[1]} features")
print(f"Churn rate: {df['churn'].mean():.1%}")

# Step 3: Exploratory Data Analysis
print("\n3Ô∏è‚É£ EXPLORATORY DATA ANALYSIS")

print("\nDataset Overview:")
print(df.info())

print(f"\nMissing values:")
print(df.isnull().sum())

print(f"\nNumerical features summary:")
numerical_cols = ['age', 'tenure_months', 'monthly_charges', 'total_charges']
print(df[numerical_cols].describe())

print(f"\nChurn analysis by categorical features:")
categorical_cols = ['contract_type', 'payment_method', 'internet_service', 'senior_citizen']
for col in categorical_cols:
    churn_rate = df.groupby(col)['churn'].mean().sort_values(ascending=False)
    print(f"\n{col}:")
    for category, rate in churn_rate.items():
        print(f"  {str(category):20}: {rate:.1%}")

# Key insights from EDA
print(f"\nüîç Key EDA Insights:")
print("‚úÖ Month-to-month contracts have highest churn (as expected)")
print("‚úÖ Electronic check payment method correlates with higher churn")
print("‚úÖ Senior citizens churn more frequently")
print("‚úÖ Higher monthly charges associated with churn")
print("‚úÖ No missing values - clean dataset")

# Step 4: Data Preprocessing
print("\n4Ô∏è‚É£ DATA PREPROCESSING")

# Prepare features for modeling
df_model = df.copy()

# Feature engineering
df_model['avg_monthly_charges'] = df_model['total_charges'] / df_model['tenure_months']
df_model['charges_per_service'] = df_model['monthly_charges'] / (
    (df_model['online_security'] == 'Yes').astype(int) +
    (df_model['tech_support'] == 'Yes').astype(int) +
    (df_model['streaming_tv'] == 'Yes').astype(int) + 1
)

print("‚úÖ Feature engineering completed:")
print("  ‚Ä¢ avg_monthly_charges: total_charges / tenure_months")
print("  ‚Ä¢ charges_per_service: monthly_charges / number_of_services")

# Encode categorical variables
categorical_features = ['contract_type', 'payment_method', 'internet_service', 
                       'online_security', 'tech_support', 'streaming_tv', 'paperless_billing']

# Use one-hot encoding for nominal features
df_encoded = pd.get_dummies(df_model, columns=categorical_features, prefix=categorical_features)

# Prepare final feature matrix
X = df_encoded.drop(['customer_id', 'churn'], axis=1)
y = df_encoded['churn']

print(f"‚úÖ Encoding completed: {len(categorical_features)} categorical ‚Üí {X.shape[1]} features")

# Train/validation/test split
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)

print(f"‚úÖ Data split completed:")
print(f"  Training: {X_train.shape[0]} samples ({X_train.shape[0]/len(X):.1%})")
print(f"  Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(X):.1%})")
print(f"  Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X):.1%})")

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print("‚úÖ Feature scaling completed")

# Step 5: Model Selection & Training
print("\n5Ô∏è‚É£ MODEL SELECTION & TRAINING")

# Define candidate models
models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(random_state=42, probability=True)
}

# Train and evaluate models using cross-validation
model_results = {}

print("Training and evaluating models...")
for name, model in models.items():
    print(f"\nü§ñ {name}:")
    
    # Use scaled data for SVM and Logistic Regression, original for Random Forest
    if name in ['Logistic Regression', 'SVM']:
        X_train_model = X_train_scaled
        X_val_model = X_val_scaled
    else:
        X_train_model = X_train
        X_val_model = X_val
    
    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train_model, y_train, cv=5, scoring='roc_auc')
    
    # Train on full training set and evaluate on validation
    model.fit(X_train_model, y_train)
    val_predictions = model.predict(X_val_model)
    val_probabilities = model.predict_proba(X_val_model)[:, 1]
    
    val_auc = roc_auc_score(y_val, val_probabilities)
    
    model_results[name] = {
        'model': model,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'val_auc': val_auc,
        'scaled': name in ['Logistic Regression', 'SVM']
    }
    
    print(f"  CV ROC-AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
    print(f"  Validation ROC-AUC: {val_auc:.3f}")

# Select best model
best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['val_auc'])
best_model_info = model_results[best_model_name]

print(f"\nüèÜ Best Model: {best_model_name}")
print(f"   Validation ROC-AUC: {best_model_info['val_auc']:.3f}")

# Hyperparameter tuning for best model
print(f"\nüîß Hyperparameter Tuning for {best_model_name}:")

if best_model_name == 'Random Forest':
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10]
    }
    X_train_tune = X_train
elif best_model_name == 'Logistic Regression':
    param_grid = {
        'C': [0.1, 1, 10, 100],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear']
    }
    X_train_tune = X_train_scaled
else:  # SVM
    param_grid = {
        'C': [0.1, 1, 10],
        'gamma': ['scale', 'auto', 0.1, 1],
        'kernel': ['rbf', 'poly']
    }
    X_train_tune = X_train_scaled

# Perform grid search
grid_search = GridSearchCV(
    models[best_model_name], 
    param_grid, 
    cv=5, 
    scoring='roc_auc',
    n_jobs=-1
)

grid_search.fit(X_train_tune, y_train)
tuned_model = grid_search.best_estimator_

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best CV score: {grid_search.best_score_:.3f}")

# Step 6: Model Evaluation
print("\n6Ô∏è‚É£ MODEL EVALUATION")

# Final evaluation on test set
if best_model_info['scaled']:
    X_test_final = X_test_scaled
else:
    X_test_final = X_test

test_predictions = tuned_model.predict(X_test_final)
test_probabilities = tuned_model.predict_proba(X_test_final)[:, 1]
test_auc = roc_auc_score(y_test, test_probabilities)

print(f"üéØ Final Test Results:")
print(f"ROC-AUC Score: {test_auc:.3f}")
print(f"Business Goal (>0.85): {'‚úÖ ACHIEVED' if test_auc > 0.85 else '‚ùå NOT MET'}")

print(f"\nDetailed Classification Report:")
print(classification_report(y_test, test_predictions))

print(f"\nConfusion Matrix:")
cm = confusion_matrix(y_test, test_predictions)
print(f"                 Predicted")
print(f"                 No    Yes")
print(f"Actual    No   {cm[0,0]:4d}  {cm[0,1]:4d}")
print(f"          Yes  {cm[1,0]:4d}  {cm[1,1]:4d}")

# Business impact analysis
false_negatives = cm[1,0]  # Churned customers we missed
false_positives = cm[0,1]  # Non-churned customers we flagged

avg_customer_value = df['monthly_charges'].mean() * 12  # Annual value
retention_cost = 100  # Cost of retention campaign per customer

print(f"\nüí∞ Business Impact Analysis:")
print(f"Average annual customer value: ${avg_customer_value:.2f}")
print(f"Retention campaign cost: ${retention_cost:.2f} per customer")
print(f"Customers we'd miss (False Negatives): {false_negatives}")
print(f"Wasted retention efforts (False Positives): {false_positives}")

potential_revenue_saved = (cm[1,1]) * avg_customer_value * 0.3  # 30% retention success
wasted_costs = false_positives * retention_cost
net_benefit = potential_revenue_saved - wasted_costs

print(f"Potential revenue saved: ${potential_revenue_saved:,.2f}")
print(f"Wasted retention costs: ${wasted_costs:,.2f}")
print(f"Net business benefit: ${net_benefit:,.2f}")

# Feature importance (if available)
if hasattr(tuned_model, 'feature_importances_'):
    print(f"\nüìä Top 10 Most Important Features:")
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': tuned_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):
        print(f"  {i+1:2d}. {row['feature']:30}: {row['importance']:.3f}")

# Step 7: Deployment Considerations
print("\n7Ô∏è‚É£ DEPLOYMENT & MONITORING")
print("üöÄ Deployment Checklist:")
print("‚úÖ Model performance meets business requirements")
print("‚úÖ Feature engineering pipeline documented")
print("‚úÖ Scaling parameters saved for production")
print("‚úÖ Model serialization completed")

print(f"\nüìä Monitoring Plan:")
print("‚Ä¢ Track model performance monthly (ROC-AUC)")
print("‚Ä¢ Monitor feature drift in customer behavior")
print("‚Ä¢ Set up alerts for performance degradation")
print("‚Ä¢ Plan model retraining every 6 months")
print("‚Ä¢ A/B test model updates before full deployment")

print(f"\nüéØ Success Metrics for Production:")
print(f"‚Ä¢ Maintain ROC-AUC > 0.80 in production")
print(f"‚Ä¢ Achieve 15-20% reduction in churn rate")
print(f"‚Ä¢ Positive ROI on retention campaigns")
print(f"‚Ä¢ Model predictions available within 100ms")

print(f"\nüí° Next Steps:")
print("1. Deploy model to staging environment")
print("2. Integrate with customer retention system")
print("3. Train customer service team on model outputs")
print("4. Set up automated model monitoring")
print("5. Plan A/B test for retention campaign effectiveness")

print(f"\n" + "="*60)
print("üéâ COMPLETE ML WORKFLOW SUCCESSFULLY EXECUTED!")
print("   From business problem to production-ready solution")
print("="*60)
                        </code></pre>           
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üéØ</span>Hands-On Projects: Apply Your Knowledge</h2>
            
            <div class="highlight-box info">
                <h4>üéì Learning by Doing</h4>
                <p>The best way to master ML is through hands-on practice. These projects progress from beginner to intermediate, covering all major ML concepts with real-world datasets.</p>
            </div>

            <div class="grid">
                <div class="card">
                    <h3>üå∏ Project 1: Iris Classification (Beginner)</h3>
                    <p><strong>Goal:</strong> Classify iris flowers into species</p>
                    <p><strong>Dataset:</strong> Classic iris dataset (150 samples, 4 features, 3 classes)</p>
                    <p><strong>Skills:</strong> Basic ML pipeline, visualization, model evaluation</p>
                    
                    <div class="code-block">
<pre><code class="language-python"> 
# Project 1: Complete Iris Classification
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd
import numpy as np

print("üå∏ Project 1: Iris Species Classification")
print("="*50)

# Load the dataset
iris = load_iris()
X, y = iris.data, iris.target

print(f"Dataset: {X.shape[0]} samples, {X.shape[1]} features")
print(f"Classes: {iris.target_names}")
print(f"Features: {iris.feature_names}")

# Create DataFrame for easier analysis
df = pd.DataFrame(X, columns=iris.feature_names)
df['species'] = iris.target_names[y]

print(f"\nDataset overview:")
print(df.head())

print(f"\nClass distribution:")
print(df['species'].value_counts())

# Exploratory analysis
print(f"\nFeature statistics by species:")
print(df.groupby('species').mean().round(2))

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Try multiple models
models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(random_state=42, kernel='rbf')
}

print(f"\nModel Comparison (5-fold CV):")
best_score = 0
best_model = None

for name, model in models.items():
    # Use scaled data for LR and SVM, original for RF
    X_cv = X_train_scaled if name != 'Random Forest' else X_train
    scores = cross_val_score(model, X_cv, y_train, cv=5)
    
    print(f"{name:20}: {scores.mean():.3f} (+/- {scores.std()*2:.3f})")
    
    if scores.mean() > best_score:
        best_score = scores.mean()
        best_model = (name, model)

# Train best model and evaluate
model_name, model = best_model
X_train_final = X_train_scaled if model_name != 'Random Forest' else X_train
X_test_final = X_test_scaled if model_name != 'Random Forest' else X_test

model.fit(X_train_final, y_train)
predictions = model.predict(X_test_final)

print(f"\nüèÜ Best Model: {model_name}")
print(f"Test Accuracy: {model.score(X_test_final, y_test):.3f}")

print(f"\nClassification Report:")
print(classification_report(y_test, predictions, target_names=iris.target_names))

print(f"\n‚úÖ Project 1 Complete!")
print("Skills learned: Data loading, EDA, model comparison, evaluation")
                        </code></pre>           
                    </div>
                </div>

                <div class="card">
                    <h3>üè† Project 2: House Price Prediction (Intermediate)</h3>
                    <p><strong>Goal:</strong> Predict house prices based on features</p>
                    <p><strong>Dataset:</strong> Synthetic housing data with realistic features</p>
                    <p><strong>Skills:</strong> Regression, feature engineering, advanced preprocessing</p>

                    <div class="code-block">
<pre><code class="language-python">
# Project 2: House Price Prediction
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import pandas as pd
import numpy as np

print("üè† Project 2: House Price Prediction")
print("="*50)

# Create realistic housing dataset
np.random.seed(42)
n_houses = 2000

# Generate features with realistic correlations
square_footage = np.random.normal(2000, 500, n_houses).clip(800, 5000)
bedrooms = np.random.poisson(3, n_houses).clip(1, 6)
bathrooms = bedrooms + np.random.normal(0, 0.5, n_houses).clip(-1, 3)
age = np.random.exponential(15, n_houses).clip(0, 100)
garage_size = np.random.choice([0, 1, 2, 3], n_houses, p=[0.1, 0.3, 0.5, 0.1])
neighborhood_quality = np.random.choice([1, 2, 3, 4, 5], n_houses, p=[0.1, 0.2, 0.4, 0.2, 0.1])

# Create price with realistic relationships
base_price = (
    square_footage * 120 +  # $120 per sq ft
    bedrooms * 15000 +      # $15k per bedroom
    bathrooms * 8000 +      # $8k per bathroom
    garage_size * 12000 +   # $12k per garage space
    neighborhood_quality * 25000  # $25k per quality level
)

# Add age effect (depreciation)
age_effect = np.where(age < 10, 1.1, 1.0 - (age - 10) * 0.005)
base_price *= age_effect

# Add noise and market factors
market_factor = np.random.normal(1.0, 0.1, n_houses).clip(0.7, 1.3)
price = base_price * market_factor + np.random.normal(0, 10000, n_houses)
price = price.clip(50000, 1000000)

# Create DataFrame
df = pd.DataFrame({
    'square_footage': square_footage,
    'bedrooms': bedrooms,
    'bathrooms': bathrooms,
    'age': age,
    'garage_size': garage_size,
    'neighborhood_quality': neighborhood_quality,
    'price': price
})

print(f"Dataset: {df.shape[0]} houses, {df.shape[1]-1} features")
print(f"\nDataset overview:")
print(df.describe().round(2))

print(f"\nPrice distribution:")
print(f"Mean: ${df['price'].mean():,.2f}")
print(f"Median: ${df['price'].median():,.2f}")
print(f"Range: ${df['price'].min():,.2f} - ${df['price'].max():,.2f}")

# Feature engineering
df['price_per_sqft'] = df['price'] / df['square_footage']
df['total_rooms'] = df['bedrooms'] + df['bathrooms']
df['bath_bed_ratio'] = df['bathrooms'] / df['bedrooms']
df['is_new'] = (df['age'] < 5).astype(int)

print(f"\nüîß Feature Engineering:")
print("‚úÖ price_per_sqft: price / square_footage")
print("‚úÖ total_rooms: bedrooms + bathrooms")
print("‚úÖ bath_bed_ratio: bathrooms / bedrooms")
print("‚úÖ is_new: age < 5 years")

# Prepare features
feature_cols = ['square_footage', 'bedrooms', 'bathrooms', 'age', 
                'garage_size', 'neighborhood_quality', 'total_rooms', 
                'bath_bed_ratio', 'is_new']
X = df[feature_cols]
y = df['price']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Model comparison
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=1.0),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)
}

print(f"\nü§ñ Model Comparison:")
results = {}

for name, model in models.items():
    # Use scaled data for linear models, original for RF
    X_train_model = X_train_scaled if 'Regression' in name else X_train
    X_test_model = X_test_scaled if 'Regression' in name else X_test
    
    # Train model
    model.fit(X_train_model, y_train)
    predictions = model.predict(X_test_model)
    
    # Calculate metrics
    mse = mean_squared_error(y_test, predictions)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)
    
    results[name] = {'RMSE': rmse, 'MAE': mae, 'R¬≤': r2}
    
    print(f"\n{name}:")
    print(f"  RMSE: ${rmse:,.2f}")
    print(f"  MAE:  ${mae:,.2f}")
    print(f"  R¬≤:   {r2:.3f}")

# Best model analysis
best_model = max(results.keys(), key=lambda k: results[k]['R¬≤'])
print(f"\nüèÜ Best Model: {best_model}")
print(f"  R¬≤ Score: {results[best_model]['R¬≤']:.3f}")

# Feature importance (for Random Forest)
if best_model == 'Random Forest':
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)
    
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\nüìä Feature Importance:")
    for _, row in feature_importance.iterrows():
        print(f"  {row['feature']:20}: {row['importance']:.3f}")

print(f"\n‚úÖ Project 2 Complete!")
print("Skills learned: Regression, feature engineering, model evaluation")
                    </code></pre>           
                    </div>
                </div>

                <div class="card">
                    <h3>üõçÔ∏è Project 3: Customer Segmentation (Advanced)</h3>
                    <p><strong>Goal:</strong> Segment customers using unsupervised learning</p>
                    <p><strong>Dataset:</strong> Customer transaction and behavior data</p>
                    <p><strong>Skills:</strong> Clustering, dimensionality reduction, business insights</p>

                    <div class="code-block">
<pre><code class="language-python"> 
# Project 3: Customer Segmentation Analysis
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

print("üõçÔ∏è Project 3: Customer Segmentation Analysis")
print("="*50)

# Generate realistic customer data
np.random.seed(42)
n_customers = 2000

# Create customer segments with different behaviors
segment_sizes = [600, 800, 400, 200]  # VIP, Regular, Budget, Inactive
segments = []

for i, size in enumerate(segment_sizes):
    if i == 0:  # VIP customers
        annual_spending = np.random.normal(5000, 1000, size).clip(3000, 8000)
        visit_frequency = np.random.normal(15, 3, size).clip(10, 25)
        avg_purchase = np.random.normal(200, 50, size).clip(100, 400)
    elif i == 1:  # Regular customers
        annual_spending = np.random.normal(2000, 500, size).clip(1000, 3500)
        visit_frequency = np.random.normal(8, 2, size).clip(4, 15)
        avg_purchase = np.random.normal(100, 30, size).clip(50, 200)
    elif i == 2:  # Budget customers
        annual_spending = np.random.normal(800, 200, size).clip(400, 1300)
        visit_frequency = np.random.normal(12, 3, size).clip(6, 20)
        avg_purchase = np.random.normal(50, 15, size).clip(20, 100)
    else:  # Inactive customers
        annual_spending = np.random.normal(300, 100, size).clip(100, 600)
        visit_frequency = np.random.normal(2, 1, size).clip(1, 5)
        avg_purchase = np.random.normal(80, 20, size).clip(40, 150)
    
    segments.append({
        'annual_spending': annual_spending,
        'visit_frequency': visit_frequency,
        'avg_purchase': avg_purchase,
        'true_segment': [f'Segment_{i+1}'] * size
    })

# Combine all segments
data = {
    'customer_id': list(range(1, sum(segment_sizes) + 1)),
    'annual_spending': np.concatenate([s['annual_spending'] for s in segments]),
    'visit_frequency': np.concatenate([s['visit_frequency'] for s in segments]),
    'avg_purchase': np.concatenate([s['avg_purchase'] for s in segments]),
    'true_segment': np.concatenate([s['true_segment'] for s in segments])
}

# Add additional realistic features
data['days_since_last_purchase'] = np.random.exponential(30, sum(segment_sizes)).clip(1, 365)
data['total_purchases'] = data['annual_spending'] / np.array(data['avg_purchase'])
data['customer_lifetime'] = np.random.exponential(2, sum(segment_sizes)).clip(0.5, 8)  # years
data['satisfaction_score'] = np.random.normal(3.5, 0.8, sum(segment_sizes)).clip(1, 5)

df = pd.DataFrame(data)

print(f"Dataset: {df.shape[0]} customers, {df.shape[1]-2} features")
print(f"\nDataset overview:")
feature_cols = ['annual_spending', 'visit_frequency', 'avg_purchase', 
                'days_since_last_purchase', 'total_purchases', 
                'customer_lifetime', 'satisfaction_score']
print(df[feature_cols].describe().round(2))

# Prepare data for clustering
X = df[feature_cols]

# Scale features (important for clustering)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print(f"\nüîç Finding Optimal Number of Clusters:")

# Elbow method and silhouette analysis
inertias = []
silhouette_scores = []
k_range = range(2, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_scaled)
    
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, cluster_labels))
    
    print(f"k={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette_scores[-1]:.3f}")

# Find optimal k (highest silhouette score)
optimal_k = k_range[np.argmax(silhouette_scores)]
print(f"\nüéØ Optimal number of clusters: {optimal_k}")

# Apply K-means with optimal k
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
cluster_labels = kmeans_final.fit_predict(X_scaled)

df['predicted_cluster'] = cluster_labels

print(f"\nüìä Cluster Analysis:")
for cluster in range(optimal_k):
    cluster_data = df[df['predicted_cluster'] == cluster]
    size = len(cluster_data)
    
    print(f"\nCluster {cluster + 1} ({size} customers, {size/len(df)*100:.1f}%):")
    print(f"  Annual Spending: ${cluster_data['annual_spending'].mean():,.2f}")
    print(f"  Visit Frequency: {cluster_data['visit_frequency'].mean():.1f} visits/year")
    print(f"  Avg Purchase: ${cluster_data['avg_purchase'].mean():.2f}")
    print(f"  Customer Lifetime: {cluster_data['customer_lifetime'].mean():.1f} years")
    print(f"  Satisfaction: {cluster_data['satisfaction_score'].mean():.2f}/5")

# Business insights and recommendations
print(f"\nüéØ Business Insights & Recommendations:")

# Identify cluster characteristics
for cluster in range(optimal_k):
    cluster_data = df[df['predicted_cluster'] == cluster]
    avg_spending = cluster_data['annual_spending'].mean()
    avg_frequency = cluster_data['visit_frequency'].mean()
    avg_satisfaction = cluster_data['satisfaction_score'].mean()
    
    if avg_spending > 3000 and avg_frequency > 10:
        cluster_type = "üíé VIP Customers"
        recommendation = "Exclusive perks, premium service, loyalty rewards"
    elif avg_spending > 1500 and avg_frequency > 6:
        cluster_type = "üéØ Regular Customers"
        recommendation = "Upselling campaigns, personalized offers"
    elif avg_frequency > 8 and avg_spending < 1500:
        cluster_type = "üõí Budget-Conscious"
        recommendation = "Discount programs, value bundles"
    else:
        cluster_type = "üò¥ At-Risk/Inactive"
        recommendation = "Re-engagement campaigns, win-back offers"
    
    print(f"\nCluster {cluster + 1}: {cluster_type}")
    print(f"  Strategy: {recommendation}")
    print(f"  Revenue potential: ${avg_spending * len(cluster_data):,.2f}")

# Dimensionality reduction for visualization
print(f"\nüìà 2D Visualization using PCA:")
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print(f"PCA Explained Variance Ratio:")
print(f"  PC1: {pca.explained_variance_ratio_[0]:.3f}")
print(f"  PC2: {pca.explained_variance_ratio_[1]:.3f}")
print(f"  Total: {sum(pca.explained_variance_ratio_):.3f}")

# Calculate business metrics
total_revenue = df['annual_spending'].sum()
print(f"\nüí∞ Business Impact:")
print(f"Total annual revenue: ${total_revenue:,.2f}")

for cluster in range(optimal_k):
    cluster_data = df[df['predicted_cluster'] == cluster]
    cluster_revenue = cluster_data['annual_spending'].sum()
    cluster_share = cluster_revenue / total_revenue
    
    print(f"Cluster {cluster + 1}: ${cluster_revenue:,.2f} ({cluster_share:.1%})")

print(f"\n‚úÖ Project 3 Complete!")
print("Skills learned: Clustering, PCA, business insights, customer analysis")
                    </code></pre>       
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üéì</span>Key Takeaways & Next Steps</h2>
            
            <div class="card">
                <h3>üß† Core Concepts Mastered</h3>
                <div class="grid">
                    <div class="highlight-box tip">
                        <h4>‚úÖ Supervised Learning</h4>
                        <ul>
                            <li>Regression vs Classification understanding</li>
                            <li>Training with labeled examples</li>
                            <li>Performance evaluation metrics</li>
                            <li>Real-world business applications</li>
                        </ul>
                    </div>
                    
                    <div class="highlight-box info">
                        <h4>‚úÖ Unsupervised Learning</h4>
                        <ul>
                            <li>Pattern discovery without labels</li>
                            <li>Clustering for customer segmentation</li>
                            <li>Dimensionality reduction techniques</li>
                            <li>Business insight generation</li>
                        </ul>
                    </div>
                    
                    <div class="highlight-box warning">
                        <h4>‚úÖ Reinforcement Learning</h4>
                        <ul>
                            <li>Learning through trial and error</li>
                            <li>Agent-environment interaction</li>
                            <li>Reward-based optimization</li>
                            <li>Advanced applications overview</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="card">
                <h3>üõ†Ô∏è Technical Skills Acquired</h3>
                <div class="grid">
                    <div style="background: #f8f9fa; padding: 20px; border-radius: 8px;">
                        <h4>üêç Python ML Stack</h4>
                        <ul>
                            <li>NumPy for numerical computing</li>
                            <li>Pandas for data manipulation</li>
                            <li>Scikit-learn for ML algorithms</li>
                            <li>Matplotlib/Seaborn for visualization</li>
                        </ul>
                    </div>
                    
                    <div style="background: #e8f5e8; padding: 20px; border-radius: 8px;">
                        <h4>üßπ Data Preprocessing</h4>
                        <ul>
                            <li>Missing value handling strategies</li>
                            <li>Feature scaling and normalization</li>
                            <li>Categorical encoding techniques</li>
                            <li>Feature engineering principles</li>
                        </ul>
                    </div>
                    
                    <div style="background: #fff3e0; padding: 20px; border-radius: 8px;">
                        <h4>üîÑ ML Workflow</h4>
                        <ul>
                            <li>Problem definition and scoping</li>
                            <li>Data exploration and analysis</li>
                            <li>Model selection and training</li>
                            <li>Evaluation and deployment</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="card">
                <h3>üöÄ Your ML Journey Roadmap</h3>
                
                <div class="flowchart">
                    <div class="flow-step" style="background: linear-gradient(135deg, #e8f5e8 0%, #c8e6c9 100%);">
                        <div class="step-number">‚úÖ</div>
                        <div>
                            <h4>Chapter 1: Foundations Complete</h4>
                            <p>You now understand ML types, Python tools, and preprocessing</p>
                        </div>
                    </div>
                    
                    <div class="flow-step" style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);">
                        <div class="step-number">2</div>
                        <div>
                            <h4>Next: Chapter 2 - Deep Dive into Regression</h4>
                            <p>Linear regression, polynomial features, regularization techniques</p>
                        </div>
                    </div>
                    
                    <div class="flow-step" style="background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);">
                        <div class="step-number">3</div>
                        <div>
                            <h4>Chapter 3 - Classification Mastery</h4>
                            <p>Logistic regression, decision trees, ensemble methods</p>
                        </div>
                    </div>
                    
                    <div class="flow-step" style="background: linear-gradient(135deg, #fff8e1 0%, #ffecb3 100%);">
                        <div class="step-number">4</div>
                        <div>
                            <h4>Chapter 4 - Advanced Topics</h4>
                            <p>Neural networks, deep learning, model interpretation</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="interactive-demo">
                <h3>üéØ Practice Recommendations</h3>
                <div class="grid">
                    <div style="background: rgba(255,255,255,0.1); padding: 20px; border-radius: 8px;">
                        <h4>üìö Immediate Practice</h4>
                        <ul>
                            <li>Complete all three hands-on projects</li>
                            <li>Experiment with different datasets from Kaggle</li>
                            <li>Try various preprocessing techniques</li>
                            <li>Compare multiple algorithms on same data</li>
                        </ul>
                    </div>
                    
                    <div style="background: rgba(255,255,255,0.1); padding: 20px; border-radius: 8px;">
                        <h4>üî¨ Extended Learning</h4>
                        <ul>
                            <li>Join ML competitions (Kaggle Learn)</li>
                            <li>Build a portfolio of ML projects</li>
                            <li>Read research papers in your domain</li>
                            <li>Contribute to open-source ML projects</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="highlight-box tip">
                <h4>üí° Final Words of Wisdom</h4>
                <p><strong>Remember:</strong> Machine learning is 80% data preparation and 20% algorithms. Focus on understanding your data, asking the right questions, and iterating based on results. The most sophisticated algorithm won't help if your data quality is poor or you're solving the wrong problem.</p>
                
                <p><strong>Keep Learning:</strong> ML is a rapidly evolving field. Stay curious, keep practicing, and don't be afraid to experiment. Every expert was once a beginner who never gave up!</p>
            </div>

            <div style="text-align: center; margin: 40px 0;">
                <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 15px;">
                    <h2>üéâ Congratulations!</h2>
                    <p style="font-size: 1.2em; margin: 20px 0;">You've completed Chapter 1 and built a solid foundation in Machine Learning!</p>
                    <p>You're now ready to tackle real-world ML problems with confidence.</p>
                    
                    <div style="margin-top: 30px;">
                        <button style="background: white; color: #667eea; border: none; padding: 15px 30px; border-radius: 8px; font-size: 1.1em; font-weight: bold; margin: 10px; cursor: pointer;">
                            üìö Continue to Chapter 2
                        </button>
                        <button style="background: rgba(255,255,255,0.2); color: white; border: 2px solid white; padding: 15px 30px; border-radius: 8px; font-size: 1.1em; font-weight: bold; margin: 10px; cursor: pointer;">
                            üîÑ Review This Chapter
                        </button>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        function showTab(tabName) {
            // Hide all tab contents
            const contents = document.querySelectorAll('.tab-content');
            contents.forEach(content => content.classList.remove('active'));
            
            // Remove active class from all tabs
            const tabs = document.querySelectorAll('.tab');
            tabs.forEach(tab => tab.classList.remove('active'));
            
            // Show selected tab content
            document.getElementById(tabName).classList.add('active');
            
            // Add active class to clicked tab
            event.target.classList.add('active');
        }

        // Add some interactive feedback
        document.addEventListener('DOMContentLoaded', function() {
            const cards = document.querySelectorAll('.card');
            cards.forEach(card => {
                card.addEventListener('mouseenter', function() {
                    this.style.transform = 'translateY(-5px)';
                    this.style.boxShadow = '0 10px 25px rgba(0,0,0,0.15)';
                });
                
                card.addEventListener('mouseleave', function() {
                    this.style.transform = 'translateY(0)';
                    this.style.boxShadow = '0 4px 6px rgba(0,0,0,0.1)';
                });
            });
            
            // Animate progress bar
            const progressFill = document.querySelector('.progress-fill');
            if (progressFill) {
                progressFill.style.width = '0%';
                setTimeout(() => {
                    progressFill.style.width = '100%';
                }, 500);
            }
        });
    </script>
</body>
</html>
                