<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning Fundamentals - Part 1 | Ali Barzin Zanganeh</title>
    <meta name="description" content="Comprehensive guide to machine learning fundamentals covering supervised learning, data preprocessing, and exploratory data analysis with practical examples.">
    <link rel="stylesheet" href="../static/css/main.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <a href="../index.html" class="nav-brand">
                <img src="../static/images/logo.png" alt="Ali Barzin Zanganeh" class="nav-logo">
                Ali Barzin Zanganeh
            </a>
            <ul class="nav-menu">
                <li class="nav-item">
                    <a href="../index.html" class="nav-link">Home</a>
                </li>
                <li class="nav-item">
                    <a href="../index.html#projects" class="nav-link">Projects</a>
                </li>
                <li class="nav-item">
                    <a href="../tutorials.html" class="nav-link active">Tutorials</a>
                </li>
                <li class="nav-item">
                    <a href="../index.html#contact" class="nav-link">Contact</a>
                </li>
            </ul>
            <div class="hamburger">
                <span class="bar"></span>
                <span class="bar"></span>
                <span class="bar"></span>
            </div>
        </div>
    </nav>

    <main class="tutorial-content">
        <div class="container">
            <div class="tutorial-header">
                <div class="breadcrumb">
                    <a href="../tutorials.html">Tutorials</a>
                    <span>→</span>
                    <span>Machine Learning Fundamentals - Part 1</span>
                </div>
                <h1>Machine Learning Fundamentals - Part 1</h1>
                <p class="tutorial-subtitle">A comprehensive introduction to supervised learning, data preprocessing, and exploratory data analysis</p>
                <div class="tutorial-meta">
                    <span class="difficulty">Intermediate</span>
                    <span class="duration">45 min read</span>
                    <span class="category">Machine Learning</span>
                </div>
            </div>

            <div class="tutorial-body">
                <section class="tutorial-section">
                    <h2>Introduction</h2>
                    <p>Machine learning is a powerful subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed. This comprehensive tutorial covers the fundamental concepts of supervised learning, focusing on practical implementation and real-world applications.</p>
                    
                    <div class="info-box">
                        <h4>What You'll Learn</h4>
                        <ul>
                            <li>Supervised learning fundamentals and terminology</li>
                            <li>Data preprocessing and feature engineering techniques</li>
                            <li>Exploratory Data Analysis (EDA) methods</li>
                            <li>Practical implementation with Python and scikit-learn</li>
                        </ul>
                    </div>
                </section>

                <section class="tutorial-section">
                    <h2>1. Supervised Learning Fundamentals</h2>
                    
                    <h3>Core Concepts</h3>
                    <p>Supervised learning is a machine learning paradigm where algorithms learn from labeled training data to make predictions on new, unseen data. It's called "supervised" because the algorithm learns under supervision using input-output pairs.</p>

                    <h4>Types of Supervised Learning</h4>
                    
                    <div class="concept-box">
                        <h5>1. Regression</h5>
                        <p><strong>Purpose:</strong> Predicting continuous numerical values</p>
                        <p><strong>Examples:</strong></p>
                        <ul>
                            <li>Predicting house prices based on features like size, location, age</li>
                            <li>Estimating stock prices using historical data</li>
                            <li>Forecasting temperature or rainfall</li>
                        </ul>
                        <div class="code-block">
                            <pre><code># Example: Predicting house prices
# Input: size=1500 sq ft, bedrooms=3, location=downtown
# Output: $350,000 (continuous value)</code></pre>
                        </div>
                    </div>

                    <div class="concept-box">
                        <h5>2. Classification</h5>
                        <p><strong>Purpose:</strong> Predicting discrete categories or classes</p>
                        
                        <h6>Binary Classification (2 categories):</h6>
                        <ul>
                            <li>Email spam detection (spam vs. not spam)</li>
                            <li>Medical diagnosis (disease vs. healthy)</li>
                            <li>Customer churn prediction (will leave vs. will stay)</li>
                        </ul>
                        
                        <h6>Multi-class Classification (3+ categories):</h6>
                        <ul>
                            <li>Image recognition (cat, dog, bird, etc.)</li>
                            <li>Sentiment analysis (positive, negative, neutral)</li>
                            <li>Product categorization (electronics, clothing, books)</li>
                        </ul>

                        <div class="code-block">
                            <pre><code># Example: Email classification
# Input: email content, sender, subject
# Output: "spam" or "not spam" (discrete category)</code></pre>
                        </div>
                    </div>

                    <h4>Essential Terminology</h4>
                    <div class="terminology-grid">
                        <div class="term-item">
                            <h5>Features (Input Variables)</h5>
                            <p>Also called: predictors, covariates, independent variables, attributes</p>
                            <p><strong>Example:</strong> In house price prediction - size, bedrooms, bathrooms, location</p>
                        </div>
                        
                        <div class="term-item">
                            <h5>Target (Output Variable)</h5>
                            <p>Also called: dependent variable, response variable, label</p>
                            <p><strong>Example:</strong> In house price prediction - the actual price ($350,000)</p>
                        </div>
                        
                        <div class="term-item">
                            <h5>Samples/Records</h5>
                            <p>Individual data points or observations in your dataset</p>
                            <p><strong>Example:</strong> Each house with its features and price is one sample</p>
                        </div>
                        
                        <div class="term-item">
                            <h5>Training Data</h5>
                            <p>The dataset used to train the machine learning model</p>
                            <p><strong>Example:</strong> 80% of your house data with known prices</p>
                        </div>
                    </div>
                </section>

                <section class="tutorial-section">
                    <h2>2. Data Preparation and Preprocessing</h2>
                    <p>Data preprocessing is arguably the most crucial step in any machine learning project. Raw data is rarely ready for modeling and requires cleaning, transformation, and preparation.</p>

                    <h3>2.1 Handling Missing Values</h3>
                    
                    <div class="concept-box">
                        <h4>Detection and Assessment</h4>
                        <div class="code-block">
                            <pre><code>import pandas as pd
import numpy as np

# Load your dataset
df = pd.read_csv('your_data.csv')

# Check for missing values
print(df.isnull().sum())
print(f"Total missing values: {df.isnull().sum().sum()}")

# Visualize missing data pattern
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cbar=True, cmap='viridis')
plt.title('Missing Data Pattern')
plt.show()</code></pre>
                        </div>
                    </div>

                    <h4>Imputation Strategies</h4>
                    
                    <div class="strategy-grid">
                        <div class="strategy-item">
                            <h5>Static Imputation</h5>
                            <p>Uses fixed statistical measures calculated from the entire dataset</p>
                            <ul>
                                <li><strong>Mean:</strong> For normally distributed numerical data</li>
                                <li><strong>Median:</strong> For skewed numerical data (robust to outliers)</li>
                                <li><strong>Mode:</strong> For categorical data</li>
                            </ul>
                            <div class="code-block">
                                <pre><code># Static imputation examples
from sklearn.impute import SimpleImputer

# Mean imputation for numerical features
mean_imputer = SimpleImputer(strategy='mean')
df['age'] = mean_imputer.fit_transform(df[['age']])

# Median imputation (better for skewed data)
median_imputer = SimpleImputer(strategy='median')
df['income'] = median_imputer.fit_transform(df[['income']])

# Mode imputation for categorical features
mode_imputer = SimpleImputer(strategy='most_frequent')
df['category'] = mode_imputer.fit_transform(df[['category']])</code></pre>
                            </div>
                        </div>

                        <div class="strategy-item">
                            <h5>Dynamic Imputation</h5>
                            <p>Uses relationships between features to predict missing values</p>
                            <ul>
                                <li><strong>K-Nearest Neighbors:</strong> Uses similar records to impute</li>
                                <li><strong>Regression:</strong> Predicts missing values using other features</li>
                                <li><strong>Iterative:</strong> Iteratively imputes using all features</li>
                            </ul>
                            <div class="code-block">
                                <pre><code># Dynamic imputation examples
from sklearn.impute import KNNImputer, IterativeImputer

# KNN imputation - uses 5 nearest neighbors
knn_imputer = KNNImputer(n_neighbors=5)
df_knn = knn_imputer.fit_transform(df)

# Iterative imputation - uses regression
iterative_imputer = IterativeImputer(random_state=42)
df_iterative = iterative_imputer.fit_transform(df)</code></pre>
                            </div>
                        </div>
                    </div>

                    <h3>2.2 Categorical Encoding</h3>
                    <p>Machine learning algorithms typically work with numerical data, so categorical variables need to be converted to numerical format.</p>

                    <div class="encoding-methods">
                        <div class="method-item">
                            <h4>Label Encoding</h4>
                            <p>Assigns a unique integer to each category. Best for ordinal data.</p>
                            <div class="code-block">
                                <pre><code>from sklearn.preprocessing import LabelEncoder

# Example: Education levels (ordinal)
education_levels = ['High School', 'Bachelor', 'Master', 'PhD']
label_encoder = LabelEncoder()

# Result: High School=0, Bachelor=1, Master=2, PhD=3
df['education_encoded'] = label_encoder.fit_transform(df['education'])</code></pre>
                            </div>
                        </div>

                        <div class="method-item">
                            <h4>One-Hot Encoding</h4>
                            <p>Creates binary columns for each category. Best for nominal data.</p>
                            <div class="code-block">
                                <pre><code>import pandas as pd

# Example: Colors (nominal - no order)
colors = ['Red', 'Blue', 'Green', 'Red', 'Blue']

# One-hot encoding
df_encoded = pd.get_dummies(df, columns=['color'], prefix='color')

# Result: color_Red, color_Blue, color_Green columns with 0/1 values</code></pre>
                            </div>
                        </div>
                    </div>

                    <h3>2.3 Feature Scaling</h3>
                    <p>Different features often have different scales (e.g., age: 20-80, income: 20,000-200,000). Scaling ensures all features contribute equally to the model.</p>

                    <div class="scaling-methods">
                        <div class="method-item">
                            <h4>Standardization (Z-score normalization)</h4>
                            <p>Transforms features to have mean=0 and standard deviation=1</p>
                            <p><strong>Formula:</strong> z = (x - μ) / σ</p>
                            <div class="code-block">
                                <pre><code>from sklearn.preprocessing import StandardScaler

# Standardization example
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[['age', 'income', 'experience']])

# Result: All features have mean≈0, std≈1
print(f"Mean after scaling: {df_scaled.mean(axis=0)}")
print(f"Std after scaling: {df_scaled.std(axis=0)}")</code></pre>
                            </div>
                        </div>

                        <div class="method-item">
                            <h4>Min-Max Normalization</h4>
                            <p>Scales features to a fixed range, typically [0, 1]</p>
                            <p><strong>Formula:</strong> x_scaled = (x - min) / (max - min)</p>
                            <div class="code-block">
                                <pre><code>from sklearn.preprocessing import MinMaxScaler

# Min-Max scaling example
minmax_scaler = MinMaxScaler()
df_minmax = minmax_scaler.fit_transform(df[['age', 'income']])

# Result: All features are between 0 and 1</code></pre>
                            </div>
                        </div>
                    </div>

                    <h3>2.4 Feature Engineering</h3>
                    <p>Creating new features from existing ones to improve model performance.</p>

                    <div class="feature-engineering-examples">
                        <div class="example-item">
                            <h4>Combining Features</h4>
                            <p>Create meaningful combinations of existing features</p>
                            <div class="code-block">
                                <pre><code># Example: Mobile phone dataset
# Create new features from existing ones

# Total memory = RAM + ROM
df['total_memory'] = df['RAM'] + df['ROM']

# Pixel resolution = length × width
df['pixel_resolution'] = df['screen_length'] * df['screen_width']

# Camera quality score = Primary camera + Selfie camera
df['total_camera_quality'] = df['Primary_Cam'] + df['Selfi_Cam']

# Performance ratio = RAM / Price (value for money)
df['performance_ratio'] = df['RAM'] / df['Price']</code></pre>
                            </div>
                        </div>

                        <div class="example-item">
                            <h4>Mathematical Transformations</h4>
                            <div class="code-block">
                                <pre><code># Logarithmic transformation (for skewed data)
df['log_price'] = np.log(df['Price'] + 1)  # +1 to handle zeros

# Square root transformation
df['sqrt_battery'] = np.sqrt(df['Battery_Power'])

# Polynomial features
df['ram_squared'] = df['RAM'] ** 2

# Interaction terms
df['ram_battery_interaction'] = df['RAM'] * df['Battery_Power']</code></pre>
                            </div>
                        </div>
                    </div>
                </section>

                <section class="tutorial-section">
                    <h2>3. Exploratory Data Analysis (EDA)</h2>
                    <p>EDA is the process of investigating your dataset to understand its structure, patterns, and relationships before building models.</p>

                    <h3>3.1 Dataset Overview</h3>
                    <div class="code-block">
                        <pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('mobile_data.csv')

# Basic dataset information
print("Dataset Shape:", df.shape)
print("\nData Types:")
print(df.dtypes)

# Display first few rows
print("\nFirst 5 rows:")
print(df.head())

# Statistical summary
print("\nStatistical Summary:")
print(df.describe().transpose())</code></pre>
                    </div>

                    <h3>3.2 Missing Value Analysis</h3>
                    <div class="code-block">
                        <pre><code># Check for missing values
missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100

missing_df = pd.DataFrame({
    'Missing Count': missing_values,
    'Missing Percentage': missing_percentage
})

print("Missing Values Analysis:")
print(missing_df[missing_df['Missing Count'] > 0])</code></pre>
                    </div>

                    <h3>3.3 Target Variable Analysis</h3>
                    
                    <h4>Distribution Analysis</h4>
                    <div class="code-block">
                        <pre><code># Analyze target variable distribution
plt.figure(figsize=(15, 5))

# Subplot 1: Histogram
plt.subplot(1, 3, 1)
plt.hist(df['Price'], bins=30, edgecolor='black', alpha=0.7)
plt.title('Price Distribution (Histogram)')
plt.xlabel('Price')
plt.ylabel('Frequency')

# Subplot 2: KDE Plot
plt.subplot(1, 3, 2)
sns.kdeplot(df['Price'], shade=True)
plt.title('Price Distribution (KDE)')
plt.xlabel('Price')
plt.ylabel('Density')

# Subplot 3: Box Plot
plt.subplot(1, 3, 3)
sns.boxplot(y=df['Price'])
plt.title('Price Distribution (Box Plot)')
plt.ylabel('Price')

plt.tight_layout()
plt.show()

# Statistical measures
print(f"Mean: {df['Price'].mean():.2f}")
print(f"Median: {df['Price'].median():.2f}")
print(f"Standard Deviation: {df['Price'].std():.2f}")
print(f"Skewness: {df['Price'].skew():.2f}")
print(f"Kurtosis: {df['Price'].kurtosis():.2f}")</code></pre>
                    </div>

                    <h4>Interpretation Guide</h4>
                    <div class="interpretation-guide">
                        <div class="interpretation-item">
                            <h5>Skewness Interpretation</h5>
                            <ul>
                                <li><strong>Skewness = 0:</strong> Perfectly symmetric distribution</li>
                                <li><strong>Skewness > 0:</strong> Right-skewed (tail extends to the right)</li>
                                <li><strong>Skewness < 0:</strong> Left-skewed (tail extends to the left)</li>
                                <li><strong>|Skewness| > 1:</strong> Highly skewed</li>
                            </ul>
                        </div>
                        
                        <div class="interpretation-item">
                            <h5>Kurtosis Interpretation</h5>
                            <ul>
                                <li><strong>Kurtosis = 3:</strong> Normal distribution (mesokurtic)</li>
                                <li><strong>Kurtosis > 3:</strong> Heavy-tailed distribution (leptokurtic)</li>
                                <li><strong>Kurtosis < 3:</strong> Light-tailed distribution (platykurtic)</li>
                            </ul>
                        </div>
                    </div>

                    <h3>3.4 Outlier Detection</h3>
                    
                    <h4>IQR Method (Interquartile Range)</h4>
                    <div class="code-block">
                        <pre><code>def detect_outliers_iqr(df, column):
    """
    Detect outliers using the IQR method
    Outliers are defined as:
    - Lower bound: Q1 - 1.5 * IQR
    - Upper bound: Q3 + 1.5 * IQR
    """
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    
    print(f"Outlier Analysis for {column}:")
    print(f"Q1: {Q1:.2f}")
    print(f"Q3: {Q3:.2f}")
    print(f"IQR: {IQR:.2f}")
    print(f"Lower Bound: {lower_bound:.2f}")
    print(f"Upper Bound: {upper_bound:.2f}")
    print(f"Number of outliers: {len(outliers)}")
    print(f"Percentage of outliers: {len(outliers)/len(df)*100:.2f}%")
    
    return outliers

# Example usage
price_outliers = detect_outliers_iqr(df, 'Price')
print(price_outliers[['Price']].head())</code></pre>
                    </div>

                    <div class="next-section">
                        <h3>Continue to Part 2</h3>
                        <p>In the next part of this tutorial, we'll cover:</p>
                        <ul>
                            <li>Correlation analysis and multicollinearity detection</li>
                            <li>Building and evaluating regression models</li>
                            <li>Classification techniques and metrics</li>
                            <li>Model selection and hyperparameter tuning</li>
                        </ul>
                        <a href="ml-fundamentals-part2.html" class="btn btn-primary">Continue to Part 2 →</a>
                    </div>
                </section>
            </div>

            <div class="tutorial-navigation">
                <a href="../tutorials.html" class="btn btn-secondary">← Back to Tutorials</a>
                <a href="ml-fundamentals-part2.html" class="btn btn-primary">Part 2 →</a>
            </div>
        </div>
    </main>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2024 Ali Barzin Zanganeh. All rights reserved.</p>
        </div>
    </footer>

    <script src="../static/js/main.js"></script>
</body>
</html>
