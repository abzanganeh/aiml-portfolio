{% extends "tutorial_detail.html" %}

{% block tutorial_content %}
<div class="tutorial-section">
    <h2>Introduction to Decision Trees</h2>
    <p>
        Decision trees are versatile machine learning algorithms that can be used for both classification 
        and regression tasks. They work by recursively splitting the data based on feature values to 
        create a tree-like model of decisions that leads to predictions.
    </p>
    
    <div class="highlight-box">
        <h3>üå≥ Why Decision Trees?</h3>
        <ul>
            <li><strong>Interpretable:</strong> Easy to understand and visualize</li>
            <li><strong>No assumptions:</strong> Don't require linear relationships</li>
            <li><strong>Handle mixed data:</strong> Work with numerical and categorical features</li>
            <li><strong>Feature selection:</strong> Automatically identify important features</li>
            <li><strong>Non-parametric:</strong> No assumptions about data distribution</li>
        </ul>
    </div>
</div>

<div class="tutorial-section">
    <h2>How Decision Trees Work</h2>
    
    <h3>The Tree Building Process</h3>
    <ol>
        <li><strong>Root Node:</strong> Start with the entire dataset</li>
        <li><strong>Feature Selection:</strong> Choose the best feature to split on</li>
        <li><strong>Splitting:</strong> Divide data based on feature threshold</li>
        <li><strong>Recursion:</strong> Repeat process for each subset</li>
        <li><strong>Stopping:</strong> Stop when criteria are met (depth, purity, etc.)</li>
    </ol>

    <h3>Splitting Criteria</h3>
    
    <h4>For Classification Trees:</h4>
    <div class="formula-box">
        <strong>Gini Impurity:</strong> Gini = 1 - Œ£(p·µ¢)¬≤<br>
        <strong>Entropy:</strong> Entropy = -Œ£(p·µ¢ √ó log‚ÇÇ(p·µ¢))<br>
        <strong>Information Gain:</strong> IG = Entropy(parent) - Œ£(weighted Entropy(children))
    </div>

    <h4>For Regression Trees:</h4>
    <div class="formula-box">
        <strong>Mean Squared Error:</strong> MSE = Œ£(y·µ¢ - »≥)¬≤ / n<br>
        <strong>Mean Absolute Error:</strong> MAE = Œ£|y·µ¢ - »≥| / n
    </div>
</div>

<div class="tutorial-section">
    <h2>Implementation Example</h2>
    <p>Here's how to implement decision trees using scikit-learn:</p>
    
    <div class="code-example">import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error
from sklearn.tree import plot_tree, export_text
import matplotlib.pyplot as plt

# Load and prepare data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Classification Tree
clf_tree = DecisionTreeClassifier(
    criterion='gini',           # or 'entropy'
    max_depth=5,               # limit tree depth
    min_samples_split=20,      # minimum samples to split
    min_samples_leaf=10,       # minimum samples in leaf
    random_state=42
)

# Train the model
clf_tree.fit(X_train, y_train)

# Make predictions
y_pred = clf_tree.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Feature importance
feature_importance = clf_tree.feature_importances_
for i, importance in enumerate(feature_importance):
    print(f"Feature {i}: {importance:.4f}")

# Visualize the tree
plt.figure(figsize=(15, 10))
plot_tree(clf_tree, max_depth=3, filled=True, feature_names=feature_names, class_names=class_names)
plt.title("Decision Tree Visualization")
plt.show()

# Text representation
tree_rules = export_text(clf_tree, feature_names=feature_names)
print("Decision Tree Rules:")
print(tree_rules)</div>
</div>

<div class="tutorial-section">
    <h2>Advantages and Disadvantages</h2>
    
    <div class="pros-cons-grid">
        <div class="pros-box">
            <h3>‚úÖ Advantages</h3>
            <ul>
                <li>Easy to understand and interpret</li>
                <li>Requires little data preparation</li>
                <li>Handles both numerical and categorical data</li>
                <li>Can model non-linear relationships</li>
                <li>Automatic feature selection</li>
                <li>Can handle missing values</li>
                <li>Robust to outliers</li>
            </ul>
        </div>
        
        <div class="cons-box">
            <h3>‚ùå Disadvantages</h3>
            <ul>
                <li>Prone to overfitting</li>
                <li>Unstable (small data changes = different trees)</li>
                <li>Biased toward features with more levels</li>
                <li>Can create overly complex trees</li>
                <li>Difficulty with linear relationships</li>
                <li>Can be biased with imbalanced datasets</li>
            </ul>
        </div>
    </div>
</div>

<div class="tutorial-section">
    <h2>Hyperparameter Tuning</h2>
    
    <h3>Key Parameters</h3>
    <ul>
        <li><strong>max_depth:</strong> Maximum depth of the tree</li>
        <li><strong>min_samples_split:</strong> Minimum samples required to split</li>
        <li><strong>min_samples_leaf:</strong> Minimum samples required in a leaf</li>
        <li><strong>max_features:</strong> Number of features to consider for splits</li>
        <li><strong>criterion:</strong> Splitting criterion (gini, entropy, mse)</li>
    </ul>

    <div class="code-example">from sklearn.model_selection import GridSearchCV

# Parameter grid
param_grid = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 5, 10],
    'criterion': ['gini', 'entropy']
}

# Grid search with cross-validation
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best cross-validation score:", grid_search.best_score_)

# Use best model
best_tree = grid_search.best_estimator_
y_pred_best = best_tree.predict(X_test)
print("Test accuracy:", accuracy_score(y_test, y_pred_best))</div>
</div>

<div class="tutorial-section">
    <h2>Ensemble Methods</h2>
    <p>Decision trees are often used as building blocks for more powerful ensemble methods:</p>
    
    <div class="ensemble-grid">
        <div class="ensemble-method">
            <h3>üå≤ Random Forest</h3>
            <p>Combines multiple decision trees with random feature selection</p>
            <ul>
                <li>Reduces overfitting</li>
                <li>Provides feature importance</li>
                <li>Handles large datasets well</li>
            </ul>
        </div>
        
        <div class="ensemble-method">
            <h3>‚ö° Gradient Boosting</h3>
            <p>Sequentially builds trees to correct previous mistakes</p>
            <ul>
                <li>High predictive accuracy</li>
                <li>Works well with different data types</li>
                <li>Includes XGBoost, LightGBM, CatBoost</li>
            </ul>
        </div>
        
        <div class="ensemble-method">
            <h3>üéØ AdaBoost</h3>
            <p>Adaptive boosting that focuses on misclassified examples</p>
            <ul>
                <li>Good for binary classification</li>
                <li>Reduces bias and variance</li>
                <li>Less prone to overfitting</li>
            </ul>
        </div>
    </div>
</div>

<div class="tutorial-section">
    <h2>üöÄ Next Steps</h2>
    <div class="example-box">
        <h3>Continue Your Learning Journey</h3>
        <ul>
            <li><strong>Practice:</strong> Implement decision trees on different datasets</li>
            <li><strong>Ensemble Methods:</strong> Learn Random Forest and Gradient Boosting</li>
            <li><strong>Visualization:</strong> Practice interpreting tree structures</li>
            <li><strong>Feature Engineering:</strong> Improve model performance with better features</li>
        </ul>
        
        <div style="margin-top: 1.5rem;">
            <a href="{{ url_for('tutorial_detail', slug='neural-networks') }}" class="btn btn-primary" style="margin-right: 1rem;">Next: Neural Networks</a>
            <a href="{{ url_for('tutorials_list') }}" class="btn btn-secondary">All Tutorials</a>
        </div>
    </div>
</div>
{% endblock %}
