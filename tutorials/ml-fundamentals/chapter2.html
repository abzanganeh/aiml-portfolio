<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Regression Analysis - Ali Zanganeh</title>
    <link rel="stylesheet" href="../../assets/css/style.css">
</head>
<body>
    <header class="azbn-header">
        <div class="azbn-container">
            <h1><a href="../../" style="text-decoration: none; color: #4f46e5;">Ali Zanganeh</a></h1>
            <nav>
                <a href="../../#home">Home</a>
                <a href="../">Tutorials</a>
                <a href="./chapter1.html">‚Üê Chapter 1</a>
                <a href="./chapter3.html">Chapter 3 ‚Üí</a>
            </nav>
        </div>
    </header>

    <main style="padding-top: 100px;">
        <section class="azbn-section">
            <div class="azbn-container">
                <h1>Chapter 2: Regression Analysis Mastery</h1>
                <p class="azbn-subtitle">From linear relationships to complex polynomial modeling with mathematical foundations</p>
                
                <div class="azbn-card" style="background: linear-gradient(135deg, #43a047 0%, #1e88e5 100%); color: white; margin-bottom: 2rem;">
                    <h2>üéØ Learning Objectives</h2>
                    <ul style="color: white;">
                        <li>Master linear regression theory and mathematical foundations</li>
                        <li>Understand polynomial regression and feature engineering</li>
                        <li>Learn multiple regression with feature importance analysis</li>
                        <li>Evaluate models using proper metrics (MSE, MAE, R¬≤)</li>
                        <li>Recognize and prevent overfitting in regression models</li>
                        <li>Apply regularization techniques (Ridge, Lasso)</li>
                    </ul>
                </div>

                <h2>üìä What is Regression?</h2>
                <div class="azbn-card">
                    <h3>Core Concept and Mathematical Foundation</h3>
                    <p><strong>Regression Analysis</strong> is a supervised learning technique used to predict continuous numerical values by modeling the relationship between input features and target variables.</p>
                    
                    <div style="background: #e8f5e8; padding: 1.5rem; border-radius: 10px; margin: 1.5rem 0;">
                        <h4>üéØ The Fundamental Equation:</h4>
                        <div style="text-align: center; font-size: 1.2rem; margin: 1rem 0;">
                            <strong>y = f(X) + Œµ</strong>
                        </div>
                        <ul style="margin-top: 1rem;">
                            <li><strong>y:</strong> Target variable (what we want to predict)</li>
                            <li><strong>f(X):</strong> The function we want to learn</li>
                            <li><strong>X:</strong> Input features (independent variables)</li>
                            <li><strong>Œµ:</strong> Error term (noise and unmeasured factors)</li>
                        </ul>
                    </div>

                    <h4>üîç Real-World Regression Examples:</h4>
                    <div class="azbn-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1rem; margin: 1rem 0;">
                        <div style="border: 1px solid #ddd; padding: 1rem; border-radius: 8px;">
                            <h5>üè† Real Estate Pricing</h5>
                            <p><strong>Predict:</strong> House price</p>
                            <p><strong>Features:</strong> Size, location, bedrooms, age</p>
                            <p><strong>Why Linear:</strong> Generally, larger houses cost more</p>
                        </div>
                        <div style="border: 1px solid #ddd; padding: 1rem; border-radius: 8px;">
                            <h5>üìà Stock Market Analysis</h5>
                            <p><strong>Predict:</strong> Stock price movement</p>
                            <p><strong>Features:</strong> Trading volume, market indicators</p>
                            <p><strong>Challenge:</strong> Non-linear, highly volatile</p>
                        </div>
                        <div style="border: 1px solid #ddd; padding: 1rem; border-radius: 8px;">
                            <h5>üå°Ô∏è Weather Forecasting</h5>
                            <p><strong>Predict:</strong> Tomorrow's temperature</p>
                            <p><strong>Features:</strong> Today's weather, pressure, humidity</p>
                            <p><strong>Complexity:</strong> Seasonal patterns, non-linear trends</p>
                        </div>
                    </div>
                </div>

                <h2>üìè Linear Regression: The Foundation</h2>
                <div class="azbn-card">
                    <h3>Mathematical Deep Dive</h3>
                    
                    <div style="background: #f8f9fa; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h4>üî¢ Simple Linear Regression Formula:</h4>
                        <div style="text-align: center; font-size: 1.3rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œµ</strong>
                        </div>
                        <ul>
                            <li><strong>Œ≤‚ÇÄ (Beta Zero):</strong> Y-intercept - value when x = 0</li>
                            <li><strong>Œ≤‚ÇÅ (Beta One):</strong> Slope - change in y per unit change in x</li>
                            <li><strong>x:</strong> Independent variable (feature)</li>
                            <li><strong>y:</strong> Dependent variable (target)</li>
                            <li><strong>Œµ:</strong> Random error term</li>
                        </ul>
                    </div>

                    <h4>üéØ Key Assumptions of Linear Regression:</h4>
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px; border-left: 4px solid #2196f3;">
                            <h5>1Ô∏è‚É£ Linearity</h5>
                            <p>The relationship between X and y is linear</p>
                            <p><em>Check: Scatter plots, residual plots</em></p>
                        </div>
                        <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px; border-left: 4px solid #9c27b0;">
                            <h5>2Ô∏è‚É£ Independence</h5>
                            <p>Observations are independent of each other</p>
                            <p><em>Important for time series and spatial data</em></p>
                        </div>
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px; border-left: 4px solid #4caf50;">
                            <h5>3Ô∏è‚É£ Homoscedasticity</h5>
                            <p>Constant variance of residuals</p>
                            <p><em>Check: Residuals vs fitted values plot</em></p>
                        </div>
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px; border-left: 4px solid #ff9800;">
                            <h5>4Ô∏è‚É£ Normality</h5>
                            <p>Residuals are normally distributed</p>
                            <p><em>Check: Q-Q plots, Shapiro-Wilk test</em></p>
                        </div>
                    </div>

                    <h4>üìê How Linear Regression Works - The Math Behind the Magic:</h4>
                    <div style="background: #fff3e0; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h5>üéØ Ordinary Least Squares (OLS) Method:</h5>
                        <p>Linear regression finds the best line by minimizing the sum of squared residuals:</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Objective Function:</strong></p>
                            <div style="text-align: center; font-size: 1.1rem; margin: 0.5rem 0;">
                                <strong>Minimize: Œ£(y·µ¢ - ≈∑·µ¢)¬≤</strong>
                            </div>
                            <p style="font-size: 0.9rem; text-align: center; margin-top: 0.5rem;">
                                Where ≈∑·µ¢ = Œ≤‚ÇÄ + Œ≤‚ÇÅx·µ¢ (predicted value)
                            </p>
                        </div>

                        <h5>üìä The Solution (for simple linear regression):</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Slope (Œ≤‚ÇÅ):</strong></p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                Œ≤‚ÇÅ = Œ£((x·µ¢ - xÃÑ)(y·µ¢ - »≥)) / Œ£((x·µ¢ - xÃÑ)¬≤)
                            </div>
                            <p><strong>Intercept (Œ≤‚ÇÄ):</strong></p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                Œ≤‚ÇÄ = »≥ - Œ≤‚ÇÅxÃÑ
                            </div>
                        </div>

                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 6px; margin-top: 1rem;">
                            <strong>üí° Intuition:</strong> The slope tells us the correlation scaled by the ratio of standard deviations. The intercept ensures the line passes through the point (xÃÑ, »≥).
                        </div>
                    </div>
                </div>

                <h2>üåü Multiple Linear Regression</h2>
                <div class="azbn-card">
                    <h3>Extending to Multiple Features</h3>
                    
                    <div style="background: #f8f9fa; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h4>üî¢ Multiple Regression Formula:</h4>
                        <div style="text-align: center; font-size: 1.2rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çöx‚Çö + Œµ</strong>
                        </div>
                        <p>Or in matrix form: <strong>y = XŒ≤ + Œµ</strong></p>
                        <ul>
                            <li><strong>p:</strong> Number of features</li>
                            <li><strong>Œ≤‚±º:</strong> Coefficient for feature x‚±º</li>
                            <li><strong>X:</strong> Design matrix (n √ó p matrix)</li>
                            <li><strong>Œ≤:</strong> Parameter vector</li>
                        </ul>
                    </div>

                    <h4>üéØ Feature Importance and Interpretation:</h4>
                    <div style="background: #e8f5e8; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h5>üìä Coefficient Interpretation:</h5>
                        <ul>
                            <li><strong>Magnitude:</strong> Larger |Œ≤‚±º| means more influence on prediction</li>
                            <li><strong>Sign:</strong> Positive Œ≤ increases y, negative Œ≤ decreases y</li>
                            <li><strong>Units:</strong> Œ≤‚±º represents change in y per unit change in x‚±º</li>
                        </ul>
                        
                        <div style="background: #fff3e0; padding: 1rem; border-radius: 6px; margin-top: 1rem;">
                            <strong>‚ö†Ô∏è Important Caveat:</strong> Coefficients represent the effect of changing one feature while holding all others constant. In practice, features are often correlated!
                        </div>
                    </div>

                    <h4>üîó Multicollinearity: When Features are Too Similar</h4>
                    <div style="background: #ffebee; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h5>‚ùå Problems with Highly Correlated Features:</h5>
                        <ul>
                            <li>Unstable coefficient estimates</li>
                            <li>Difficult to interpret individual feature importance</li>
                            <li>High variance in predictions</li>
                            <li>Numerical instability in matrix inversion</li>
                        </ul>

                        <h5>üîç Detection Methods:</h5>
                        <ul>
                            <li><strong>Correlation Matrix:</strong> Look for correlations > 0.8</li>
                            <li><strong>Variance Inflation Factor (VIF):</strong> VIF > 10 indicates problems</li>
                            <li><strong>Condition Number:</strong> > 30 suggests multicollinearity</li>
                        </ul>

                        <h5>üí° Solutions:</h5>
                        <ul>
                            <li>Remove highly correlated features</li>
                            <li>Use Principal Component Analysis (PCA)</li>
                            <li>Apply regularization (Ridge, Lasso)</li>
                            <li>Collect more data if possible</li>
                        </ul>
                    </div>
                </div>

                <h2>üå™Ô∏è Polynomial Regression: Capturing Non-Linear Relationships</h2>
                <div class="azbn-card">
                    <h3>Beyond Straight Lines</h3>
                    
                    <div style="background: #f3e5f5; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h4>üîÑ Polynomial Transformation:</h4>
                        <p>Polynomial regression extends linear regression by adding polynomial features:</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Degree 2 (Quadratic):</strong></p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + Œµ
                            </div>
                            <p><strong>Degree 3 (Cubic):</strong></p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + Œ≤‚ÇÉx¬≥ + Œµ
                            </div>
                            <p><strong>General Form:</strong></p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + ... + Œ≤‚Çêx·µà + Œµ
                            </div>
                        </div>

                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 6px; margin-top: 1rem;">
                            <strong>üéØ Key Insight:</strong> Polynomial regression is still linear in the parameters Œ≤! We just transform the features.
                        </div>
                    </div>

                    <h4>‚öñÔ∏è The Bias-Variance Tradeoff</h4>
                    <div style="background: #fff8e1; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem;">
                            <div style="background: #ffebee; padding: 1rem; border-radius: 6px;">
                                <h5>üìâ Underfitting (High Bias)</h5>
                                <ul style="font-size: 0.9rem;">
                                    <li>Model too simple</li>
                                    <li>Cannot capture underlying pattern</li>
                                    <li>Poor performance on both training and test data</li>
                                    <li><strong>Solution:</strong> Increase model complexity</li>
                                </ul>
                            </div>
                            <div style="background: #e8f5e8; padding: 1rem; border-radius: 6px;">
                                <h5>üìà Overfitting (High Variance)</h5>
                                <ul style="font-size: 0.9rem;">
                                    <li>Model too complex</li>
                                    <li>Memorizes training data noise</li>
                                    <li>Good training, poor test performance</li>
                                    <li><strong>Solution:</strong> Reduce complexity or add data</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 6px; margin-top: 1rem;">
                            <strong>üéØ Sweet Spot:</strong> Find the optimal degree that minimizes total error = bias¬≤ + variance + noise
                        </div>
                    </div>

                    <h4>üéöÔ∏è Choosing the Right Polynomial Degree</h4>
                    <div style="background: #e8f5e8; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h5>üìä Practical Guidelines:</h5>
                        <ul>
                            <li><strong>Degree 1:</strong> Linear relationship</li>
                            <li><strong>Degree 2:</strong> One curve (parabola) - good for many real-world phenomena</li>
                            <li><strong>Degree 3-4:</strong> More complex curves with multiple turns</li>
                            <li><strong>Degree >5:</strong> Usually overfitting unless you have lots of data</li>
                        </ul>

                        <h5>üîç Selection Methods:</h5>
                        <ol>
                            <li><strong>Cross-Validation:</strong> Test different degrees, pick best CV score</li>
                            <li><strong>Learning Curves:</strong> Plot training vs validation error</li>
                            <li><strong>Information Criteria:</strong> AIC, BIC balance fit and complexity</li>
                            <li><strong>Domain Knowledge:</strong> Physics/business understanding of relationship</li>
                        </ol>

                        <div style="background: #fff3e0; padding: 1rem; border-radius: 6px; margin-top: 1rem;">
                            <strong>üí° Pro Tip:</strong> Start simple (degree 1-2) and increase complexity only if validation performance improves!
                        </div>
                    </div>
                </div>

                <h2>üìè Regression Evaluation Metrics</h2>
                <div class="azbn-card">
                    <h3>Measuring Model Performance</h3>
                    
                    <div style="background: #f8f9fa; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h4>üéØ Essential Regression Metrics:</h4>
                        
                        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1rem; margin: 1rem 0;">
                            <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                                <h5>1Ô∏è‚É£ Mean Squared Error (MSE)</h5>
                                <div style="text-align: center; margin: 0.5rem 0; background: white; padding: 0.5rem; border-radius: 4px;">
                                    <strong>MSE = (1/n) Œ£(y·µ¢ - ≈∑·µ¢)¬≤</strong>
                                </div>
                                <p><strong>Pros:</strong> Heavily penalizes large errors</p>
                                <p><strong>Cons:</strong> Same units as y¬≤, hard to interpret</p>
                                <p><strong>Use when:</strong> Large errors are especially bad</p>
                            </div>

                            <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                                <h5>2Ô∏è‚É£ Root Mean Squared Error (RMSE)</h5>
                                <div style="text-align: center; margin: 0.5rem 0; background: white; padding: 0.5rem; border-radius: 4px;">
                                    <strong>RMSE = ‚àöMSE</strong>
                                </div>
                                <p><strong>Pros:</strong> Same units as y, interpretable</p>
                                <p><strong>Cons:</strong> Still penalizes large errors heavily</p>
                                <p><strong>Use when:</strong> You want MSE benefits with interpretability</p>
                            </div>

                            <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                                <h5>3Ô∏è‚É£ Mean Absolute Error (MAE)</h5>
                                <div style="text-align: center; margin: 0.5rem 0; background: white; padding: 0.5rem; border-radius: 4px;">
                                    <strong>MAE = (1/n) Œ£|y·µ¢ - ≈∑·µ¢|</strong>
                                </div>
                                <p><strong>Pros:</strong> Robust to outliers, easy to interpret</p>
                                <p><strong>Cons:</strong> Doesn't distinguish small vs large errors</p>
                                <p><strong>Use when:</strong> You have outliers or all errors are equally bad</p>
                            </div>

                            <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px;">
                                <h5>4Ô∏è‚É£ R-squared (R¬≤)</h5>
                                <div style="text-align: center; margin: 0.5rem 0; background: white; padding: 0.5rem; border-radius: 4px;">
                                    <strong>R¬≤ = 1 - (SS_res / SS_tot)</strong>
                                </div>
                                <p><strong>Range:</strong> 0 to 1 (higher is better)</p>
                                <p><strong>Interpretation:</strong> % of variance explained</p>
                                <p><strong>Caveat:</strong> Can be misleading with non-linear relationships</p>
                            </div>
                        </div>

                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px; margin-top: 1rem;">
                            <h4>üìä Which Metric to Use?</h4>
                            <ul style="margin: 0.5rem 0;">
                                <li><strong>RMSE:</strong> Most common, good for normally distributed errors</li>
                                <li><strong>MAE:</strong> When you have outliers or skewed error distribution</li>
                                <li><strong>R¬≤:</strong> For understanding model explanatory power</li>
                                <li><strong>Multiple metrics:</strong> Always use several metrics for complete picture!</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <h2>üõ°Ô∏è Regularization: Preventing Overfitting</h2>
                <div class="azbn-card">
                    <h3>Ridge and Lasso Regression</h3>
                    
                    <div style="background: #e3f2fd; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h4>üéØ Why Regularization?</h4>
                        <p>When we have many features or polynomial terms, the model can become too complex and overfit. Regularization adds a penalty term to prevent this.</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>General Regularized Objective:</strong></p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                <strong>Minimize: MSE + Œª √ó Penalty(Œ≤)</strong>
                            </div>
                            <p style="font-size: 0.9rem; text-align: center;">Where Œª (lambda) controls the strength of regularization</p>
                        </div>
                    </div>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üèîÔ∏è Ridge Regression (L2)</h4>
                            <div style="background: white; padding: 0.8rem; border-radius: 4px; margin: 0.5rem 0;">
                                <strong>Penalty = Œ£Œ≤‚±º¬≤</strong>
                            </div>
                            <h5>Characteristics:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li>Shrinks coefficients toward zero</li>
                                <li>Keeps all features (no feature selection)</li>
                                <li>Good when all features are somewhat relevant</li>
                                <li>Handles multicollinearity well</li>
                            </ul>
                            <div style="background: #f1f8e9; padding: 0.5rem; border-radius: 4px; margin-top: 0.5rem;">
                                <strong>Best for:</strong> Many relevant features
                            </div>
                        </div>

                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>üóª Lasso Regression (L1)</h4>
                            <div style="background: white; padding: 0.8rem; border-radius: 4px; margin: 0.5rem 0;">
                                <strong>Penalty = Œ£|Œ≤‚±º|</strong>
                            </div>
                            <h5>Characteristics:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li>Can set coefficients exactly to zero</li>
                                <li>Automatic feature selection</li>
                                <li>Produces sparse models</li>
                                <li>Good when only some features are relevant</li>
                            </ul>
                            <div style="background: #fef7e0; padding: 0.5rem; border-radius: 4px; margin-top: 0.5rem;">
                                <strong>Best for:</strong> Feature selection needed
                            </div>
                        </div>
                    </div>

                    <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px; margin-top: 1rem;">
                        <h4>üéöÔ∏è Choosing Œª (Regularization Strength):</h4>
                        <ul>
                            <li><strong>Œª = 0:</strong> No regularization (standard regression)</li>
                            <li><strong>Small Œª:</strong> Light penalty, close to unregularized</li>
                            <li><strong>Large Œª:</strong> Heavy penalty, coefficients shrink toward zero</li>
                            <li><strong>Œª ‚Üí ‚àû:</strong> All coefficients approach zero (underfitting)</li>
                        </ul>
                        <div style="background: #e1bee7; padding: 0.8rem; border-radius: 4px; margin-top: 0.5rem;">
                            <strong>üí° Selection Method:</strong> Use cross-validation to find optimal Œª that minimizes validation error!
                        </div>
                    </div>
                </div>

                <h2>üéØ Key Takeaways and Best Practices</h2>
                <div class="azbn-deployment-status">
                    <p><strong>‚úÖ Chapter 2 Mastery:</strong></p>
                    <p>‚Ä¢ Linear regression mathematical foundations and assumptions</p>
                    <p>‚Ä¢ Multiple regression with feature importance interpretation</p>
                    <p>‚Ä¢ Polynomial regression for non-linear relationships</p>
                    <p>‚Ä¢ Comprehensive evaluation metrics (MSE, RMSE, MAE, R¬≤)</p>
                    <p>‚Ä¢ Overfitting detection and regularization techniques</p>
                    <p>‚Ä¢ Practical model selection and validation strategies</p>
                </div>

                <div style="background: #e3f2fd; padding: 1.5rem; border-radius: 10px; margin: 2rem 0;">
                    <h3>üéì Practical Guidelines for Regression Success:</h3>
                    <ol>
                        <li><strong>Always start simple:</strong> Begin with linear regression before trying polynomial</li>
                        <li><strong>Check assumptions:</strong> Plot residuals to verify linearity and homoscedasticity</li>
                        <li><strong>Handle multicollinearity:</strong> Use correlation matrices and VIF</li>
                        <li><strong>Use multiple metrics:</strong> Don't rely on R¬≤ alone</li>
                        <li><strong>Validate properly:</strong> Use cross-validation for model selection</li>
                        <li><strong>Consider regularization:</strong> Especially with many features or limited data</li>
                        <li><strong>Understand your domain:</strong> Let business knowledge guide feature engineering</li>
                    </ol>
                </div>

                <div style="margin: 2rem 0; text-align: center;">
                    <a href="./chapter1.html" class="azbn-btn azbn-secondary" style="text-decoration: none; margin-right: 1rem;">‚Üê Chapter 1: Introduction</a>
                    <a href="./chapter3.html" class="azbn-btn" style="text-decoration: none;">Next: Chapter 3 - Classification ‚Üí</a>
                </div>
            </div>
        </section>
    </main>
</body>
</html>
