<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Complete Introduction to Machine Learning - Ali Zanganeh</title>
    <link rel="stylesheet" href="../../assets/css/style.css">
    <link rel="stylesheet" href="../../assets/css/ml_fundamentals_ch1.css">

</head>
<body>
    <a id="top"></a>
    <div class="container">
        <div class="header">
            <h1><span class="emoji">ü§ñ</span>Machine Learning Masterclass</h1>
            <p>Chapter 1: From Zero to ML Hero - Complete Interactive Guide</p>
            <div class="progress-bar">
                <div class="progress-fill" style="width: 100%"></div>
            </div>
            
        </div>
                 <div class="chapter-navigation">
                    <h2>üéì Course Navigation</h2>
                    <div class="chapter-nav-container">
                        <a href="./chapter1.html" class="azbn-btn azbn-secondary chapter-nav-btn">Chapter 1: Introduction</a>
                        <a href="./chapter2.html" class="azbn-btn azbn-secondary chapter-nav-btn">Chapter 2: Regression</a>
                        <a href="./chapter3.html" class="azbn-btn azbn-secondary chapter-nav-btn">Chapter 3: Classification</a>
                    </div>
                </div>

        <div class="section">
            <h2><span class="emoji">üéØ</span>What You'll Master Today</h2>
            <div class="grid">
                <div class="card">
                    <h3>üß† Core Concepts</h3>
                    <ul>
                        <li>Deep understanding of ML types and when to use each</li>
                        <li>Mathematical intuition behind algorithms</li>
                        <li>Real-world problem identification</li>
                        <li>Data quality assessment</li>
                    </ul>
                </div>
                <div class="card">
                    <h3>üíª Technical Skills</h3>
                    <ul>
                        <li>Complete Python ML toolkit mastery</li>
                        <li>Advanced data preprocessing techniques</li>
                        <li>Model evaluation and interpretation</li>
                        <li>Production-ready code practices</li>
                    </ul>
                </div>
                <div class="card">
                    <h3>üöÄ Practical Projects</h3>
                    <ul>
                        <li>End-to-end Iris classification</li>
                        <li>Real estate price prediction</li>
                        <li>Customer segmentation analysis</li>
                        <li>Time series forecasting basics</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">ü§ñ</span>Machine Learning: The Complete Picture</h2>
            
            <div class="highlight-box info">
                <h3>üéì The Fundamental Definition</h3>
                <p><strong>Machine Learning</strong> is the science of programming computers to learn patterns from data and make predictions or decisions without being explicitly programmed for every possible scenario.</p>
            </div>

            <div class="card">
                <h3>üîç The Paradigm Shift</h3>
                <div class="grid comparison-grid">
                    <div class="traditional-programming">
                        <h4>‚ùå Traditional Programming</h4>
                        <p><strong>Input:</strong> Data + Rules (Code)</p>
                        <p><strong>Output:</strong> Answers</p>
                        <p><strong>Problem:</strong> We must anticipate every scenario and write explicit rules for each case.</p>
                    </div>
                    <div class="machine-learning">
                        <h4>‚úÖ Machine Learning</h4>
                        <p><strong>Input:</strong> Data + Answers (Examples)</p>
                        <p><strong>Output:</strong> Rules (Model)</p>
                        <p><strong>Advantage:</strong> The system discovers patterns and rules automatically from examples.</p>
                    </div>
                </div>
            </div>

            <div class="interactive-demo" style="color: #fff;"></div>
                <h3>üåü Real-World Impact Stories</h3>
                <div class="tabs">
                    <button class="tab active" onclick="showTab('healthcare')">üè• Healthcare</button>
                    <button class="tab" onclick="showTab('finance')">üí∞ Finance</button>
                    <button class="tab" onclick="showTab('tech')">üì± Technology</button>
                    <button class="tab" onclick="showTab('transport')">üöó Transportation</button>
                </div>
                
                <div id="healthcare" class="tab-content active">
                    <h4>Revolutionary Medical Diagnostics</h4>
                    <p><strong>Problem:</strong> Radiologists can miss up to 30% of early-stage cancers in mammograms.</p>
                    <p><strong>ML Solution:</strong> Google's AI system can detect breast cancer with 94.5% accuracy, reducing false negatives by 9.4%.</p>
                    <p><strong>Impact:</strong> Earlier detection saves thousands of lives annually and reduces healthcare costs by billions.</p>
                </div>
                
                <div id="finance" class="tab-content">
                    <h4>Fraud Detection & Risk Assessment</h4>
                    <p><strong>Problem:</strong> Credit card fraud costs $28 billion annually worldwide.</p>
                    <p><strong>ML Solution:</strong> Real-time transaction analysis using ensemble methods and neural networks.</p>
                    <p><strong>Impact:</strong> Fraud detection accuracy improved from 60% to 99.9%, with false positives down 70%.</p>
                </div>
                
                <div id="tech" class="tab-content">
                    <h4>Personalized User Experiences</h4>
                    <p><strong>Problem:</strong> Information overload - Netflix has 15,000+ titles, YouTube uploads 500 hours/minute.</p>
                    <p><strong>ML Solution:</strong> Collaborative filtering and deep learning recommendation systems.</p>
                    <p><strong>Impact:</strong> 80% of Netflix viewing comes from recommendations, saving users 1+ hour daily in search time.</p>
                </div>
                
                <div id="transport" class="tab-content">
                    <h4>Autonomous Vehicle Safety</h4>
                    <p><strong>Problem:</strong> Human error causes 94% of serious traffic crashes (1.35M deaths annually).</p>
                    <p><strong>ML Solution:</strong> Computer vision, sensor fusion, and reinforcement learning for navigation.</p>
                    <p><strong>Impact:</strong> Waymo's self-driving cars are 2.3x safer than human drivers per mile driven.</p>
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üìä</span>The Three Pillars of Machine Learning</h2>
            
            <div class="card">
                <h3><span class="emoji">üßë‚Äçüè´</span>Supervised Learning: Learning with a Teacher</h3>
                
                <div class="highlight-box info">
                    <h4>üéØ Core Concept</h4>
                    <p>Like a student learning with a teacher who provides correct answers. The algorithm learns by studying input-output pairs (labeled examples) and then makes predictions on new, unseen data.</p>
                </div>

                <div class="grid">
                    <div class="card">
                        <h4>üî¢ Regression: Predicting Numbers</h4>
                        <p><strong>Goal:</strong> Predict continuous numerical values</p>
                        <p><strong>Mathematical Foundation:</strong> Finding the best function f(x) that maps inputs to continuous outputs</p>
                        
                        <h5>Real Examples with Business Impact:</h5>
                        <ul>
                            <li><strong>Real Estate:</strong> Zillow's Zestimate uses 100+ features to predict home values (¬±1.9% median error)</li>
                            <li><strong>Stock Trading:</strong> Renaissance Technologies' Medallion Fund uses ML for 66% annual returns</li>
                            <li><strong>Energy:</strong> Google reduced data center cooling costs by 40% using ML temperature prediction</li>
                            <li><strong>Retail:</strong> Amazon's demand forecasting prevents $1.1B in overstock annually</li>
                        </ul>

                        <div class="highlight-box tip">
                            <strong>üí° Key Insight:</strong> Regression answers "How much?" or "How many?" questions. If you can measure it with a ruler, scale, or stopwatch, it's likely a regression problem.
                        </div>
                    </div>

                    <div class="card">
                        <h4>üè∑Ô∏è Classification: Predicting Categories</h4>
                        <p><strong>Goal:</strong> Predict discrete categories or classes</p>
                        <p><strong>Mathematical Foundation:</strong> Finding decision boundaries that separate different classes in feature space</p>

                        <h5>Classification Types:</h5>
                        <div class="grid" style="grid-template-columns: 1fr 1fr;">
                            <div style="background: #e3f2fd; padding: 15px; border-radius: 8px;">
                                <h6>Binary Classification</h6>
                                <p>Two possible outcomes (Yes/No, Spam/Ham, Fraud/Legitimate)</p>
                                <ul>
                                    <li>Email spam detection (99.9% accuracy)</li>
                                    <li>Medical diagnosis (Cancer/No Cancer)</li>
                                    <li>Credit approval (Approve/Deny)</li>
                                </ul>
                            </div>
                            <div style="background: #fff3e0; padding: 15px; border-radius: 8px;">
                                <h6>Multi-class Classification</h6>
                                <p>Multiple possible outcomes (A, B, C...)</p>
                                <ul>
                                    <li>Image classification (Cat/Dog/Bird)</li>
                                    <li>Handwriting recognition (0-9 digits)</li>
                                    <li>Sentiment analysis (Positive/Neutral/Negative)</li>
                                </ul>
                                <p>Multiple possible outcomes (A, B, C, D...)</p>
                                <ul>
                                    <li>Image recognition (1000+ object classes)</li>
                                    <li>Language detection (100+ languages)</li>
                                    <li>Product categorization (Electronics, Clothing, Books...)</li>
                                </ul>
                            </div>
                        </div>

                        <div class="highlight-box warning">
                            <strong>‚ö†Ô∏è Common Pitfall:</strong> Class imbalance! If 99% of emails are legitimate and 1% are spam, a naive model could achieve 99% accuracy by always predicting "legitimate" - but it would be useless!
                        </div>
                    </div>
                </div>
                <div class="code-block">
                <pre><code class="language-python">
<span style="color:#6A9955"># Supervised Learning Intuition: House Price Prediction</span>
<span style="color:#DCDCAA">import</span> numpy <span style="color:#DCDCAA">as</span> np
<span style="color:#DCDCAA">import</span> matplotlib.pyplot <span style="color:#DCDCAA">as</span> plt
<span style="color:#DCDCAA">from</span> sklearn.linear_model <span style="color:#DCDCAA">import</span> LinearRegression

<span style="color:#6A9955"># Let's understand how supervised learning works step by step</span>
<span style="color:#6A9955"># Imagine we're a real estate agent trying to price houses</span>

<span style="color:#6A9955"># Historical data (our "teacher")</span>
house_sizes = np.array([1000, 1500, 2000, 2500, 3000, 3500]).reshape(-1, 1)  <span style="color:#6A9955"># sq ft</span>
house_prices = np.array([200000, 300000, 400000, 500000, 600000, 700000])    <span style="color:#6A9955"># dollars</span>

print(<span style="color:#CE9178">"üè† Historical Data (What we learn from):"</span>)
<span style="color:#DCDCAA">for</span> size, price <span style="color:#DCDCAA">in</span> zip(house_sizes.flatten(), house_prices):
    print(f<span style="color:#CE9178">"   {size:,} sq ft ‚Üí ${price:,}"</span>)

<span style="color:#6A9955"># Create and train our model (learning phase)</span>
model = LinearRegression()
model.fit(house_sizes, house_prices)

<span style="color:#6A9955"># The model has learned: Price ‚âà $200/sq ft (the pattern!)</span>
print(f<span style="color:#CE9178">"\nüß† What the model learned:"</span>)
print(f<span style="color:#CE9178">"   Price per sq ft: ${model.coef_[0]:.2f}"</span>)
print(f<span style="color:#CE9178">"   Base price: ${model.intercept_:,.2f}"</span>)

<span style="color:#6A9955"># Now predict prices for new houses (applying what we learned)</span>
new_houses = np.array([[1800], [2200], [4000]])
predicted_prices = model.predict(new_houses)

print(<span style="color:#CE9178">"\nüîÆ Predictions for new houses:"</span>)
<span style="color:#DCDCAA">for</span> size, price <span style="color:#DCDCAA">in</span> zip(new_houses.flatten(), predicted_prices):
    print(f<span style="color:#CE9178">"   {size:,} sq ft ‚Üí ${price:,.2f}"</span>)

<span style="color:#6A9955"># This is the essence of supervised learning:</span>
<span style="color:#6A9955"># 1. Learn patterns from labeled examples</span>
<span style="color:#6A9955"># 2. Apply those patterns to make predictions</span>
                </code></pre>
                </div>

            <div class="card">
                <h3><span class="emoji">üîç</span>Unsupervised Learning: Finding Hidden Patterns</h3>
                
                <div class="highlight-box info">
                    <h4>üéØ Core Concept</h4>
                    <p>Like an explorer discovering hidden treasures. No teacher, no right answers - just data with hidden structures waiting to be uncovered. The algorithm must find patterns, groupings, and relationships on its own.</p>
                </div>

                <div class="grid">
                    <div class="card">
                        <h4>üéØ Clustering: Finding Natural Groups</h4>
                        <p><strong>Question it answers:</strong> "What natural groups exist in my data?"</p>
                        
                        <h5>Business Applications:</h5>
                        <ul>
                            <li><strong>Customer Segmentation:</strong> Starbucks identified 5 customer types, increasing targeted marketing ROI by 300%</li>
                            <li><strong>Market Research:</strong> Netflix discovered 76,897 unique viewer "taste clusters" for personalization</li>
                            <li><strong>Anomaly Detection:</strong> Banks detect unusual transaction patterns (fraud prevention)</li>
                            <li><strong>Gene Analysis:</strong> Identify disease subtypes for personalized medicine</li>
                        </ul>

                        <div class="highlight-box tip">
                            <strong>üí° Real Example:</strong> Spotify's "Discover Weekly" uses clustering to group similar songs and find music you might like based on listening patterns of users similar to you.
                        </div>
                    </div>

                    <div class="card">
                        <h4>üìè Dimensionality Reduction: Simplifying Complexity</h4>
                        <p><strong>Question it answers:</strong> "What are the most important patterns in my high-dimensional data?"</p>
                        
                        <h5>Why It Matters:</h5>
                        <ul>
                            <li><strong>Visualization:</strong> Plot 1000-dimensional data in 2D/3D</li>
                            <li><strong>Storage:</strong> Compress data while preserving important information</li>
                            <li><strong>Speed:</strong> Faster algorithms with fewer dimensions</li>
                            <li><strong>Noise Reduction:</strong> Remove irrelevant features</li>
                        </ul>

                        <div class="highlight-box warning">
                            <strong>‚ö†Ô∏è The Curse of Dimensionality:</strong> As dimensions increase, data becomes sparse. In 1000 dimensions, most points are equally far apart, making patterns hard to find!
                        </div>
                    </div>
                </div>

                <div class="code-block">
<pre><code class="language-python">
<span style="color:#6A9955"># Unsupervised Learning Example: Customer Segmentation</span>
<span style="color:#DCDCAA">import</span> numpy <span style="color:#DCDCAA">as</span> np
<span style="color:#DCDCAA">import</span> matplotlib.pyplot <span style="color:#DCDCAA">as</span> plt
<span style="color:#DCDCAA">from</span> sklearn.cluster <span style="color:#DCDCAA">import</span> KMeans
<span style="color:#DCDCAA">from</span> sklearn.datasets <span style="color:#DCDCAA">import</span> make_blobs

<span style="color:#6A9955"># Simulate customer data: spending habits and visit frequency</span>
np.random.seed(42)
<span style="color:#6A9955"># Generate synthetic customer data</span>
customer_spending = np.random.normal(100, 30, 200)  <span style="color:#6A9955"># Average spending</span>
visit_frequency = np.random.normal(5, 2, 200)       <span style="color:#6A9955"># Visits per month</span>

<span style="color:#6A9955"># Combine into feature matrix</span>
customer_data = np.column_stack([customer_spending, visit_frequency])

print(<span style="color:#CE9178">"üõçÔ∏è Customer Behavior Analysis (Unsupervised Learning)"</span>)
print(<span style="color:#CE9178">"We have customer data but don't know what groups exist..."</span>)

<span style="color:#6A9955"># Apply K-means clustering to find customer segments</span>
kmeans = KMeans(n_clusters=3, random_state=42)
customer_segments = kmeans.fit_predict(customer_data)

<span style="color:#6A9955"># Analyze the discovered segments</span>
print(<span style="color:#CE9178">"\nüéØ Discovered Customer Segments:"</span>)
<span style="color:#DCDCAA">for</span> i <span style="color:#DCDCAA">in</span> range(3):
    segment_data = customer_data[customer_segments == i]
    avg_spending = segment_data[:, 0].mean()
    avg_visits = segment_data[:, 1].mean()
    size = len(segment_data)
    
    <span style="color:#DCDCAA">if</span> avg_spending > 120:
        segment_name = <span style="color:#CE9178">"üíé VIP Customers"</span>
    <span style="color:#DCDCAA">elif</span> avg_spending > 100:
        segment_name = <span style="color:#CE9178">"üéØ Regular Customers"</span>
    <span style="color:#DCDCAA">else</span>:
        segment_name = <span style="color:#CE9178">"üå± Budget-Conscious"</span>
    
    print(f<span style="color:#CE9178">"   Segment {i+1} - {segment_name}:"</span>)
    print(f<span style="color:#CE9178">"     ‚Ä¢ Size: {size} customers ({size/200*100:.1f}%)"</span>)
    print(f<span style="color:#CE9178">"     ‚Ä¢ Avg Spending: ${avg_spending:.2f}"</span>)
    print(f<span style="color:#CE9178">"     ‚Ä¢ Avg Visits: {avg_visits:.1f}/month"</span>)
    print()

print(<span style="color:#CE9178">"üí° Business Insight: Now we can create targeted marketing campaigns!"</span>)
print(<span style="color:#CE9178">"   ‚Ä¢ VIP: Exclusive offers and premium services"</span>)
print(<span style="color:#CE9178">"   ‚Ä¢ Regular: Loyalty programs and upselling"</span>)
print(<span style="color:#CE9178">"   ‚Ä¢ Budget: Discounts and value propositions"</span>)
</code></pre>
                </div>
            </div>

            <div class="card">
                <h3><span class="emoji">üéÆ</span>Reinforcement Learning: Learning Through Trial and Error</h3>
                
                <div class="highlight-box info">
                    <h4>üéØ Core Concept</h4>
                    <p>Like training a pet or learning to ride a bike. An agent learns by interacting with an environment, receiving rewards for good actions and penalties for bad ones. Over time, it discovers the optimal strategy.</p>
                </div>

                <div class="grid">
                    <div class="card">
                        <h4>üèóÔ∏è Key Components</h4>
                        <ul>
                            <li><strong>Agent:</strong> The decision maker (AI player, robot, trading algorithm)</li>
                            <li><strong>Environment:</strong> The world/context (game board, real world, stock market)</li>
                            <li><strong>Actions:</strong> What the agent can do (move, buy/sell, accelerate)</li>
                            <li><strong>States:</strong> Current situation (game position, sensor readings, portfolio value)</li>
                            <li><strong>Rewards:</strong> Feedback signal (+1 for winning, -1 for losing, profit/loss)</li>
                            <li><strong>Policy:</strong> Strategy for choosing actions (the "brain" of the agent)</li>
                        </ul>
                    </div>

                    <div class="card">
                        <h4>üåü Breakthrough Applications</h4>
                        <ul>
                            <li><strong>Gaming:</strong> AlphaGo defeated world Go champion (10^170 possible games!)</li>
                            <li><strong>Robotics:</strong> Boston Dynamics' robots learn to walk and recover from falls</li>
                            <li><strong>Autonomous Vehicles:</strong> Learning optimal driving policies in simulation</li>
                            <li><strong>Finance:</strong> Algorithmic trading that adapts to market conditions</li>
                            <li><strong>Resource Management:</strong> Google's data centers use 40% less energy</li>
                            <li><strong>Personalization:</strong> Dynamic content recommendation (YouTube, TikTok)</li>
                        </ul>
                    </div>
                </div>

                <div class="highlight-box danger">
                    <strong>üöÄ Advanced Warning:</strong> RL is the most complex ML type. It requires significant computational resources, careful reward design, and extensive testing. Start with supervised learning first!
                </div>

                <div class="code-block">
<pre><code class="language-python">
<span style="color:#6A9955"># Reinforcement Learning Concept: Simple Trading Agent</span>
<span style="color:#DCDCAA">import</span> numpy <span style="color:#DCDCAA">as</span> np
<span style="color:#DCDCAA">import</span> random

<span style="color:#B5CEA8">class</span> SimpleTrader:
      <span style="color:#B5CEA8">return</span> <span style="color:#CE9178">"falling"</span>
        <span style="color:#B5CEA8">else</span>:
            <span style="color:#B5CEA8">return</span> <span style="color:#CE9178">"stable"</span>
    
    <span style="color:#B5CEA8">def</span> choose_action(self, state):
        <span style="color:#CE9178">"""Choose action using epsilon-greedy strategy"""</span>
        <span style="color:#B5CEA8">if</span> random.random() < self.epsilon:
            <span style="color:#B5CEA8">return</span> random.choice(self.actions)  <span style="color:#6A9955"># Explore</span>
        <span style="color:#B5CEA8">else</span>:
            <span style="color:#6A9955"># Exploit: choose best known action for this state</span>
            <span style="color:#B5CEA8">if</span> state <span style="color:#B5CEA8">not</span> <span style="color:#B5CEA8">in</span> self.q_table:
                self.q_table[state] = {action: <span style="color:#B5CEA8">0</span> <span style="color:#B5CEA8">for</span> action <span style="color:#B5CEA8">in</span> self.actions}
            <span style="color:#B5CEA8">return</span> max(self.q_table[state], key=self.q_table[state].get)
    
    <span style="color:#B5CEA8">def</span> update_q_value(self, state, action, reward, next_state):
        <span style="color:#CE9178">"""Update Q-value based on experience"""</span>
        <span style="color:#B5CEA8">if</span> state <span style="color:#B5CEA8">not</span> <span style="color:#B5CEA8">in</span> self.q_table:
            self.q_table[state] = {action: <span style="color:#B5CEA8">0</span> <span style="color:#B5CEA8">for</span> action <span style="color:#B5CEA8">in</span> self.actions}
        <span style="color:#B5CEA8">if</span> next_state <span style="color:#B5CEA8">not</span> <span style="color:#B5CEA8">in</span> self.q_table:
            self.q_table[next_state] = {action: <span style="color:#B5CEA8">0</span> <span style="color:#B5CEA8">for</span> action <span style="color:#B5CEA8">in</span> self.actions}
        
        <span style="color:#6A9955"># Q-learning update rule</span>
        best_next_action = max(self.q_table[next_state].values())
        current_q = self.q_table[state][action]
        
        <span style="color:#6A9955"># Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]</span>
        self.q_table[state][action] = current_q + self.learning_rate * (
            reward + <span style="color:#B5CEA8">0.9</span> * best_next_action - current_q
        ) <span style="color:#CE9178">"""A basic RL trading agent that learns to buy/sell stocks"""</span>
    
    <span style="color:#B5CEA8">def</span> __init__(self):
        self.portfolio_value = <span style="color:#B5CEA8">10000</span>  <span style="color:#6A9955"># Starting money</span>
        self.stock_shares = <span style="color:#B5CEA8">0</span>
        self.actions = [<span style="color:#CE9178">'buy'</span>, <span style="color:#CE9178">'sell'</span>, <span style="color:#CE9178">'hold'</span>]
        self.q_table = {}  <span style="color:#6A9955"># Stores learned values for state-action pairs</span>
        self.learning_rate = <span style="color:#B5CEA8">0.1</span>
        self.epsilon = <span style="color:#B5CEA8">0.1</span>  <span style="color:#6A9955"># Exploration rate</span>
        
    <span style="color:#B5CEA8">def</span> get_state(self, price_trend):
        <span style="color:#CE9178">"""Simplified state: just the price trend"""</span>
        <span style="color:#B5CEA8">if</span> price_trend > <span style="color:#B5CEA8">0.05</span>:
            <span style="color:#B5CEA8">return</span> <span style="color:#CE9178">"rising"</span>
        <span style="color:#B5CEA8">elif</span> price_trend < -<span style="color:#B5CEA8">0.05</span>:
         

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"ü§ñ Reinforcement Learning Trading Agent"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Learning to trade through experience..."</span>)

<span style="color:#6A9955"># Simulate the learning process</span>
trader = SimpleTrader()
prices = [<span style="color:#B5CEA8">100</span>, <span style="color:#B5CEA8">102</span>, <span style="color:#B5CEA8">98</span>, <span style="color:#B5CEA8">105</span>, <span style="color:#B5CEA8">103</span>, <span style="color:#B5CEA8">110</span>, <span style="color:#B5CEA8">95</span>, <span style="color:#B5CEA8">108</span>, <span style="color:#B5CEA8">112</span>, <span style="color:#B5CEA8">98</span>]  <span style="color:#6A9955"># Sample price data</span>

<span style="color:#B5CEA8">for</span> day, price <span style="color:#B5CEA8">in</span> enumerate(prices[:-<span style="color:#B5CEA8">1</span>]):
    <span style="color:#6A9955"># Calculate price trend</span>
    next_price = prices[day + <span style="color:#B5CEA8">1</span>]
    trend = (next_price - price) / price
    current_state = trader.get_state(trend)
    
    <span style="color:#6A9955"># Agent chooses action</span>
    action = trader.choose_action(current_state)
    
    <span style="color:#6A9955"># Execute action and calculate reward</span>
    <span style="color:#B5CEA8">if</span> action == <span style="color:#CE9178">'buy'</span> <span style="color:#B5CEA8">and</span> trader.portfolio_value >= price:
        trader.stock_shares += <span style="color:#B5CEA8">1</span>
        trader.portfolio_value -= price
        reward = (next_price - price)  <span style="color:#6A9955"># Profit/loss from this trade</span>
    <span style="color:#B5CEA8">elif</span> action == <span style="color:#CE9178">'sell'</span> <span style="color:#B5CEA8">and</span> trader.stock_shares > <span style="color:#B5CEA8">0</span>:
        trader.stock_shares -= <span style="color:#B5CEA8">1</span>
        trader.portfolio_value += price
        reward = <span style="color:#B5CEA8">10</span>  <span style="color:#6A9955"># Small reward for selling</span>
    <span style="color:#B5CEA8">else</span>:
        reward = <span style="color:#B5CEA8">0</span>  <span style="color:#6A9955"># No action taken</span>
    
    <span style="color:#6A9955"># Learn from this experience</span>
    next_state = trader.get_state((prices[day + <span style="color:#B5CEA8">2</span>] - next_price) / next_price <span style="color:#B5CEA8">if</span> day + <span style="color:#B5CEA8">2</span> <span style="color:#B5CEA8"><</span> len(prices) <span style="color:#B5CEA8">else</span> <span style="color:#B5CEA8">0</span>)
    trader.update_q_value(current_state, action, reward, next_state)
    
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Day {day+1}: Price=${price:.2f}, State={current_state}, Action={action}, Reward={reward:.2f}"</span>)

final_value = trader.portfolio_value + trader.stock_shares * prices[-<span style="color:#B5CEA8">1</span>]
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"\nüìä Final Portfolio Value: ${final_value:.2f}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"üìà Return: {(final_value - 10000) / 10000 * 100:.1f}%"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\nüí° The agent learned which actions work best in different market conditions!"</span>)
</code></pre>
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üêç</span>Python ML Ecosystem: Your Complete Toolkit</h2>
            
            <div class="highlight-box info">
                <h4>üéØ Why Python Dominates ML</h4>
                <p>Python powers 57% of ML projects because of its simplicity, extensive libraries, and strong community. It's the lingua franca of data science!</p>
            </div>

            <div class="grid">
                <div class="card">
                    <h3>üî¢ NumPy: The Foundation</h3>
                    <p><strong>What it does:</strong> Efficient numerical computing with N-dimensional arrays</p>
                    
                    <h4>Why NumPy is Essential:</h4>
                    <ul>
                        <li><strong>Speed:</strong> 50x faster than pure Python (C implementation)</li>
                        <li><strong>Memory:</strong> Uses 80% less memory than Python lists</li>
                        <li><strong>Vectorization:</strong> Apply operations to entire arrays at once</li>
                        <li><strong>Broadcasting:</strong> Perform operations on arrays of different shapes</li>
                    </ul>

                    <div class="code-block">
                    <pre><code class="language-python">
<span style="color:#6A9955"># NumPy Deep Dive: Why it's the ML Foundation</span>
<span style="color:#DCDCAA">import</span> numpy <span style="color:#DCDCAA">as</span> np
<span style="color:#DCDCAA">import</span> time

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üî¢ NumPy vs Pure Python Performance Test"</span>)

<span style="color:#6A9955"># Test 1: Speed comparison</span>
python_list = <span style="color:#B5CEA8">list</span>(<span style="color:#B5CEA8">range</span>(<span style="color:#B5CEA8">1000000</span>))
numpy_array = np.arange(<span style="color:#B5CEA8">1000000</span>)

<span style="color:#6A9955"># Pure Python multiplication</span>
start = time.time()
python_result = [x * <span style="color:#B5CEA8">2</span> <span style="color:#DCDCAA">for</span> x <span style="color:#DCDCAA">in</span> python_list]
python_time = time.time() - start

<span style="color:#6A9955"># NumPy vectorized multiplication</span>
start = time.time()
numpy_result = numpy_array * <span style="color:#B5CEA8">2</span>
numpy_time = time.time() - start

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Python list time: {python_time:.4f} seconds"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"NumPy array time: {numpy_time:.4f} seconds"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"NumPy is {python_time/numpy_time:.1f}x faster!"</span>)

<span style="color:#6A9955"># Test 2: Advanced operations that make ML possible</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\nüßÆ Advanced NumPy Operations for ML:"</span>)

<span style="color:#6A9955"># Create sample data (like features in ML)</span>
data = np.random.random((<span style="color:#B5CEA8">1000</span>, <span style="color:#B5CEA8">10</span>))  <span style="color:#6A9955"># 1000 samples, 10 features</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Data shape: {data.shape}"</span>)

<span style="color:#6A9955"># Statistical operations (crucial for ML)</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Mean per feature: {data.mean(axis=0)[:3]}... (first 3)"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Standard deviation: {data.std(axis=0)[:3]}... (first 3)"</span>)

<span style="color:#6A9955"># Matrix operations (heart of ML algorithms)</span>
weights = np.random.random((<span style="color:#B5CEA8">10</span>, <span style="color:#B5CEA8">1</span>))
predictions = data @ weights  <span style="color:#6A9955"># Matrix multiplication</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Predictions shape: {predictions.shape}"</span>)

<span style="color:#6A9955"># Broadcasting example (different sized arrays)</span>
bias = np.array([<span style="color:#B5CEA8">0.5</span>])  <span style="color:#6A9955"># Single value</span>
predictions_with_bias = predictions + bias  <span style="color:#6A9955"># Broadcasts to all elements</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Broadcasting bias to {predictions.shape} array: ‚úÖ"</span>)

<span style="color:#6A9955"># Boolean indexing (data filtering)</span>
high_predictions = data[predictions.flatten() > <span style="color:#B5CEA8">0.5</span>]
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Samples with high predictions: {len(high_predictions)}"</span>)
                    </code></pre>
                    </div>
                </div>

                <div class="card">
                    <h3>üêº Pandas: Data Manipulation Master</h3>
                    <p><strong>What it does:</strong> Structured data analysis and manipulation</p>
                    
                    <h4>Core Capabilities:</h4>
                    <ul>
                        <li><strong>DataFrames:</strong> Excel-like data structures in code</li>
                        <li><strong>Data Cleaning:</strong> Handle missing values, duplicates, outliers</li>
                        <li><strong>Data Transformation:</strong> Group, pivot, merge, reshape</li>
                        <li><strong>I/O Operations:</strong> Read/write CSV, Excel, JSON, SQL, APIs</li>
                    </ul>

                    <div class="code-block">
<pre><code class="language-python">
<span style="color:#6A9955"># Pandas Deep Dive: Real-World Data Manipulation</span>
<span style="color:#DCDCAA">import</span> pandas <span style="color:#DCDCAA">as</span> pd
<span style="color:#DCDCAA">import</span> numpy <span style="color:#DCDCAA">as</span> np

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üêº Pandas for Real-World Data Analysis"</span>)

<span style="color:#6A9955"># Create realistic sample dataset</span>
np.random.seed(<span style="color:#B5CEA8">42</span>)
n_customers = <span style="color:#B5CEA8">1000</span>

<span style="color:#6A9955"># Simulate e-commerce customer data</span>
data = {
    <span style="color:#CE9178">'customer_id'</span>: <span style="color:#B5CEA8">range</span>(<span style="color:#B5CEA8">1</span>, n_customers + <span style="""color:#B5CEA8">1</span>),
    <span style="""color:#CE9178">'age'</span>: np.random.normal(<span style="""color:#B5CEA8">35</span>, <span style="""color:#B5CEA8">12</span>, n_customers).astype(<span style="""color:#CE9178">int</span>),
    <span style="""color:#CE9178">'annual_income'</span>: np.random.normal(<span style="""color:#B5CEA8">50000</span>, <span style="""color:#B5CEA8">15000</span>, n_customers),
    <span style="""color:#CE9178">'purchase_frequency'</span>: np.random.poisson(<span style="""color:#B5CEA8">8</span>, n_customers),
    <span style="""color:#CE9178">'total_spent'</span>: np.random.exponential(<span style="""color:#B5CEA8">500</span>, n_customers),
    <span style="""color:#CE9178">'membership_tier'</span>: np.random.choice([<span style="""color:#CE9178">'Bronze'</span>, <span style="""color:#CE9178">'Silver'</span>, <span style="""color:#CE9178">'Gold'</span>], n_customers, p=[<span style="color:#B5CEA8">0.6</span>, <span style="color:#B5CEA8">0.3</span>, <span style="color:#B5CEA8">0.1</span>]),
    <span style="color:#CE9178">'last_purchase_days'</span>: np.random.exponential(<span style="color:#B5CEA8">30</span>, n_customers).astype(<span style="color:#CE9178">int</span>),
    <span style="color:#CE9178">'customer_satisfaction'</span>: np.random.normal(<span style="color:#B5CEA8">3.5</span>, <span style="color:#B5CEA8">0.8</span>, n_customers)
}

<span style="color:#6A9955"># Add some missing values (realistic scenario)</span>
missing_indices = np.random.choice(n_customers, size=<span style="color:#B5CEA8">50</span>, replace=<span style="color:#B5CEA8">False</span>)
data[<span style="color:#CE9178">'customer_satisfaction'</span>][missing_indices] = np.nan

df = pd.DataFrame(data)

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üìä Dataset Overview:"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Shape: {df.shape}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\nFirst 3 rows:"</span>)
<span style="color:#DCDCAA">print</span>(df.head(<span style="color:#B5CEA8">3</span>))

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\nüîç Data Quality Assessment:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Missing values per column:"</span>)
<span style="color:#DCDCAA">print</span>(df.isnull().sum())

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\nüìà Statistical Summary:"</span>)
<span style="color:#DCDCAA">print</span>(df.describe())

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\nüéØ Advanced Pandas Operations:"</span>)

<span style="color:#6A9955"># 1. Customer segmentation using groupby</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n1. Customer Analysis by Membership Tier:"</span>)
tier_analysis = df.groupby(<span style="color:#CE9178">'membership_tier'</span>).agg({
    <span style="color:#CE9178">'annual_income'</span>: [<span style="color:#CE9178">'mean'</span>, <span style="color:#CE9178">'median'</span>],
    <span style="color:#CE9178">'total_spent'</span>: [<span style="color:#CE9178">'mean'</span>, <span style="color:#CE9178">'sum'</span>],
    <span style="color:#CE9178">'customer_satisfaction'</span>: <span style="color:#CE9178">'mean'</span>,
    <span style="color:#CE9178">'customer_id'</span>: <span style="color:#CE9178">'count'</span>
}).round(<span style="color:#B5CEA8">2</span>)
<span style="color:#DCDCAA">print</span>(tier_analysis)

<span style="color:#6A9955"># 2. High-value customer identification</span>
high_value_threshold = df[<span style="color:#CE9178">'total_spent'</span>].quantile(<span style="color:#B5CEA8">0.8</span>)
df[<span style="color:#CE9178">'is_high_value'</span>] = df[<span style="color:#CE9178">'total_spent'</span>] > high_value_threshold

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"\n2. High-Value Customers (top 20%):"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Threshold: ${high_value_threshold:.2f}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Count: {df['is_high_value'].sum()} customers"</span>)

<span style="color:#6A9955"># 3. Customer lifecycle analysis</span>
<span style="color:#B5CEA8">def</span> categorize_recency(days):
    <span style="color:#B5CEA8">if</span> days <= <span style="color:#B5CEA8">7</span>:
        <span style="color:#B5CEA8">return</span> <span style="color:#CE9178">'Active'</span>
    <span style="color:#B5CEA8">elif</span> days <= <span style="color:#B5CEA8">30</span>:
        <span style="color:#B5CEA8">return</span> <span style="color:#CE9178">'Recent'</span>
    <span style="color:#B5CEA8">elif</span> days <= <span style="color:#B5CEA8">90</span>:
        <span style="color:#B5CEA8">return</span> <span style="color:#CE9178">'At Risk'</span>
    <span style="color:#B5CEA8">else</span>:
        <span style="color:#B5CEA8">return</span> <span style="color:#CE9178">'Inactive'</span>

df[<span style="color:#CE9178">'customer_status'</span>] = df[<span style="color:#CE9178">'last_purchase_days'</span>].apply(categorize_recency)

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n3. Customer Status Distribution:"</span>)
<span style="color:#DCDCAA">print</span>(df[<span style="color:#CE9178">'customer_status'</span>].value_counts())

<span style="color:#6A9955"># 4. Data cleaning and preprocessing</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n4. Data Preprocessing:"</span>)

<span style="color:#6A9955"># Handle missing values</span>
df[<span style="color:#CE9178">'customer_satisfaction'</span>].fillna(df[<span style="color:#CE9178">'customer_satisfaction'</span>].mean(), inplace=<span style="color:#B5CEA8">True</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Missing values after imputation: {df.isnull().sum().sum()}"</span>)

<span style="color:#6A9955"># Feature engineering</span>
df[<span style="color:#CE9178">'spending_per_purchase'</span>] = df[<span style="color:#CE9178">'total_spent'</span>] / df[<span style="color:#CE9178">'purchase_frequency'</span>]
df[<span style="color:#CE9178">'income_spending_ratio'</span>] = df[<span style="color:#CE9178">'total_spent'</span>] / df[<span style="color:#CE9178">'annual_income'</span>]

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"New engineered features created: spending_per_purchase, income_spending_ratio"</span>)

<span style="color:#6A9955"># 5. Advanced filtering and selection</span>
premium_customers = df[
    (df[<span style="color:#CE9178">'membership_tier'</span>] == <span style="color:#CE9178">'Gold'</span>) & 
    (df[<span style="color:#CE9178">'customer_satisfaction'</span>] > <span style="color:#B5CEA8">4.0</span>) &
    (df[<span style="color:#CE9178">'is_high_value'</span>] == <span style="color:#B5CEA8">True</span>)
]

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"\n5. Premium Customer Segment: {len(premium_customers)} customers"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Average annual value: ${premium_customers['total_spent'].mean():.2f}"</span>)
</code></pre>
                    </div>  
                   </div>
            </div>
            <div class="grid">
                <div class="card">
                    <h3>üìä Matplotlib & Seaborn: Data Visualization</h3>
                    <p><strong>What they do:</strong> Create insightful visualizations to understand data patterns</p>
                    
                    <h4>Why Visualization Matters in ML:</h4>
                    <ul>
                        <li><strong>Pattern Recognition:</strong> Spot trends humans might miss</li>
                        <li><strong>Outlier Detection:</strong> Identify data quality issues</li>
                        <li><strong>Feature Relationships:</strong> Understand correlations</li>
                        <li><strong>Model Evaluation:</strong> Visualize performance metrics</li>
                    </ul>

                    <div class="code-block">
                        <pre><code class="language-python">
<span style="color:#6A9955"># Data Visualization: The Art of Making Data Speak</span>
<span style="color:#DCDCAA">import</span> matplotlib.pyplot <span style="color:#DCDCAA">as</span> plt
<span style="color:#DCDCAA">import</span> seaborn <span style="color:#DCDCAA">as</span> sns
<span style="color:#DCDCAA">import</span> numpy <span style="color:#DCDCAA">as</span> np
<span style="color:#DCDCAA">import</span> pandas <span style="color:#DCDCAA">as</span> pd

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üìä Data Visualization for Machine Learning"</span>)

<span style="color:#6A9955"># Set up the plotting style</span>
plt.style.use(<span style="color:#CE9178">'default'</span>)
sns.set_palette(<span style="color:#CE9178">"husl"</span>)

<span style="color:#6A9955"># Generate sample data for demonstration</span>
np.random.seed(<span style="color:#B5CEA8">42</span>)
n_samples = <span style="color:#B5CEA8">500</span>

<span style="color:#6A9955"># Create correlated features (like real ML data)</span>
feature1 = np.random.normal(<span style="color:#B5CEA8">100</span>, <span style="color:#B5CEA8">15</span>, n_samples)
feature2 = feature1 * <span style="color:#B5CEA8">0.8</span> + np.random.normal(<span style="color:#B5CEA8">0</span>, <span style="color:#B5CEA8">10</span>, n_samples)  <span style="color:#6A9955"># Correlated</span>
feature3 = np.random.exponential(<span style="color:#B5CEA8">20</span>, n_samples)  <span style="color:#6A9955"># Different distribution</span>
target = (feature1 * <span style="color:#B5CEA8">0.3</span> + feature2 * <span style="color:#B5CEA8">0.2</span> + np.random.normal(<span style="color:#B5CEA8">0</span>, <span style="color:#B5CEA8">5</span>, n_samples)) > <span style="color:#B5CEA8">85</span>

<span style="color:#6A9955"># Create DataFrame</span>
viz_data = pd.DataFrame({
    <span style="color:#CE9178">'feature1'</span>: feature1,
    <span style="color:#CE9178">'feature2'</span>: feature2, 
    <span style="color:#CE9178">'feature3'</span>: feature3,
    <span style="color:#CE9178">'target'</span>: target,
    <span style="color:#CE9178">'category'</span>: np.random.choice([<span style="color:#CE9178">'A'</span>, <span style="color:#CE9178">'B'</span>, <span style="color:#CE9178">'C'</span>], n_samples, p=[<span style="color:#B5CEA8">0.5</span>, <span style="color:#B5CEA8">0.3</span>, <span style="color:#B5CEA8">0.2</span>])
})

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üé® Creating Essential ML Visualizations:"</span>)

<span style="color:#6A9955"># 1. Distribution Analysis</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n1. Feature Distributions (crucial for preprocessing)"</span>)
fig, axes = plt.subplots(<span style="color:#B5CEA8">2</span>, <span style="color:#B5CEA8">2</span>, figsize=(<span style="color:#B5CEA8">12</span>, <span style="color:#B5CEA8">10</span>))

<span style="color:#6A9955"># Histogram with multiple features</span>
axes[<span style="color:#B5CEA8">0</span>,<span style="color:#B5CEA8">0</span>].hist([viz_data[<span style="color:#CE9178">'feature1'</span>], viz_data[<span style="color:#CE9178">'feature2'</span>]], 
                bins=<span style="color:#B5CEA8">30</span>, alpha=<span style="color:#B5CEA8">0.7</span>, label=[<span style="color:#CE9178">'Feature1'</span>, <span style="color:#CE9178">'Feature2'</span>])
axes[<span style="color:#B5CEA8">0</span>,<span style="color:#B5CEA8">0</span>].set_title(<span style="color:#CE9178">'Feature Distributions'</span>)
axes[<span style="color:#B5CEA8">0</span>,<span style="color:#B5CEA8">0</span>].legend()

<span style="color:#6A9955"># Box plot for outlier detection</span>
viz_data[[<span style="color:#CE9178">'feature1'</span>, <span style="color:#CE9178">'feature2'</span>, <span style="color:#CE9178">'feature3'</span>]].boxplot(ax=axes[<span style="color:#B5CEA8">0</span>,<span style="color:#B5CEA8">1</span>])
axes[<span style="color:#B5CEA8">0</span>,<span style="color:#B5CEA8">1</span>].set_title(<span style="color:#CE9178">'Outlier Detection'</span>)

<span style="color:#6A9955"># Correlation heatmap</span>
correlation_matrix = viz_data[[<span style="color:#CE9178">'feature1'</span>, <span style="color:#CE9178">'feature2'</span>, <span style="color:#CE9178">'feature3'</span>]].corr()
sns.heatmap(correlation_matrix, annot=<span style="color:#B5CEA8">True</span>, cmap=<span style="color:#CE9178">'coolwarm'</span>, center=<span style="color:#B5CEA8">0</span>, ax=axes[<span style="color:#B5CEA8">1</span>,<span style="color:#B5CEA8">0</span>])
axes[<span style="color:#B5CEA8">1</span>,<span style="color:#B5CEA8">0</span>].set_title(<span style="color:#CE9178">'Feature Correlations'</span>)

<span style="color:#6A9955"># Scatter plot with target coloring</span>
scatter = axes[<span style="color:#B5CEA8">1</span>,<span style="color:#B5CEA8">1</span>].scatter(viz_data[<span style="color:#CE9178">'feature1'</span>], viz_data[<span style="color:#CE9178">'feature2'</span>], 
                            c=viz_data[<span style="color:#CE9178">'target'</span>], alpha=<span style="color:#B5CEA8">0.6</span>, cmap=<span style="color:#CE9178">'viridis'</span>)
axes[<span style="color:#B5CEA8">1</span>,<span style="color:#B5CEA8">1</span>].set_xlabel(<span style="color:#CE9178">'Feature 1'</span>)
axes[<span style="color:#B5CEA8">1</span>,<span style="color:#B5CEA8">1</span>].set_ylabel(<span style="color:#CE9178">'Feature 2'</span>)
axes[<span style="color:#B5CEA8">1</span>,<span style="color:#B5CEA8">1</span>].set_title(<span style="color:#CE9178">'Feature Relationship by Target'</span>)

plt.tight_layout()
<span style="color:#6A9955"># In a real notebook, you'd see: plt.show()</span>

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ Visualization insights:"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"   ‚Ä¢ Feature1 & Feature2 correlation: {correlation_matrix.loc['feature1', 'feature2']:.3f}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"   ‚Ä¢ Feature3 appears to follow exponential distribution"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"   ‚Ä¢ Clear separation visible between target classes"</span>)

<span style="color:#6A9955"># 2. Advanced Statistical Plots</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n2. Advanced Analysis Plots:"</span>)

<span style="color:#6A9955"># Pair plot for comprehensive relationship analysis</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚Ä¢ Pair plot shows all feature relationships simultaneously"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚Ä¢ Diagonal shows distributions, off-diagonal shows relationships"</span>)

<span style="color:#6A9955"># Distribution by category</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚Ä¢ Category-wise analysis reveals segment differences:"</span>)
<span style="color:#DCDCAA">for</span> cat <span style="color:#DCDCAA">in</span> viz_data[<span style="color:#CE9178">'category'</span>].unique():
    cat_data = viz_data[viz_data[<span style="color:#CE9178">'category'</span>] == cat]
    mean_f1 = cat_data[<span style="color:#CE9178">'feature1'</span>].mean()
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"     Category {cat}: Feature1 mean = {mean_f1:.1f}"</span>)

<span style="color:#6A9955"># 3. Model Evaluation Visualizations (preview)</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n3. ML Model Evaluation Plots (you'll learn these later):"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚Ä¢ Confusion Matrix: Shows classification accuracy"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚Ä¢ ROC Curve: Plots true vs false positive rates"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚Ä¢ Learning Curves: Show model performance vs training size"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚Ä¢ Feature Importance: Shows which features matter most"</span>)

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\nüí° Visualization Best Practices:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚úÖ Always explore data visually before modeling"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚úÖ Use appropriate plot types for data types"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚úÖ Color-code by target variable when possible"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚úÖ Look for outliers, missing values, and distributions"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚úÖ Check correlations to avoid redundant features"</span>)
                        </code></pre>   
                    </div>
                </div>

                <div class="card">
                    <h3>ü§ñ Scikit-learn: The ML Swiss Army Knife</h3>
                    <p><strong>What it does:</strong> Comprehensive machine learning algorithms and tools</p>
                    
                    <h4>Core Components:</h4>
                    <ul>
                        <li><strong>Algorithms:</strong> 50+ ML algorithms ready to use</li>
                        <li><strong>Preprocessing:</strong> Data cleaning and transformation</li>
                        <li><strong>Model Selection:</strong> Cross-validation and hyperparameter tuning</li>
                        <li><strong>Evaluation:</strong> Comprehensive metrics for all ML tasks</li>
                    </ul>

                    <div class="code-block">
                    <pre><code class="language-python">
<span style="color:#6A9955"># Scikit-learn: Complete ML Pipeline</span>
<span style="color:#DCDCAA">from</span> sklearn.datasets <span style="color:#DCDCAA">import</span> make_classification, make_regression
<span style="color:#DCDCAA">from</span> sklearn.model_selection <span style="color:#DCDCAA">import</span> train_test_split, cross_val_score
<span style="color:#DCDCAA">from</span> sklearn.preprocessing <span style="color:#DCDCAA">import</span> StandardScaler, LabelEncoder
<span style="color:#DCDCAA">from</span> sklearn.ensemble <span style="color:#DCDCAA">import</span> RandomForestClassifier, GradientBoostingClassifier
<span style="color:#DCDCAA">from</span> sklearn.linear_model <span style="color:#DCDCAA">import</span> LinearRegression
<span style="color:#DCDCAA">from</span> sklearn.svm <span style="color:#DCDCAA">import</span> SVC
<span style="color:#DCDCAA">from</span> sklearn.metrics <span style="color:#DCDCAA">import</span> classification_report, mean_squared_error, r2_score
<span style="color:#DCDCAA">import</span> numpy <span style="color:#DCDCAA">as</span> np

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"ü§ñ Scikit-learn: Complete ML Workflow"</span>)

<span style="color:#6A9955"># 1. Generate realistic datasets</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n1. Creating Realistic ML Datasets:"</span>)

<span style="color:#6A9955"># Classification dataset (predicting customer churn)</span>
X_class, y_class = make_classification(
    n_samples=<span style="color:#B5CEA8">1000</span>, n_features=<span style="color:#B5CEA8">10</span>, n_informative=<span style="color:#B5CEA8">7</span>, n_redundant=<span style="color:#B5CEA8">2</span>,
    n_classes=<span style="color:#B5CEA8">2</span>, class_sep=<span style="color:#B5CEA8">0.8</span>, random_state=<span style="color:#B5CEA8">42</span>
)

<span style="color:#6A9955"># Regression dataset (predicting house prices)</span>
X_reg, y_reg = make_regression(
    n_samples=<span style="color:#B5CEA8">1000</span>, n_features=<span style="color:#B5CEA8">8</span>, n_informative=<span style="color:#B5CEA8">6</span>, noise=<span style="color:#B5CEA8">0.1</span>,
    random_state=<span style="color:#B5CEA8">42</span>
)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Classification dataset: {X_class.shape} features, {len(np.unique(y_class))} classes"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Regression dataset: {X_reg.shape} features, continuous target"</span>)

<span style="color:#6A9955"># 2. Data Preprocessing Pipeline</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n2. Data Preprocessing (Critical for Success):"</span>)

<span style="color:#6A9955"># Split the data first (ALWAYS do this before preprocessing!)</span>
X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
    X_class, y_class, test_size=<span style="color:#B5CEA8">0.2</span>, random_state=<span style="color:#B5CEA8">42</span>, stratify=y_class
)

X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
    X_reg, y_reg, test_size=<span style="color:#B5CEA8">0.2</span>, random_state=<span style="color:#B5CEA8">42</span>
)

<span style="color:#6A9955"># Feature scaling (essential for many algorithms)</span>
scaler_c = StandardScaler()
X_train_c_scaled = scaler_c.fit_transform(X_train_c)
X_test_c_scaled = scaler_c.transform(X_test_c)  <span style="color:#6A9955"># Never fit on test data!</span>

scaler_r = StandardScaler()
X_train_r_scaled = scaler_r.fit_transform(X_train_r)
X_test_r_scaled = scaler_r.transform(X_test_r)

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ Data preprocessing completed:"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"   ‚Ä¢ Training set: {X_train_c.shape[0]} samples"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"   ‚Ä¢ Test set: {X_test_c.shape[0]} samples"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"   ‚Ä¢ Features scaled to mean=0, std=1"</span>)

<span style="color:#6A9955"># 3. Model Training and Evaluation</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n3. Model Training & Evaluation:"</span>)

<span style="color:#6A9955"># Classification Model</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\nüìä Classification Model (Customer Churn Prediction):"</span>)
clf = RandomForestClassifier(n_estimators=<span style="color:#B5CEA8">100</span>, random_state=<span style="color:#B5CEA8">42</span>)
clf.fit(X_train_c_scaled, y_train_c)

<span style="color:#6A9955"># Make predictions</span>
y_pred_c = clf.predict(X_test_c_scaled)
y_pred_proba_c = clf.predict_proba(X_test_c_scaled)

<span style="color:#6A9955"># Evaluate classification</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Accuracy: {clf.score(X_test_c_scaled, y_test_c):.3f}"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\nDetailed Classification Report:"</span>)
<span style="color:#DCDCAA">print</span>(classification_report(y_test_c, y_pred_c))

<span style="color:#6A9955"># Cross-validation for robust evaluation</span>
cv_scores = cross_val_score(clf, X_train_c_scaled, y_train_c, cv=<span style="color:#B5CEA8">5</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Cross-validation scores: {cv_scores}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Mean CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})"</span>)

<span style="color:#6A9955"># Regression Model</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\nüè† Regression Model (House Price Prediction):"</span>)
reg = LinearRegression()
reg.fit(X_train_r_scaled, y_train_r)

<span style="color:#6A9955"># Make predictions</span>
y_pred_r = reg.predict(X_test_r_scaled)

<span style="color:#6A9955"># Evaluate regression</span>
mse = mean_squared_error(y_test_r, y_pred_r)
r2 = r2_score(y_test_r, y_pred_r)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Mean Squared Error: {mse:.2f}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"R¬≤ Score: {r2:.3f} (higher = better, 1.0 = perfect)"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Root MSE: {np.sqrt(mse):.2f}"</span>)

<span style="color:#6A9955"># 4. Feature Importance Analysis</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n4. Feature Importance (Model Interpretability):"</span>)
feature_importance = clf.feature_importances_
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Top 5 most important features for classification:"</span>)
<span style="color:#DCDCAA">for</span> i, importance <span style="color:#DCDCAA">in</span> enumerate(feature_importance[:5]):
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"   Feature {i}: {importance:.3f}"</span>)

<span style="color:#6A9955"># 5. Model Comparison</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n5. Quick Model Comparison:"</span>)

models = {
    <span style="color:#CE9178">'Random Forest'</span>: RandomForestClassifier(n_estimators=<span style="color:#B5CEA8">50</span>, random_state=<span style="color:#B5CEA8">42</span>),
    <span style="color:#CE9178">'Gradient Boosting'</span>: GradientBoostingClassifier(n_estimators=<span style="color:#B5CEA8">50</span>, random_state=<span style="color:#B5CEA8">42</span>),
    <span style="color:#CE9178">'SVM'</span>: SVC(random_state=<span style="color:#B5CEA8">42</span>)
}

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Model performance comparison (5-fold CV):"</span>)
<span style="color:#DCDCAA">for</span> name, model <span style="color:#DCDCAA">in</span> models.items():
    scores = cross_val_score(model, X_train_c_scaled, y_train_c, cv=<span style="color:#B5CEA8">5</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"   {name}: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"\nüéØ Key Scikit-learn Advantages:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚úÖ Consistent API across all algorithms"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚úÖ Excellent documentation and examples"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚úÖ Built-in preprocessing and evaluation tools"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚úÖ Efficient implementations of classic algorithms"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚úÖ Great for learning and production use"</span>)
                    </code></pre>
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üõ†Ô∏è</span>Data Preprocessing: The Make-or-Break Phase</h2>
            
            <div class="highlight-box danger">
                <h4>üéØ The 80/20 Rule of Machine Learning</h4>
                <p><strong>80% of ML success comes from data quality and preprocessing, only 20% from algorithm selection!</strong> This is why data scientists spend most of their time on data preparation, not modeling.</p>
            </div>

            <div class="card">
                <h3>üîç Understanding Your Data Quality Issues</h3>
                
                <div class="flowchart">
                    <div class="flow-step">
                        <div class="step-number">1</div>
                        <div>
                            <h4>Data Collection Issues</h4>
                            <p>Sensor failures, human errors, system outages, incomplete surveys</p>
                        </div>
                    </div>
                    <div class="flow-step">
                        <div class="step-number">2</div>
                        <div>
                            <h4>Integration Problems</h4>
                            <p>Different formats, units, time zones, encoding standards</p>
                        </div>
                    </div>
                    <div class="flow-step">
                        <div class="step-number">3</div>
                        <div>
                            <h4>Natural Variations</h4>
                            <p>Seasonal patterns, human behavior changes, market fluctuations</p>
                        </div>
                    </div>
                    <div class="flow-step">
                        <div class="step-number">4</div>
                        <div>
                            <h4>Processing Artifacts</h4>
                            <p>Rounding errors, compression losses, sampling biases</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="card">
                <h3>üßπ Advanced Missing Value Strategies</h3>
                
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Strategy</th>
                            <th>When to Use</th>
                            <th>Pros</th>
                            <th>Cons</th>
                            <th>Example</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Deletion</strong></td>
                            <td>&lt;5% missing, random pattern</td>
                            <td>Simple, no bias if random</td>
                            <td>Lose data, may introduce bias</td>
                            <td>Remove incomplete survey responses</td>
                        </tr>
                        <tr>
                            <td><strong>Mean/Median</strong></td>
                            <td>Numerical, normal distribution</td>
                            <td>Fast, preserves sample size</td>
                            <td>Reduces variance, ignores relationships</td>
                            <td>Fill missing ages with average age</td>
                        </tr>
                        <tr>
                            <td><strong>Mode</strong></td>
                            <td>Categorical data</td>
                            <td>Preserves most common pattern</td>
                            <td>May overrepresent majority class</td>
                            <td>Fill missing gender with most common</td>
                        </tr>
                        <tr>
                            <td><strong>Forward/Backward Fill</strong></td>
                            <td>Time series data</td>
                            <td>Preserves trends</td>
                            <td>May propagate old values too far</td>
                            <td>Fill missing stock prices with last known</td>
                        </tr>
                        <tr>
                            <td><strong>Interpolation</strong></td>
                            <td>Ordered data with trends</td>
                            <td>Smooth, logical values</td>
                            <td>Assumes linear relationships</td>
                            <td>Estimate missing temperature readings</td>
                        </tr>
                        <tr>
                            <td><strong>ML Imputation</strong></td>
                            <td>Complex patterns, &gt;10% missing</td>
                            <td>Uses all available information</td>
                            <td>Computationally expensive</td>
                            <td>Predict missing income from other features</td>
                        </tr>
                    </tbody>
                </table>

                <div class="code-block">
<pre><code class="language-python">
<span style="color:#6A9955"># Data Preprocessing: Advanced Missing Value Strategies</span>
<span style="color:#6A9955"># Advanced Missing Value Handling: Real-World Scenarios</span>
<span style="color:#DCDCAA">import</span> pandas <span style="color:#DCDCAA">as</span> pd
<span style="color:#DCDCAA">import</span> numpy <span style="color:#DCDCAA">as</span> np
<span style="color:#DCDCAA">from</span> sklearn.impute <span style="color:#DCDCAA">import</span> SimpleImputer, KNNImputer
<span style="color:#DCDCAA">from</span> sklearn.experimental <span style="color:#DCDCAA">import</span> enable_iterative_imputer
<span style="color:#DCDCAA">from</span> sklearn.impute <span style="color:#DCDCAA">import</span> IterativeImputer

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üßπ Advanced Missing Value Handling"</span>)

<span style="color:#6A9955"># Create realistic dataset with different missing patterns</span>
np.random.seed(<span style="color:#B5CEA8">42</span>)
n_samples = <span style="color:#B5CEA8">1000</span>

<span style="color:#6A9955"># Simulate customer dataset with realistic missing patterns</span>
data = {
    <span style="color:#CE9178">'customer_id'</span>: <span style="color:#B5CEA8">range</span>(<span style="color:#B5CEA8">1</span>, n_samples + <span style="color:#B5CEA8">1</span>),
    <span style="color:#CE9178">'age'</span>: np.random.normal(<span style="color:#B5CEA8">35</span>, <span style="color:#B5CEA8">12</span>, n_samples),
    <span style="color:#CE9178">'income'</span>: np.random.normal(<span style="color:#B5CEA8">50000</span>, <span style="color:#B5CEA8">15000</span>, n_samples),
    <span style="color:#CE9178">'credit_score'</span>: np.random.normal(<span style="color:#B5CEA8">650</span>, <span style="color:#B5CEA8">100</span>, n_samples),
    <span style="color:#CE9178">'years_employed'</span>: np.random.exponential(<span style="color:#B5CEA8">5</span>, n_samples),
    <span style="color:#CE9178">'education'</span>: np.random.choice([<span style="color:#CE9178">'High School'</span>, <span style="color:#CE9178">'Bachelor'</span>, <span style="color:#CE9178">'Master'</span>, <span style="color:#CE9178">'PhD'</span>], 
                                 n_samples, p=[<span style="color:#B5CEA8">0.4</span>, <span style="color:#B5CEA8">0.4</span>, <span style="color:#B5CEA8">0.15</span>, <span style="color:#B5CEA8">0.05</span>]),
    <span style="color:#CE9178">'loan_approved'</span>: np.random.choice([<span style="color:#B5CEA8">0</span>, <span style="color:#B5CEA8">1</span>], n_samples, p=[<span style="color:#B5CEA8">0.3</span>, <span style="color:#B5CEA8">0.7</span>])
}

df = pd.DataFrame(data)

<span style="color:#6A9955"># Introduce realistic missing patterns</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Introducing realistic missing value patterns:"</span>)

<span style="color:#6A9955"># 1. Missing Completely at Random (MCAR) - equipment failure</span>
mcar_indices = np.random.choice(n_samples, size=<span style="color:#B5CEA8">50</span>, replace=<span style="color:#B5CEA8">False</span>)
df.loc[mcar_indices, <span style="color:#CE9178">'credit_score'</span>] = np.nan
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"1. MCAR: {len(mcar_indices)} random credit scores missing (equipment failure)"</span>)

<span style="color:#6A9955"># 2. Missing at Random (MAR) - older people less likely to report income</span>
older_customers = df[df[<span style="color:#CE9178">'age'</span>] > <span style="color:#B5CEA8">60</span>].index
income_missing = np.random.choice(older_customers, 
                                 size=<span style="color:#B5CEA8">int</span>(<span style="color:#B5CEA8">len</span>(older_customers) * <span style="color:#B5CEA8">0.3</span>), 
                                 replace=<span style="color:#B5CEA8">False</span>)
df.loc[income_missing, <span style="color:#CE9178">'income'</span>] = np.nan
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"2. MAR: {len(income_missing)} income values missing (older customers)"</span>)

<span style="color:#6A9955"># 3. Missing Not at Random (MNAR) - people with low education don't report it</span>
low_education = df[df[<span style="color:#CE9178">'education'</span>] == <span style="color:#CE9178">'High School'</span>].index
education_missing = np.random.choice(low_education, 
                                   size=<span style="color:#B5CEA8">int</span>(<span style="color:#B5CEA8">len</span>(low_education) * <span style="color:#B5CEA8">0.2</span>), 
                                   replace=<span style="color:#B5CEA8">False</span>)
df.loc[education_missing, <span style="color:#CE9178">'education'</span>] = np.nan
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"3. MNAR: {len(education_missing)} education values missing (systematic)"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"\nMissing value summary:"</span>)
<span style="color:#DCDCAA">print</span>(df.isnull().sum())

<span style="color:#6A9955"># Strategy 1: Simple Imputation</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\nüîß Strategy 1: Simple Imputation"</span>)
simple_imputer_num = SimpleImputer(strategy=<span style="color:#CE9178">'median'</span>)
simple_imputer_cat = SimpleImputer(strategy=<span style="color:#CE9178">'most_frequent'</span>)

df_simple = df.copy()
numerical_cols = [<span style="color:#CE9178">'age'</span>, <span style="color:#CE9178">'income'</span>, <span style="color:#CE9178">'credit_score'</span>, <span style="color:#CE9178">'years_employed'</span>]
df_simple[numerical_cols] = simple_imputer_num.fit_transform(df_simple[numerical_cols])
df_simple[<span style="color:#CE9178">'education'</span>] = simple_imputer_cat.fit_transform(df_simple[[<span style="color:#CE9178">'education'</span>]]).ravel()

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Simple imputation completed. Missing values: {df_simple.isnull().sum().sum()}"</span>)

<span style="color:#6A9955"># Strategy 2: KNN Imputation (uses similar customers)</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\nü§ñ Strategy 2: KNN Imputation (using similar customers)"</span>)
knn_imputer = KNNImputer(n_neighbors=<span style="color:#B5CEA8">5</span>)

<span style="color:#6A9955"># Prepare data for KNN (encode categorical)</span>
df_knn = df.copy()
education_encoded = pd.get_dummies(df_knn[<span style="color:#CE9178">'education'</span>], prefix=<span style="color:#CE9178">'education'</span>)
df_knn = pd.concat([df_knn.drop(<span style="color:#CE9178">'education'</span>, axis=<span style="color:#B5CEA8">1</span>), education_encoded], axis=<span style="color:#B5CEA8">1</span>)

<span style="color:#6A9955"># Apply KNN imputation</span>
df_knn_imputed = pd.DataFrame(
    knn_imputer.fit_transform(df_knn.select_dtypes(include=[np.number])),
    columns=df_knn.select_dtypes(include=[np.number]).columns
)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"KNN imputation completed. Missing values: {df_knn_imputed.isnull().sum().sum()}"</span>)

<span style="color:#6A9955"># Strategy 3: Iterative Imputation (MICE - Multiple Imputation by Chained Equations)</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\nüî¨ Strategy 3: Iterative Imputation (MICE)"</span>)
iterative_imputer = IterativeImputer(random_state=<span style="color:#B5CEA8">42</span>, max_iter=<span style="color:#B5CEA8">10</span>)

df_mice = df.copy()
<span style="color:#6A9955"># Encode categorical for MICE</span>
df_mice['education_encoded'] = pd.Categorical(df_mice['education']).codes
df_mice['education_encoded'] = df_mice['education_encoded'].replace(-1, np.nan)

numerical_cols_mice = ['age', 'income', 'credit_score', 'years_employed', 'education_encoded']
df_mice[numerical_cols_mice] = iterative_imputer.fit_transform(df_mice[numerical_cols_mice])

print(f"MICE imputation completed. Missing values: {df_mice[numerical_cols_mice].isnull().sum().sum()}")

# Compare imputation quality
print("\nüìä Imputation Quality Comparison:")
original_income_mean = df['income'].mean()
print(f"Original income mean: ${original_income_mean:,.2f}")
print(f"Simple imputation: ${df_simple['income'].mean():,.2f}")
print(f"KNN imputation: ${df_knn_imputed['income'].mean():,.2f}")
print(f"MICE imputation: ${df_mice['income'].mean():,.2f}")

# Best practices
print("\nüí° Missing Value Best Practices:")
print("‚úÖ Always analyze WHY data is missing")
print("‚úÖ Document your imputation strategy")
print("‚úÖ Consider creating 'missing' indicator variables")
print("‚úÖ Validate imputation quality on a subset with known values")
print("‚úÖ Use domain knowledge to choose appropriate strategies")
print("‚úÖ Consider multiple imputation for critical decisions")
                        </code></pre>   
                </div>
            </div>

            <div class="card">
                <h3>‚öñÔ∏è Feature Scaling: Making Features Play Nice</h3>
                
                <div class="highlight-box warning">
                    <h4>üéØ Why Scaling Matters</h4>
                    <p>Imagine comparing running speed (measured in mph) with salary (measured in dollars). Without scaling, the salary feature would dominate simply because its numbers are larger, even if speed is more predictive!</p>
                </div>

                <div class="grid">
                    <div class="card">
                        <h4>üìè StandardScaler (Z-score normalization)</h4>
                        <p><strong>Formula:</strong> (x - mean) / standard_deviation</p>
                        <p><strong>Result:</strong> Mean = 0, Standard Deviation = 1</p>
                        <p><strong>Best for:</strong> Normal distributions, algorithms sensitive to variance (SVM, Neural Networks)</p>
                        <ul>
                            <li>Preserves relationships between features</li>
                            <li>Works well with normally distributed data</li>
                            <li>Most commonly used scaling method</li>
                        </ul>
                    </div>

                    <div class="card">
                        <h4>üéØ MinMaxScaler</h4>
                        <p><strong>Formula:</strong> (x - min) / (max - min)</p>
                        <p><strong>Result:</strong> Values between 0 and 1</p>
                        <p><strong>Best for:</strong> Bounded algorithms (Neural Networks), when you need [0,1] range</p>
                        <ul>
                            <li>Preserves zero values</li>
                            <li>Sensitive to outliers</li>
                            <li>Good for image data (pixels)</li>
                        </ul>
                    </div>

                    <div class="card">
                        <h4>üõ°Ô∏è RobustScaler</h4>
                        <p><strong>Formula:</strong> (x - median) / IQR</p>
                        <p><strong>Result:</strong> Median = 0, robust to outliers</p>
                        <p><strong>Best for:</strong> Data with many outliers</p>
                        <ul>
                            <li>Uses median instead of mean</li>
                            <li>Uses Interquartile Range instead of std</li>
                            <li>Less affected by extreme values</li>
                        </ul>
                    </div>
                </div>

                <div class="code-block">
<pre><code class="language-python">
<span style="color:#6A9955"># Feature Scaling Deep Dive: When and How to Scale</span>
<span style="color:#DCDCAA">import</span> numpy <span style="color:#DCDCAA">as</span> np
<span style="color:#DCDCAA">import</span> pandas <span style="color:#DCDCAA">as</span> pd
<span style="color:#DCDCAA">from</span> sklearn.preprocessing <span style="color:#DCDCAA">import</span> StandardScaler, MinMaxScaler, RobustScaler
<span style="color:#DCDCAA">from</span> sklearn.model_selection <span style="color:#DCDCAA">import</span> train_test_split
<span style="color:#DCDCAA">from</span> sklearn.linear_model <span style="color:#DCDCAA">import</span> LogisticRegression
<span style="color:#DCDCAA">from</span> sklearn.metrics <span style="color:#DCDCAA">import</span> accuracy_score
<span style="color:#DCDCAA">import</span> matplotlib.pyplot <span style="color:#DCDCAA">as</span> plt

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚öñÔ∏è Feature Scaling: The Science Behind the Magic"</span>)

<span style="color:#6A9955"># Create realistic dataset with different scales</span>
np.random.seed(<span style="color:#B5CEA8">42</span>)
n_samples = <span style="color:#B5CEA8">1000</span>

<span style="color:#6A9955"># Simulate employee data with vastly different scales</span>
data = {
    <span style="color:#CE9178">'age'</span>: np.random.normal(<span style="color:#B5CEA8">35</span>, <span style="color:#B5CEA8">8</span>, n_samples),                    <span style="color:#6A9955"># 20-60 range</span>
    <span style="color:#CE9178">'salary'</span>: np.random.normal(<span style="color:#B5CEA8">75000</span>, <span style="color:#B5CEA8">25000</span>, n_samples),         <span style="color:#6A9955"># 20k-150k range</span>
    <span style="color:#CE9178">'years_experience'</span>: np.random.exponential(<span style="color:#B5CEA8">5</span>, n_samples),     <span style="color:#6A9955"># 0-30 range</span>
    <span style="color:#CE9178">'performance_score'</span>: np.random.normal(<span style="color:#B5CEA8">3.5</span>, <span style="color:#B5CEA8">0.8</span>, n_samples), <span style="color:#6A9955"># 1-5 range</span>
    <span style="color:#CE9178">'hours_per_week'</span>: np.random.normal(<span style="color:#B5CEA8">42</span>, <span style="color:#B5CEA8">8</span>, n_samples)        <span style="color:#6A9955"># 20-60 range</span>
}

<span style="color:#6A9955"># Add some outliers (realistic scenario)</span>
outlier_indices = np.random.choice(n_samples, size=<span style="color:#B5CEA8">20</span>, replace=<span style="color:#B5CEA8">False</span>)
data[<span style="color:#CE9178">'salary'</span>][outlier_indices] *= <span style="color:#B5CEA8">3</span>  <span style="color:#6A9955"># Some very high earners</span>

df = pd.DataFrame(data)

<span style="color:#6A9955"># Create target variable (promotion eligibility)</span>
promotion_score = (
    df[<span style="color:#CE9178">'performance_score'</span>] * <span style="color:#B5CEA8">0.4</span> + 
    df[<span style="color:#CE9178">'years_experience'</span>] * <span style="color:#B5CEA8">0.1</span> + 
    (df[<span style="color:#CE9178">'salary'</span>] / <span style="color:#B5CEA8">100000</span>) * <span style="color:#B5CEA8">0.3</span> +
    np.random.normal(<span style="color:#B5CEA8">0</span>, <span style="color:#B5CEA8">0.3</span>, n_samples)
)
df[<span style="color:#CE9178">'promotion_eligible'</span>] = (promotion_score > promotion_score.median()).astype(<span style="color:#CE9178">int</span>)

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üìä Original Data Statistics:"</span>)
<span style="color:#DCDCAA">print</span>(df.describe().round(<span style="color:#B5CEA8">2</span>))

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"\nüéØ Feature Scale Comparison:"</span>)
<span style="color:#DCDCAA">for</span> col <span style="color:#DCDCAA">in</span> [<span style="color:#CE9178">'age'</span>, <span style="color:#CE9178">'salary'</span>, <span style="color:#CE9178">'years_experience'</span>, <span style="color:#CE9178">'performance_score'</span>]:
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">{col:20}: Range = {df[col].min():.0f} to {df[col].max():.0f}</span>)

<span style="color:#6A9955"># Split data</span>
X = df.drop(<span style="color:#CE9178">'promotion_eligible'</span>, axis=<span style="color:#B5CEA8">1</span>)
y = df[<span style="color:#CE9178">'promotion_eligible'</span>]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span style="color:#B5CEA8">0.2</span>, random_state=<span style="color:#B5CEA8">42</span>)

<span style="color:#6A9955"># Test model performance WITHOUT scaling</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"\nüö´ Model Performance WITHOUT Scaling:"</span>)
model_unscaled = LogisticRegression(random_state=<span style="color:#B5CEA8">42</span>, max_iter=<span style="color:#B5CEA8">1000</span>)
model_unscaled.fit(X_train, y_train)
accuracy_unscaled = accuracy_score(y_test, model_unscaled.predict(X_test))
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Accuracy: {accuracy_unscaled:.3f}"</span>)

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Feature coefficients (shows scale bias):"</span>)
<span style="color:#DCDCAA">for</span> feature, coef <span style="color:#DCDCAA">in</span> zip(X.columns, model_unscaled.coef_[<span style="color:#B5CEA8">0</span>]):
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  {feature:20}: {coef:.6f}"</span>)

<span style="color:#6A9955"># Apply different scaling methods</span>
scalers = {
    <span style="color:#CE9178">'StandardScaler'</span>: StandardScaler(),
    <span style="color:#CE9178">'MinMaxScaler'</span>: MinMaxScaler(),
    <span style="color:#CE9178">'RobustScaler'</span>: RobustScaler()
}

results = {}

<span style="color:#DCDCAA">for</span> scaler_name, scaler <span style="color:#DCDCAA">in</span> scalers.items():
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüîß Testing {scaler_name}:"</span>)
    
    <span style="color:#6A9955"># Fit scaler on training data only!</span>
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    <span style="color:#6A9955"># Show scaling effect</span>
    scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"After scaling - Mean: {scaled_df.mean().round(3).values}"</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"After scaling - Std:  {scaled_df.std().round(3).values}"</span>)
    
    <span style="color:#6A9955"># Train model on scaled data</span>
    model = LogisticRegression(random_state=<span style="color:#B5CEA8">42</span>, max_iter=<span style="color:#B5CEA8">1000</span>)
    model.fit(X_train_scaled, y_train)
    
    <span style="color:#6A9955"># Evaluate</span>
    accuracy = accuracy_score(y_test, model.predict(X_test_scaled))
    results[scaler_name] = accuracy
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Accuracy: {accuracy:.3f}"</span>)

<span style="color:#6A9955"># Compare all results</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüìà Scaling Method Comparison:"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"No Scaling:     {accuracy_unscaled:.3f}"</span>)
<span style="color:#DCDCAA">for</span> method, acc <span style="color:#DCDCAA">in</span> results.items():
    improvement = acc - accuracy_unscaled
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">{method:15}: {acc:.3f} ({improvement:+.3f})"</span>)

<span style="color:#6A9955"># Demonstrate why scaling matters with a visual example</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüëÅÔ∏è Visual Example: Why Salary Dominates Without Scaling"</span>)
sample_customer = X_test.iloc[<span style="color:#B5CEA8">0</span>]
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Sample employee features:"</span>)
<span style="color:#DCDCAA">for</span> feature, value <span style="color:#DCDCAA">in</span> sample_customer.items():
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  {feature:20}: {value:.1f}"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nWithout scaling, salary ({sample_customer['salary']:.0f}) dominates age ({sample_customer['age']:.0f})"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"The algorithm treats salary as 2000x more important just due to scale!"</span>)

<span style="color:#6A9955"># Advanced scaling considerations</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüéì Advanced Scaling Considerations:"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\n1. When to use each scaler:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   StandardScaler: Normal distributions, most ML algorithms"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   MinMaxScaler: Bounded outputs needed, neural networks"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   RobustScaler: Many outliers present, skewed distributions"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\n2. Features that typically DON'T need scaling:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚Ä¢ Already scaled features (percentages, ratios)"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚Ä¢ Tree-based algorithms (Random Forest, XGBoost)"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚Ä¢ Categorical encoded features"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\n3. Critical scaling rules:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚úÖ Always fit scaler on training data only"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚úÖ Apply same scaling to train, validation, and test sets"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚úÖ Save scaler for production predictions"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   ‚úÖ Scale features before dimensionality reduction"</span>)

<span style="color:#6A9955"># Demonstrate the cardinal sin: fitting scaler on test data</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\n‚ùå NEVER DO THIS - Common Scaling Mistake:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"# Wrong way - fits on all data!"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"scaler.fit(X)  # This includes test data!"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"X_train_scaled = scaler.transform(X_train)"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"X_test_scaled = scaler.transform(X_test)"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">""</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ Correct way:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"scaler.fit(X_train)  # Only training data!"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"X_train_scaled = scaler.transform(X_train)"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"X_test_scaled = scaler.transform(X_test)"</span>)
</code></pre>   
                </div>
            </div>

            <div class="card">
                <h3>üî§ Categorical Encoding: From Text to Numbers</h3>
                
                <div class="highlight-box info">
                    <h4>üéØ The Challenge</h4>
                    <p>Machine learning algorithms work with numbers, not text. Converting categorical data (like "Red", "Blue", "Green") into numerical format while preserving meaningful relationships is both an art and science.</p>
                </div>

                <div class="grid">
                    <div class="card">
                        <h4>üè∑Ô∏è Label Encoding</h4>
                        <p><strong>Method:</strong> Assign each category a unique integer</p>
                        <p><strong>Example:</strong> Red=0, Blue=1, Green=2</p>
                        <p><strong>Pros:</strong> Memory efficient, simple</p>
                        <p><strong>Cons:</strong> Implies ordering (Blue > Red)</p>
                        <p><strong>Use for:</strong> Ordinal data, tree-based algorithms</p>
                    </div>

                    <div class="card">
                        <h4>üéØ One-Hot Encoding</h4>
                        <p><strong>Method:</strong> Create binary column for each category</p>
                        <p><strong>Example:</strong> Red=[1,0,0], Blue=[0,1,0], Green=[0,0,1]</p>
                        <p><strong>Pros:</strong> No artificial ordering</p>
                        <p><strong>Cons:</strong> High dimensionality, sparse data</p>
                        <p><strong>Use for:</strong> Nominal data, linear algorithms</p>
                    </div>

                    <div class="card">
                        <h4>üìä Target Encoding</h4>
                        <p><strong>Method:</strong> Replace category with target statistic</p>
                        <p><strong>Example:</strong> City ‚Üí Average income in that city</p>
                        <p><strong>Pros:</strong> Captures predictive power</p>
                        <p><strong>Cons:</strong> Risk of overfitting</p>
                        <p><strong>Use for:</strong> High cardinality categorical features</p>
                    </div>
                </div>

                <div class="code-block">
<pre><code class="language-python">
<span style="color:#6A9955"># Categorical Encoding: Comprehensive Guide</span>
<span style="color:#DCDCAA">import</span> pandas <span style="color:#DCDCAA">as</span> pd
<span style="color:#DCDCAA">import</span> numpy <span style="color:#DCDCAA">as</span> np
<span style="color:#DCDCAA">from</span> sklearn.preprocessing <span style="color:#DCDCAA">import</span> LabelEncoder, OneHotEncoder
<span style="color:#DCDCAA">from</span> sklearn.model_selection <span style="color:#DCDCAA">import</span> train_test_split
<span style="color:#DCDCAA">from</span> sklearn.ensemble <span style="color:#DCDCAA">import</span> RandomForestClassifier
<span style="color:#DCDCAA">from</span> sklearn.linear_model <span style="color:#DCDCAA">import</span> LogisticRegression
<span style="color:#DCDCAA">from</span> sklearn.metrics <span style="color:#DCDCAA">import</span> accuracy_score
<span style="color:#DCDCAA">import</span> category_encoders <span style="color:#DCDCAA">as</span> ce  <span style="color:#6A9955"># pip install category_encoders</span>

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üî§ Categorical Encoding: From Text to Meaningful Numbers"</span>)

<span style="color:#6A9955"># Create realistic dataset with different categorical types</span>
np.random.seed(<span style="color:#B5CEA8">42</span>)
n_samples = <span style="color:#B5CEA8">1000</span>

<span style="color:#6A9955"># Simulate car sales data</span>
brands = [<span style="color:#CE9178">'Toyota'</span>, <span style="color:#CE9178">'Honda'</span>, <span style="color:#CE9178">'Ford'</span>, <span style="color:#CE9178">'BMW'</span>, <span style="color:#CE9178">'Audi'</span>, <span style="color:#CE9178">'Mercedes'</span>, <span style="color:#CE9178">'Hyundai'</span>, <span style="color:#CE9178">'Kia'</span>]
colors = [<span style="color:#CE9178">'White'</span>, <span style="color:#CE9178">'Black'</span>, <span style="color:#CE9178">'Silver'</span>, <span style="color:#CE9178">'Red'</span>, <span style="color:#CE9178">'Blue'</span>]
sizes = [<span style="color:#CE9178">'Compact'</span>, <span style="color:#CE9178">'Mid-size'</span>, <span style="color:#CE9178">'Full-size'</span>, <span style="color:#CE9178">'SUV'</span>]
fuel_types = [<span style="color:#CE9178">'Gasoline'</span>, <span style="color:#CE9178">'Hybrid'</span>, <span style="color:#CE9178">'Electric'</span>]

<span style="color:#6A9955"># Different types of categorical variables</span>
data = {
    <span style="color:#CE9178">'brand'</span>: np.random.choice(brands, n_samples, p=[<span style="color:#B5CEA8">0.15</span>, <span style="color:#B5CEA8">0.15</span>, <span style="color:#B5CEA8">0.12</span>, <span style="color:#B5CEA8">0.08</span>, <span style="color:#B5CEA8">0.08</span>, <span style="color:#B5CEA8">0.07</span>, <span style="color:#B5CEA8">0.175</span>, <span style="color:#B5CEA8">0.175</span>]),
    <span style="color:#CE9178">'color'</span>: np.random.choice(colors, n_samples, p=[<span style="color:#B5CEA8">0.3</span>, <span style="color:#B5CEA8">0.25</span>, <span style="color:#B5CEA8">0.2</span>, <span style="color:#B5CEA8">0.15</span>, <span style="color:#B5CEA8">0.1</span>]),
    <span style="color:#CE9178">'size'</span>: np.random.choice(sizes, n_samples, p=[<span style="color:#B5CEA8">0.3</span>, <span style="color:#B5CEA8">0.3</span>, <span style="color:#B5CEA8">0.2</span>, <span style="color:#B5CEA8">0.2</span>]),
    <span style="color:#CE9178">'fuel_type'</span>: np.random.choice(fuel_types, n_samples, p=[<span style="color:#B5CEA8">0.7</span>, <span style="color:#B5CEA8">0.25</span>, <span style="color:#B5CEA8">0.05</span>]),
    <span style="color:#CE9178">'year'</span>: np.random.randint(<span style="color:#B5CEA8">2015</span>, <span style="color:#B5CEA8">2024</span>, n_samples),
    <span style="color:#CE9178">'mileage'</span>: np.random.exponential(<span style="color:#B5CEA8">30000</span>, n_samples),
    <span style="color:#CE9178">'price'</span>: np.random.normal(<span style="color:#B5CEA8">25000</span>, <span style="color:#B5CEA8">8000</span>, n_samples)
}

df = pd.DataFrame(data)

<span style="color:#6A9955"># Create target: high-value car (above median price)</span>
df[<span style="color:#CE9178">'high_value'</span>] = (df[<span style="color:#CE9178">'price'</span>] > df[<span style="color:#CE9178">'price'</span>].median()).astype(<span style="color:#CE9178">int</span>)

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üöó Car Sales Dataset:"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Shape: {df.shape}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nCategorical features:"</span>)
categorical_features = [<span style="color:#CE9178">'brand'</span>, <span style="color:#CE9178">'color'</span>, <span style="color:#CE9178">'size'</span>, <span style="color:#CE9178">'fuel_type'</span>]
<span style="color:#DCDCAA">for</span> col <span style="color:#DCDCAA">in</span> categorical_features:
    unique_count = df[col].nunique()
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  {col:12}: {unique_count:2d} unique values"</span>)
    <span style="color:#DCDCAA">if</span> unique_count <= <span style="color:#B5CEA8">5</span>:
        <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"    Values: {list(df[col].unique())}"</span>)

<span style="color:#6A9955"># Analyze categorical relationships with target</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüìä Categorical Feature Analysis:"</span>)
<span style="color:#DCDCAA">for</span> col <span style="color:#DCDCAA">in</span> categorical_features:
    high_value_rate = df.groupby(col)[<span style="color:#CE9178">'high_value'</span>].mean().sort_values(ascending=<span style="color:#B5CEA8">False</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\n{col} - High value rate by category:"</span>)
    <span style="color:#DCDCAA">for</span> category, rate <span style="color:#DCDCAA">in</span> high_value_rate.head(<span style="color:#B5CEA8">3</span>).items():
        <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  {category:12}: {rate:.1%}"</span>)

<span style="color:#6A9955"># Prepare data for encoding experiments</span>
X = df[categorical_features + [<span style="color:#CE9178">'year'</span>, <span style="color:#CE9178">'mileage'</span>]]
y = df[<span style="color:#CE9178">'high_value'</span>]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span style="color:#B5CEA8">0.2</span>, random_state=<span style="color:#B5CEA8">42</span>)

<span style="color:#6A9955"># Method 1: Label Encoding</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüè∑Ô∏è Method 1: Label Encoding"</span>)
X_train_label = X_train.copy()
X_test_label = X_test.copy()

label_encoders = {}
<span style="color:#DCDCAA">for</span> col <span style="color:#DCDCAA">in</span> categorical_features:
    le = LabelEncoder()
    X_train_label[col] = le.fit_transform(X_train[col])
    X_test_label[col] = le.transform(X_test[col])
    label_encoders[col] = le
    
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  {col}: {le.classes_[:3]}... ‚Üí {[0, 1, 2]}..."</span>)

<span style="color:#6A9955"># Test with tree-based model (works well with label encoding)</span>
rf_label = RandomForestClassifier(n_estimators=<span style="color:#B5CEA8">100</span>, random_state=<span style="color:#B5CEA8">42</span>)
rf_label.fit(X_train_label, y_train)
accuracy_label_rf = accuracy_score(y_test, rf_label.predict(X_test_label))
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Random Forest accuracy: {accuracy_label_rf:.3f}"</span>)

<span style="color:#6A9955"># Method 2: One-Hot Encoding</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüéØ Method 2: One-Hot Encoding"</span>)
X_train_onehot = pd.get_dummies(X_train, columns=categorical_features, prefix=categorical_features)
X_test_onehot = pd.get_dummies(X_test, columns=categorical_features, prefix=categorical_features)

<span style="color:#6A9955"># Ensure same columns in train and test</span>
missing_cols = set(X_train_onehot.columns) - set(X_test_onehot.columns)
<span style="color:#DCDCAA">for</span> col <span style="color:#DCDCAA">in</span> missing_cols:
    X_test_onehot[col] = <span style="color:#B5CEA8">0</span>
X_test_onehot = X_test_onehot[X_train_onehot.columns]

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  Original features: {len(categorical_features)}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  After one-hot: {X_train_onehot.shape[1]} features"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  Memory increase: {X_train_onehot.memory_usage(deep=True).sum() / X_train.memory_usage(deep=True).sum():.1f}x"</span>)

<span style="color:#6A9955"># Test with linear model (benefits from one-hot)</span>
lr_onehot = LogisticRegression(random_state=<span style="color:#B5CEA8">42</span>, max_iter=<span style="color:#B5CEA8">1000</span>)
lr_onehot.fit(X_train_onehot, y_train)
accuracy_onehot_lr = accuracy_score(y_test, lr_onehot.predict(X_test_onehot))
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Logistic Regression accuracy: {accuracy_onehot_lr:.3f}"</span>)

<span style="color:#6A9955"># Method 3: Target Encoding (Mean Encoding)</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüìä Method 3: Target Encoding"</span>)
X_train_target = X_train.copy()
X_test_target = X_test.copy()

<span style="color:#6A9955"># Calculate target means for each category (with smoothing)</span>
target_encoders = {}
<span style="color:#DCDCAA">for</span> col <span style="color:#DCDCAA">in</span> categorical_features:
    <span style="color:#6A9955"># Calculate mean target value for each category</span>
    target_means = X_train.groupby(col)[y_train.reset_index(drop=True)].mean()
    
    <span style="color:#6A9955"># Add smoothing to prevent overfitting (using global mean)</span>
    global_mean = y_train.mean()
    category_counts = X_train[col].value_counts()
    
    <span style="color:#6A9955"># Smoothed encoding: (category_mean * count + global_mean * smooth) / (count + smooth)</span>
    smooth_factor = <span style="color:#B5CEA8">10</span>
    smoothed_means = (target_means * category_counts + global_mean * smooth_factor) / (category_counts + smooth_factor)
    
    target_encoders[col] = smoothed_means
    
    <span style="color:#6A9955"># Apply encoding</span>
    X_train_target[col] = X_train[col].map(smoothed_means)
    X_test_target[col] = X_test[col].map(smoothed_means).fillna(global_mean)
    
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  {col} encoding samples:"</span>)
    <span style="color:#DCDCAA">for</span> category, encoded_value <span style="color:#DCDCAA">in</span> smoothed_means.head(<span style="color:#B5CEA8">3</span>).items():
        <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"    {category:12} ‚Üí {encoded_value:.3f}"</span>)

<span style="color:#6A9955"># Test target encoding</span>
rf_target = RandomForestClassifier(n_estimators=<span style="color:#B5CEA8">100</span>, random_state=<span style="color:#B5CEA8">42</span>)
rf_target.fit(X_train_target, y_train)
accuracy_target_rf = accuracy_score(y_test, rf_target.predict(X_test_target))
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Random Forest accuracy: {accuracy_target_rf:.3f}"</span>)

<span style="color:#6A9955"># Method 4: Advanced Encoders (using category_encoders library)</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüöÄ Method 4: Advanced Encoders"</span>)

<span style="color:#6A9955"># Binary Encoding (combines benefits of label and one-hot)</span>
binary_encoder = ce.BinaryEncoder(cols=categorical_features)
X_train_binary = binary_encoder.fit_transform(X_train, y_train)
X_test_binary = binary_encoder.transform(X_test)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Binary encoding: {X_train.shape[1]} ‚Üí {X_train_binary.shape[1]} features"</span>)

<span style="color:#6A9955"># Test binary encoding</span>
rf_binary = RandomForestClassifier(n_estimators=<span style="color:#B5CEA8">100</span>, random_state=<span style="color:#B5CEA8">42</span>)
rf_binary.fit(X_train_binary, y_train)
accuracy_binary_rf = accuracy_score(y_test, rf_binary.predict(X_test_binary))
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Random Forest accuracy: {accuracy_binary_rf:.3f}"</span>)

<span style="color:#6A9955"># Compare all methods</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüìà Encoding Method Comparison:"</span>)
results = {
    <span style="color:#CE9178">'Label Encoding (RF)'</span>: accuracy_label_rf,
    <span style="color:#CE9178">'One-Hot Encoding (LR)'</span>: accuracy_onehot_lr,
    <span style="color:#CE9178">'Target Encoding (RF)'</span>: accuracy_target_rf,
    <span style="color:#CE9178">'Binary Encoding (RF)'</span>: accuracy_binary_rf
}

<span style="color:#DCDCAA">for</span> method, accuracy <span style="color:#DCDCAA">in</span> sorted(results.items(), key=<span style="color:#DCDCAA">lambda</span> x: x[<span style="color:#B5CEA8">1</span>], reverse=<span style="color:#B5CEA8">True</span>):
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  {method:25}: {accuracy:.3f}"</span>)

<span style="color:#6A9955"># Encoding selection guide</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüéì Encoding Selection Guide:"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüìä High Cardinality (many categories):"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  ‚úÖ Target Encoding - captures predictive power"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  ‚úÖ Binary Encoding - compact representation"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  ‚ùå One-Hot - creates too many features"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüìè Low Cardinality (few categories):"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  ‚úÖ One-Hot - for linear models"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  ‚úÖ Label Encoding - for tree models"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  ‚ö†Ô∏è Target Encoding - may overfit"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüéØ Ordinal Data (natural order):"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  ‚úÖ Ordinal Encoding - preserves order"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  ‚ùå One-Hot - loses ordering information"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\n‚ö†Ô∏è Critical Encoding Rules:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  1. Always fit encoders on training data only"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  2. Handle unseen categories in test data"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  3. Consider target leakage with target encoding"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  4. Validate encoding choice with cross-validation"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  5. Document encoding decisions for production"</span>)
</code></pre>       
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üîÑ</span>The Complete ML Workflow: From Problem to Production</h2>
            
            <div class="highlight-box info">
                <h4>üéØ The Iterative Nature of ML</h4>
                <p>ML isn't a linear process‚Äîit's a cycle of experimentation, learning, and refinement. Each step informs the others, and you'll often loop back to earlier stages as you gain insights.</p>
            </div>

            <div class="flowchart">
                <div class="flow-step" style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);">
                    <div class="step-number">1</div>
                    <div>
                        <h4>üéØ Problem Definition & Business Understanding</h4>
                        <p><strong>Questions to ask:</strong> What business problem are we solving? What does success look like? What's the impact of being wrong?</p>
                        <ul style="margin-top: 10px;">
                            <li>Define success metrics (accuracy, ROI, user satisfaction)</li>
                            <li>Understand business constraints (time, budget, interpretability)</li>
                            <li>Identify stakeholders and their needs</li>
                            <li>Determine if ML is the right solution</li>
                        </ul>
                    </div>
                </div>

                <div class="flow-step" style="background: linear-gradient(135deg, #e8f5e8 0%, #c8e6c9 100%);">
                    <div class="step-number">2</div>
                    <div>
                        <h4>üìä Data Collection & Understanding</h4>
                        <p><strong>Data audit:</strong> What data do we have? What's missing? How reliable is it?</p>
                        <ul style="margin-top: 10px;">
                            <li>Inventory available data sources</li>
                            <li>Assess data quality and completeness</li>
                            <li>Understand data collection process</li>
                            <li>Identify potential biases and limitations</li>
                        </ul>
                    </div>
                </div>

                <div class="flow-step" style="background: linear-gradient(135deg, #fff8e1 0%, #ffecb3 100%);">
                    <div class="step-number">3</div>
                    <div>
                        <h4>üîç Exploratory Data Analysis (EDA)</h4>
                        <p><strong>Detective work:</strong> What patterns exist? What insights can guide our modeling?</p>
                        <ul style="margin-top: 10px;">
                            <li>Visualize distributions and relationships</li>
                            <li>Identify outliers and anomalies</li>
                            <li>Discover feature correlations</li>
                            <li>Generate hypotheses for modeling</li>
                        </ul>
                    </div>
                </div>

                <div class="flow-step" style="background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);">
                    <div class="step-number">4</div>
                    <div>
                        <h4>üõ†Ô∏è Data Preprocessing & Feature Engineering</h4>
                        <p><strong>Data preparation:</strong> Transform raw data into ML-ready format</p>
                        <ul style="margin-top: 10px;">
                            <li>Handle missing values and outliers</li>
                            <li>Scale and encode features</li>
                            <li>Create new meaningful features</li>
                            <li>Split data into train/validation/test sets</li>
                        </ul>
                    </div>
                </div>

                <div class="flow-step" style="background: linear-gradient(135deg, #ffebee 0%, #ffcdd2 100%);">
                    <div class="step-number">5</div>
                    <div>
                        <h4>ü§ñ Model Selection & Training</h4>
                        <p><strong>Algorithm choice:</strong> Which ML approach best fits our problem and data?</p>
                        <ul style="margin-top: 10px;">
                            <li>Select appropriate algorithm family</li>
                            <li>Implement baseline models</li>
                            <li>Train multiple candidate models</li>
                            <li>Tune hyperparameters systematically</li>
                        </ul>
                    </div>
                </div>

                <div class="flow-step" style="background: linear-gradient(135deg, #e0f2f1 0%, #b2dfdb 100%);">
                    <div class="step-number">6</div>
                    <div>
                        <h4>üìà Model Evaluation & Validation</h4>
                        <p><strong>Performance assessment:</strong> How well does our model generalize to new data?</p>
                        <ul style="margin-top: 10px;">
                            <li>Evaluate using appropriate metrics</li>
                            <li>Perform cross-validation</li>
                            <li>Test on held-out data</li>
                            <li>Analyze errors and edge cases</li>
                        </ul>
                    </div>
                </div>

                <div class="flow-step" style="background: linear-gradient(135deg, #fce4ec 0%, #f8bbd9 100%);">
                    <div class="step-number">7</div>
                    <div>
                        <h4>üöÄ Deployment & Monitoring</h4>
                        <p><strong>Production deployment:</strong> Make the model available for real-world use</p>
                        <ul style="margin-top: 10px;">
                            <li>Deploy model to production environment</li>
                            <li>Monitor performance and data drift</li>
                            <li>Set up alerts and feedback loops</li>
                            <li>Plan for model updates and maintenance</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="code-block">
<pre><code class="language-python">
<span style="color:#6A9955"># Complete ML Workflow: End-to-End Project</span>
<span style="color:#DCDCAA">import</span> pandas <span style="color:#DCDCAA">as</span> pd
<span style="color:#DCDCAA">import</span> numpy <span style="color:#DCDCAA">as</span> np
<span style="color:#DCDCAA">import</span> matplotlib.pyplot <span style="color:#DCDCAA">as</span> plt
<span style="color:#DCDCAA">import</span> seaborn <span style="color:#DCDCAA">as</span> sns
<span style="color:#DCDCAA">from</span> sklearn.model_selection <span style="color:#DCDCAA">import</span> train_test_split, cross_val_score, GridSearchCV
<span style="color:#DCDCAA">from</span> sklearn.preprocessing <span style="color:#DCDCAA">import</span> StandardScaler, LabelEncoder
<span style="color:#DCDCAA">from</span> sklearn.ensemble <span style="color:#DCDCAA">import</span> RandomForestClassifier
<span style="color:#DCDCAA">from</span> sklearn.linear_model <span style="color:#DCDCAA">import</span> LogisticRegression
<span style="color:#DCDCAA">from</span> sklearn.svm <span style="color:#DCDCAA">import</span> SVC
<span style="color:#DCDCAA">from</span> sklearn.metrics <span style="color:#DCDCAA">import</span> classification_report, confusion_matrix, roc_auc_score
<span style="color:#DCDCAA">import</span> warnings
warnings.filterwarnings(<span style="color:#CE9178">'ignore'</span>)

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üîÑ Complete ML Workflow: Customer Churn Prediction"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"="*60</span>)

<span style="color:#6A9955"># Step 1: Problem Definition</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"1Ô∏è‚É£ PROBLEM DEFINITION"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Business Goal: Predict which customers will churn (cancel subscription)"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Success Metric: ROC-AUC > 0.85 (balanced for precision/recall)"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Business Impact: Reduce churn by 20% through targeted retention"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Constraints: Model must be interpretable for business stakeholders"</span>)

<span style="color:#6A9955"># Step 2: Data Collection (Simulated)</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n2Ô∏è‚É£ DATA COLLECTION"</span>)
np.random.seed(<span style="color:#B5CEA8">42</span>)
n_customers = <span style="color:#B5CEA8">5000</span>

<span style="color:#6A9955"># Simulate realistic customer data</span>
data = {
    <span style="color:#CE9178">'customer_id'</span>: <span style="color:#B5CEA8">range</span>(<span style="color:#B5CEA8">1</span>, n_customers + <span style="color:#B5CEA8">1</span>),
    <span style="color:#CE9178">'age'</span>: np.random.normal(<span style="color:#B5CEA8">40</span>, <span style="color:#B5CEA8">15</span>, n_customers).clip(<span style="color:#B5CEA8">18</span>, <span style="color:#B5CEA8">80</span>),
    <span style="color:#CE9178">'tenure_months'</span>: np.random.exponential(<span style="color:#B5CEA8">24</span>, n_customers).clip(<span style="color:#B5CEA8">1</span>, <span style="color:#B5CEA8">120</span>),
    <span style="color:#CE9178">'monthly_charges'</span>: np.random.normal(<span style="color:#B5CEA8">65</span>, <span style="color:#B5CEA8">20</span>, n_customers).clip(<span style="color:#B5CEA8">20</span>, <span style="color:#B5CEA8">150</span>),
    <span style="color:#CE9178">'total_charges'</span>: <span style="color:#B5CEA8">lambda</span>: <span style="color:#B5CEA8">None</span>,  <span style="color:#6A9955"># Will calculate based on tenure and monthly</span>
    <span style="color:#CE9178">'contract_type'</span>: np.random.choice([<span style="color:#CE9178">'Month-to-month'</span>, <span style="color:#CE9178">'One year'</span>, <span style="color:#CE9178">'Two year'</span>], 
                                     n_customers, p=[<span style="color:#B5CEA8">0.5</span>, <span style="color:#B5CEA8">0.3</span>, <span style="color:#B5CEA8">0.2</span>]),
    <span style="color:#CE9178">'payment_method'</span>: np.random.choice([<span style="color:#CE9178">'Electronic check'</span>, <span style="color:#CE9178">'Mailed check'</span>, <span style="color:#CE9178">'Bank transfer'</span>, <span style="color:#CE9178">'Credit card'</span>],
                                      n_customers, p=[<span style="color:#B5CEA8">0.35</span>, <span style="color:#B5CEA8">0.2</span>, <span style="color:#B5CEA8">0.25</span>, <span style="color:#B5CEA8">0.2</span>]),
    <span style="color:#CE9178">'internet_service'</span>: np.random.choice([<span style="color:#CE9178">'DSL'</span>, <span style="color:#CE9178">'Fiber optic'</span>, <span style="color:#CE9178">'No'</span>], 
                                        n_customers, p=[<span style="color:#B5CEA8">0.4</span>, <span style="color:#B5CEA8">0.45</span>, <span style="color:#B5CEA8">0.15</span>]),
    <span style="color:#CE9178">'online_security'</span>: np.random.choice([<span style="color:#CE9178">'Yes'</span>, <span style="color:#CE9178">'No'</span>, <span style="color:#CE9178">'No internet service'</span>], 
                                       n_customers, p=[<span style="color:#B5CEA8">0.3</span>, <span style="color:#B5CEA8">0.55</span>, <span style="color:#B5CEA8">0.15</span>]),
    <span style="color:#CE9178">'tech_support'</span>: np.random.choice([<span style="color:#CE9178">'Yes'</span>, <span style="color:#CE9178">'No'</span>, <span style="color:#CE9178">'No internet service'</span>], 
                                    n_customers, p=[<span style="color:#B5CEA8">0.35</span>, <span style="color:#B5CEA8">0.5</span>, <span style="color:#B5CEA8">0.15</span>]),
    <span style="color:#CE9178">'streaming_tv'</span>: np.random.choice([<span style="color:#CE9178">'Yes'</span>, <span style="color:#CE9178">'No'</span>, <span style="color:#CE9178">'No internet service'</span>], 
                                    n_customers, p=[<span style="color:#B5CEA8">0.4</span>, <span style="color:#B5CEA8">0.45</span>, <span style="color:#B5CEA8">0.15</span>]),
    <span style="color:#CE9178">'paperless_billing'</span>: np.random.choice([<span style="color:#CE9178">'Yes'</span>, <span style="color:#CE9178">'No'</span>], n_customers, p=[<span style="color:#B5CEA8">0.6</span>, <span style="color:#B5CEA8">0.4</span>]),
    <span style="color:#CE9178">'senior_citizen'</span>: np.random.choice([<span style="color:#B5CEA8">0</span>, <span style="color:#B5CEA8">1</span>], n_customers, p=[<span style="color:#B5CEA8">0.85</span>, <span style="color:#B5CEA8">0.15</span>])
}

df = pd.DataFrame(data)

<span style="color:#6A9955"># Calculate total charges</span>
df[<span style="color:#CE9178">'total_charges'</span>] = df[<span style="color:#CE9178">'tenure_months'</span>] * df[<span style="color:#CE9178">'monthly_charges'</span>] + np.random.normal(<span style="color:#B5CEA8">0</span>, <span style="color:#B5CEA8">100</span>, n_customers)
df[<span style="color:#CE9178">'total_charges'</span>] = df[<span style="color:#CE9178">'total_charges'</span>].clip(<span style="color:#B5CEA8">0</span>, <span style="color:#B5CEA8">None</span>)

<span style="color:#6A9955"># Create realistic churn target</span>
churn_probability = (
    <span style="color:#B5CEA8">0.3</span> * (df[<span style="color:#CE9178">'contract_type'</span>] == <span style="color:#CE9178">'Month-to-month'</span>) +
    <span style="color:#B5CEA8">0.2</span> * (df[<span style="color:#CE9178">'payment_method'</span>] == <span style="color:#CE9178">'Electronic check'</span>) +
    <span style="color:#B5CEA8">0.15</span> * (df[<span style="color:#CE9178">'monthly_charges'</span>] > <span style="color:#B5CEA8">80</span>) +
    <span style="color:#B5CEA8">0.1</span> * (df[<span style="color:#CE9178">'tenure_months'</span>] < <span style="color:#B5CEA8">12</span>) +
    <span style="color:#B5CEA8">0.1</span> * (df[<span style="color:#CE9178">'senior_citizen'</span>] == <span style="color:#B5CEA8">1</span>) +
    <span style="color:#B5CEA8">0.05</span> * (df[<span style="color:#CE9178">'online_security'</span>] == <span style="color:#CE9178">'No'</span>) +
    np.random.normal(<span style="color:#B5CEA8">0</span>, <span style="color:#B5CEA8">0.1</span>, n_customers)
)
df[<span style="color:#CE9178">'churn'</span>] = (churn_probability > <span style="color:#B5CEA8">0.5</span>).astype(<span style="color:#CE9178">int</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Dataset created: {df.shape[0]} customers, {df.shape[1]} features"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Churn rate: {df['churn'].mean():.1%}"</span>)

<span style="color:#6A9955"># Step 3: Exploratory Data Analysis</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n3Ô∏è‚É£ EXPLORATORY DATA ANALYSIS"</span>)

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\nDataset Overview:"</span>)
<span style="color:#DCDCAA">print</span>(df.info())

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"\nMissing values:"</span>)
<span style="color:#DCDCAA">print</span>(df.isnull().sum())

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"\nNumerical features summary:"</span>)
numerical_cols = [<span style="color:#CE9178">'age'</span>, <span style="color:#CE9178">'tenure_months'</span>, <span style="color:#CE9178">'monthly_charges'</span>, <span style="color:#CE9178">'total_charges'</span>]
<span style="color:#DCDCAA">print</span>(df[numerical_cols].describe())

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"\nChurn analysis by categorical features:"</span>)
categorical_cols = [<span style="color:#CE9178">'contract_type'</span>, <span style="color:#CE9178">'payment_method'</span>, <span style="color:#CE9178">'internet_service'</span>, <span style="color:#CE9178">'senior_citizen'</span>]
<span style="color:#DCDCAA">for</span> col <span style="color:#DCDCAA">in</span> categorical_cols:
    churn_rate = df.groupby(col)[<span style="color:#CE9178">'churn'</span>].mean().sort_values(ascending=<span style="color:#B5CEA8">False</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\n{col}:"</span>)
    <span style="color:#DCDCAA">for</span> category, rate <span style="color:#DCDCAA">in</span> churn_rate.items():
        <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  {str(category):20}: {rate:.1%}"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüîç Key EDA Insights:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ Month-to-month contracts have highest churn (as expected)"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ Electronic check payment method correlates with higher churn"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ Senior citizens churn more frequently"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ Higher monthly charges associated with churn"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ No missing values - clean dataset"</span>)

<span style="color:#6A9955"># Step 4: Data Preprocessing</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n4Ô∏è‚É£ DATA PREPROCESSING"</span>)

<span style="color:#6A9955"># Prepare features for modeling</span>
df_model = df.copy()

<span style="color:#6A9955"># Feature engineering</span>
df_model[<span style="color:#CE9178">'avg_monthly_charges'</span>] = df_model[<span style="color:#CE9178">'total_charges'</span>] / df_model[<span style="color:#CE9178">'tenure_months'</span>]
df_model[<span style="color:#CE9178">'charges_per_service'</span>] = df_model[<span style="color:#CE9178">'monthly_charges'</span>] / (
    (df_model[<span style="color:#CE9178">'online_security'</span>] == <span style="color:#CE9178">'Yes'</span>).astype(<span style="color:#CE9178">int</span>) +
    (df_model[<span style="color:#CE9178">'tech_support'</span>] == <span style="color:#CE9178">'Yes'</span>).astype(<span style="color:#CE9178">int</span>) +
    (df_model[<span style="color:#CE9178">'streaming_tv'</span>] == <span style="color:#CE9178">'Yes'</span>).astype(<span style="color:#CE9178">int</span>) + <span style="color:#B5CEA8">1</span>
)

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ Feature engineering completed:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  ‚Ä¢ avg_monthly_charges: total_charges / tenure_months"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"  ‚Ä¢ charges_per_service: monthly_charges / number_of_services"</span>)

<span style="color:#6A9955"># Encode categorical variables</span>
categorical_features = [<span style="color:#CE9178">'contract_type'</span>, <span style="color:#CE9178">'payment_method'</span>, <span style="color:#CE9178">'internet_service'</span>, 
                       <span style="color:#CE9178">'online_security'</span>, <span style="color:#CE9178">'tech_support'</span>, <span style="color:#CE9178">'streaming_tv'</span>, <span style="color:#CE9178">'paperless_billing'</span>]

<span style="color:#6A9955"># Use one-hot encoding for nominal features</span>
df_encoded = pd.get_dummies(df_model, columns=categorical_features, prefix=categorical_features)

<span style="color:#6A9955"># Prepare final feature matrix</span>
X = df_encoded.drop([<span style="color:#CE9178">'customer_id'</span>, <span style="color:#CE9178">'churn'</span>], axis=<span style="color:#B5CEA8">1</span>)
y = df_encoded[<span style="color:#CE9178">'churn'</span>]

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"‚úÖ Encoding completed: {len(categorical_features)} categorical ‚Üí {X.shape[1]} features"</span>)

<span style="color:#6A9955"># Train/validation/test split</span>
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=<span style="color:#B5CEA8">0.2</span>, random_state=<span style="color:#B5CEA8">42</span>, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=<span style="color:#B5CEA8">0.25</span>, random_state=<span style="color:#B5CEA8">42</span>, stratify=y_temp)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"‚úÖ Data split completed:"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  Training: {X_train.shape[0]} samples ({X_train.shape[0]/len(X):.1%})"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(X):.1%})"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X):.1%})"</span>)

<span style="color:#6A9955"># Feature scaling</span>
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ Feature scaling completed"</span>)

<span style="color:#6A9955"># Step 5: Model Selection & Training</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n5Ô∏è‚É£ MODEL SELECTION & TRAINING"</span>)

<span style="color:#6A9955"># Define candidate models</span>
models = {
    <span style="color:#CE9178">'Logistic Regression'</span>: LogisticRegression(random_state=<span style="color:#B5CEA8">42</span>, max_iter=<span style="color:#B5CEA8">1000</span>),
    <span style="color:#CE9178">'Random Forest'</span>: RandomForestClassifier(n_estimators=<span style="color:#B5CEA8">100</span>, random_state=<span style="color:#B5CEA8">42</span>),
    <span style="color:#CE9178">'SVM'</span>: SVC(random_state=<span style="color:#B5CEA8">42</span>, probability=<span style="color:#B5CEA8">True</span>)
}

<span style="color:#6A9955"># Train and evaluate models using cross-validation</span>
model_results = {}

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Training and evaluating models..."</span>)
<span style="color:#DCDCAA">for</span> name, model <span style="color:#DCDCAA">in</span> models.items():
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nü§ñ {name}:"</span>)
    
    <span style="color:#6A9955"># Use scaled data for SVM and Logistic Regression, original for Random Forest</span>
    <span style="color:#DCDCAA">if</span> name <span style="color:#DCDCAA">in</span> [<span style="color:#CE9178">'Logistic Regression'</span>, <span style="color:#CE9178">'SVM'</span>]:
        X_train_model = X_train_scaled
        X_val_model = X_val_scaled
    <span style="color:#DCDCAA">else</span>:
        X_train_model = X_train
        X_val_model = X_val
    
    <span style="color:#6A9955"># Cross-validation on training set</span>
    cv_scores = cross_val_score(model, X_train_model, y_train, cv=<span style="color:#B5CEA8">5</span>, scoring=<span style="color:#CE9178">'roc_auc'</span>)
    
    <span style="color:#6A9955"># Train on full training set and evaluate on validation</span>
    model.fit(X_train_model, y_train)
    val_predictions = model.predict(X_val_model)
    val_probabilities = model.predict_proba(X_val_model)[:, <span style="color:#B5CEA8">1</span>]
    
    val_auc = roc_auc_score(y_val, val_probabilities)
    
    model_results[name] = {
        <span style="color:#CE9178">'model'</span>: model,
        <span style="color:#CE9178">'cv_mean'</span>: cv_scores.mean(),
        <span style="color:#CE9178">'cv_std'</span>: cv_scores.std(),
        <span style="color:#CE9178">'val_auc'</span>: val_auc,
        <span style="color:#CE9178">'scaled'</span>: name <span style="color:#DCDCAA">in</span> [<span style="color:#CE9178">'Logistic Regression'</span>, <span style="color:#CE9178">'SVM'</span>]
    }
    
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  CV ROC-AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})"</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  Validation ROC-AUC: {val_auc:.3f}"</span>)

<span style="color:#6A9955"># Select best model</span>
best_model_name = max(model_results.keys(), key=<span style="color:#DCDCAA">lambda</span> k: model_results[k][<span style="color:#CE9178">'val_auc'</span>])
best_model_info = model_results[best_model_name]

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüèÜ Best Model: {best_model_name}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"   Validation ROC-AUC: {best_model_info['val_auc']:.3f}"</span>)

<span style="color:#6A9955"># Hyperparameter tuning for best model</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüîß Hyperparameter Tuning for {best_model_name}:"</span>)

<span style="color:#DCDCAA">if</span> best_model_name == <span style="color:#CE9178">'Random Forest'</span>:
    param_grid = {
        <span style="color:#CE9178">'n_estimators'</span>: [<span style="color:#B5CEA8">50</span>, <span style="color:#B5CEA8">100</span>, <span style="color:#B5CEA8">200</span>],
        <span style="color:#CE9178">'max_depth'</span>: [<span style="color:#B5CEA8">10</span>, <span style="color:#B5CEA8">20</span>, <span style="color:#B5CEA8">None</span>],
        <span style="color:#CE9178">'min_samples_split'</span>: [<span style="color:#B5CEA8">2</span>, <span style="color:#B5CEA8">5</span>, <span style="color:#B5CEA8">10</span>]
    }
    X_train_tune = X_train
<span style="color:#DCDCAA">elif</span> best_model_name == <span style="color:#CE9178">'Logistic Regression'</span>:
    param_grid = {
        <span style="color:#CE9178">'C'</span>: [<span style="color:#B5CEA8">0.1</span>, <span style="color:#B5CEA8">1</span>, <span style="color:#B5CEA8">10</span>, <span style="color:#B5CEA8">100</span>],
        <span style="color:#CE9178">'penalty'</span>: [<span style="color:#CE9178">'l1'</span>, <span style="color:#CE9178">'l2'</span>],
        <span style="color:#CE9178">'solver'</span>: [<span style="color:#CE9178">'liblinear'</span>]
    }
    X_train_tune = X_train_scaled
<span style="color:#DCDCAA">else</span>:  <span style="color:#6A9955"># SVM</span>
    param_grid = {
        <span style="color:#CE9178">'C'</span>: [<span style="color:#B5CEA8">0.1</span>, <span style="color:#B5CEA8">1</span>, <span style="color:#B5CEA8">10</span>],
        <span style="color:#CE9178">'gamma'</span>: [<span style="color:#CE9178">'scale'</span>, <span style="color:#CE9178">'auto'</span>, <span style="color:#B5CEA8">0.1</span>, <span style="color:#B5CEA8">1</span>],
        <span style="color:#CE9178">'kernel'</span>: [<span style="color:#CE9178">'rbf'</span>, <span style="color:#CE9178">'poly'</span>]
    }
    X_train_tune = X_train_scaled

<span style="color:#6A9955"># Perform grid search</span>
grid_search = GridSearchCV(
    models[best_model_name], 
    param_grid, 
    cv=<span style="color:#B5CEA8">5</span>, 
    scoring=<span style="color:#CE9178">'roc_auc'</span>,
    n_jobs=-<span style="color:#B5CEA8">1</span>
)

grid_search.fit(X_train_tune, y_train)
tuned_model = grid_search.best_estimator_

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Best parameters: {grid_search.best_params_}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Best CV score: {grid_search.best_score_:.3f}"</span>)

<span style="color:#6A9955"># Step 6: Model Evaluation</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"\n6Ô∏è‚É£ MODEL EVALUATION"</span>)

<span style="color:#6A9955"># Final evaluation on test set</span>
<span style="color:#DCDCAA">if</span> best_model_info[<span style="color:#CE9178">'scaled'</span>]:
    X_test_final = X_test_scaled
<span style="color:#DCDCAA">else</span>:
    X_test_final = X_test

test_predictions = tuned_model.predict(X_test_final)
test_probabilities = tuned_model.predict_proba(X_test_final)[:, <span style="color:#B5CEA8">1</span>]
test_auc = roc_auc_score(y_test, test_probabilities)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"üéØ Final Test Results:"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"ROC-AUC Score: {test_auc:.3f}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Business Goal (>0.85): {'‚úÖ ACHIEVED' if test_auc > 0.85 else '‚ùå NOT MET'}"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nDetailed Classification Report:"</span>)
<span style="color:#DCDCAA">print</span>(classification_report(y_test, test_predictions))

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nConfusion Matrix:"</span>)
cm = confusion_matrix(y_test, test_predictions)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"                 Predicted"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"                 No    Yes"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Actual    No   {cm[0,0]:4d}  {cm[0,1]:4d}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"          Yes  {cm[1,0]:4d}  {cm[1,1]:4d}"</span>)

<span style="color:#6A9955"># Business impact analysis</span>
false_negatives = cm[<span style="color:#B5CEA8">1</span>,<span style="color:#B5CEA8">0</span>]  <span style="color:#6A9955"># Churned customers we missed</span>
false_positives = cm[<span style="color:#B5CEA8">0</span>,<span style="color:#B5CEA8">1</span>]  <span style="color:#6A9955"># Non-churned customers we flagged</span>

avg_customer_value = df[<span style="color:#CE9178">'monthly_charges'</span>].mean() * <span style="color:#B5CEA8">12</span>  <span style="color:#6A9955"># Annual value</span>
retention_cost = <span style="color:#B5CEA8">100</span>  <span style="color:#6A9955"># Cost of retention campaign per customer</span>

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüí∞ Business Impact Analysis:"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Average annual customer value: ${avg_customer_value:.2f}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Retention campaign cost: ${retention_cost:.2f} per customer"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Customers we'd miss (False Negatives): {false_negatives}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Wasted retention efforts (False Positives): {false_positives}"</span>)

potential_revenue_saved = (cm[<span style="color:#B5CEA8">1</span>,<span style="color:#B5CEA8">1</span>]) * avg_customer_value * <span style="color:#B5CEA8">0.3</span>  <span style="color:#6A9955"># 30% retention success</span>
wasted_costs = false_positives * retention_cost
net_benefit = potential_revenue_saved - wasted_costs

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Potential revenue saved: ${potential_revenue_saved:,.2f}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Wasted retention costs: ${wasted_costs:,.2f}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Net business benefit: ${net_benefit:,.2f}"</span>)

<span style="color:#6A9955"># Feature importance (if available)</span>
<span style="color:#DCDCAA">if</span> hasattr(tuned_model, <span style="color:#CE9178">'feature_importances_'</span>):
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüìä Top 10 Most Important Features:"</span>)
    feature_importance = pd.DataFrame({
        <span style="color:#CE9178">'feature'</span>: X.columns,
        <span style="color:#CE9178">'importance'</span>: tuned_model.feature_importances_
    }).sort_values(<span style="color:#CE9178">'importance'</span>, ascending=<span style="color:#B5CEA8">False</span>)
    
    <span style="color:#DCDCAA">for</span> i, (_, row) <span style="color:#DCDCAA">in</span> enumerate(feature_importance.head(<span style="color:#B5CEA8">10</span>).iterrows()):
        <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  {i+1:2d}. {row['feature']:30}: {row['importance']:.3f}"</span>)

<span style="color:#6A9955"># Step 7: Deployment Considerations</span>
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">\n7Ô∏è‚É£ DEPLOYMENT & MONITORING"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üöÄ Deployment Checklist:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ Model performance meets business requirements"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ Feature engineering pipeline documented"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ Scaling parameters saved for production"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ Model serialization completed"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüìä Monitoring Plan:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚Ä¢ Track model performance monthly (ROC-AUC)"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚Ä¢ Monitor feature drift in customer behavior"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚Ä¢ Set up alerts for performance degradation"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚Ä¢ Plan model retraining every 6 months"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚Ä¢ A/B test model updates before full deployment"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüéØ Success Metrics for Production:"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"‚Ä¢ Maintain ROC-AUC > 0.80 in production"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"‚Ä¢ Achieve 15-20% reduction in churn rate"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"‚Ä¢ Positive ROI on retention campaigns"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"‚Ä¢ Model predictions available within 100ms"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüí° Next Steps:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"1. Deploy model to staging environment"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"2. Integrate with customer retention system"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"3. Train customer service team on model outputs"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"4. Set up automated model monitoring"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"5. Plan A/B test for retention campaign effectiveness"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\n" + "="*60</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üéâ COMPLETE ML WORKFLOW SUCCESSFULLY EXECUTED!"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"   From business problem to production-ready solution"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"="*60</span>)
</code></pre>           
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üéØ</span>Hands-On Projects: Apply Your Knowledge</h2>
            
            <div class="highlight-box info">
                <h4>üéì Learning by Doing</h4>
                <p>The best way to master ML is through hands-on practice. These projects progress from beginner to intermediate, covering all major ML concepts with real-world datasets.</p>
            </div>

            <div class="grid">
                <div class="card">
                    <h3>üå∏ Project 1: Iris Classification (Beginner)</h3>
                    <p><strong>Goal:</strong> Classify iris flowers into species</p>
                    <p><strong>Dataset:</strong> Classic iris dataset (150 samples, 4 features, 3 classes)</p>
                    <p><strong>Skills:</strong> Basic ML pipeline, visualization, model evaluation</p>
                    
                    <div class="code-block">
                    <pre><code class="language-python">
<span style="color:#6A9955"># Project 1: Complete Iris Classification</span>
<span style="color:#DCDCAA">from</span> sklearn.datasets <span style="color:#DCDCAA">import</span> load_iris
<span style="color:#DCDCAA">from</span> sklearn.model_selection <span style="color:#DCDCAA">import</span> train_test_split, cross_val_score
<span style="color:#DCDCAA">from</span> sklearn.preprocessing <span style="color:#DCDCAA">import</span> StandardScaler
<span style="color:#DCDCAA">from</span> sklearn.linear_model <span style="color:#DCDCAA">import</span> LogisticRegression
<span style="color:#DCDCAA">from</span> sklearn.ensemble <span style="color:#DCDCAA">import</span> RandomForestClassifier
<span style="color:#DCDCAA">from</span> sklearn.svm <span style="color:#DCDCAA">import</span> SVC
<span style="color:#DCDCAA">from</span> sklearn.metrics <span style="color:#DCDCAA">import</span> classification_report, confusion_matrix
<span style="color:#DCDCAA">import</span> pandas <span style="color:#DCDCAA">as</span> pd
<span style="color:#DCDCAA">import</span> numpy <span style="color:#DCDCAA">as</span> np

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üå∏ Project 1: Iris Species Classification"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"="*50</span>)

<span style="color:#6A9955"># Load the dataset</span>
iris = load_iris()
X, y = iris.data, iris.target

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Dataset: {X.shape[0]} samples, {X.shape[1]} features"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Classes: {iris.target_names}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Features: {iris.feature_names}"</span>)

<span style="color:#6A9955"># Create DataFrame for easier analysis</span>
df = pd.DataFrame(X, columns=iris.feature_names)
df[<span style="color:#CE9178">'species'</span>] = iris.target_names[y]

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nDataset overview:"</span>)
<span style="color:#DCDCAA">print</span>(df.head())

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nClass distribution:"</span>)
<span style="color:#DCDCAA">print</span>(df[<span style="color:#CE9178">'species'</span>].value_counts())

<span style="color:#6A9955"># Exploratory analysis</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nFeature statistics by species:"</span>)
<span style="color:#DCDCAA">print</span>(df.groupby(<span style="color:#CE9178">'species'</span>).mean().round(<span style="color:#B5CEA8">2</span>))

<span style="color:#6A9955"># Split data</span>
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=<span style="color:#B5CEA8">0.3</span>, random_state=<span style="color:#B5CEA8">42</span>, stratify=y
)

<span style="color:#6A9955"># Scale features</span>
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

<span style="color:#6A9955"># Try multiple models</span>
models = {
    <span style="color:#CE9178">'Logistic Regression'</span>: LogisticRegression(random_state=<span style="color:#B5CEA8">42</span>),
    <span style="color:#CE9178">'Random Forest'</span>: RandomForestClassifier(n_estimators=<span style="color:#B5CEA8">100</span>, random_state=<span style="color:#B5CEA8">42</span>),
    <span style="color:#CE9178">'SVM'</span>: SVC(random_state=<span style="color:#B5CEA8">42</span>, kernel=<span style="color:#CE9178">'rbf'</span>)
}

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nModel Comparison (5-fold CV):"</span>)
best_score = <span style="color:#B5CEA8">0</span>
best_model = <span style="color:#B5CEA8">None</span>

<span style="color:#DCDCAA">for</span> name, model <span style="color:#DCDCAA">in</span> models.items():
    <span style="color:#6A9955"># Use scaled data for LR and SVM, original for RF</span>
    X_cv = X_train_scaled <span style="color:#DCDCAA">if</span> name != <span style="color:#CE9178">'Random Forest'</span> <span style="color:#DCDCAA">else</span> X_train
    scores = cross_val_score(model, X_cv, y_train, cv=<span style="color:#B5CEA8">5</span>)
    
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">{name:20}: {scores.mean():.3f} (+/- {scores.std()*2:.3f})"</span>)
    
    <span style="color:#DCDCAA">if</span> scores.mean() > best_score:
        best_score = scores.mean()
        best_model = (name, model)

<span style="color:#6A9955"># Train best model and evaluate</span>
model_name, model = best_model
X_train_final = X_train_scaled <span style="color:#DCDCAA">if</span> model_name != <span style="color:#CE9178">'Random Forest'</span> <span style="color:#DCDCAA">else</span> X_train
X_test_final = X_test_scaled <span style="color:#DCDCAA">if</span> model_name != <span style="color:#CE9178">'Random Forest'</span> <span style="color:#DCDCAA">else</span> X_test

model.fit(X_train_final, y_train)
predictions = model.predict(X_test_final)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüèÜ Best Model: {model_name}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Test Accuracy: {model.score(X_test_final, y_test):.3f}"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nClassification Report:"</span>)
<span style="color:#DCDCAA">print</span>(classification_report(y_test, predictions, target_names=iris.target_names))

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\n‚úÖ Project 1 Complete!"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Skills learned: Data loading, EDA, model comparison, evaluation"</span>)
                    </code></pre>
                    </div>
                </div>

                <div class="card">
                    <h3>üè† Project 2: House Price Prediction (Intermediate)</h3>
                    <p><strong>Goal:</strong> Predict house prices based on features</p>
                    <p><strong>Dataset:</strong> Synthetic housing data with realistic features</p>
                    <p><strong>Skills:</strong> Regression, feature engineering, advanced preprocessing</p>

                    <div class="code-block">
                    <pre><code class="language-python">
<span style="color:#6A9955"># Project 2: House Price Prediction</span>
<span style="color:#DCDCAA">from</span> sklearn.model_selection <span style="color:#DCDCAA">import</span> train_test_split
<span style="color:#DCDCAA">from</span> sklearn.preprocessing <span style="color:#DCDCAA">import</span> StandardScaler, PolynomialFeatures
<span style="color:#DCDCAA">from</span> sklearn.linear_model <span style="color:#DCDCAA">import</span> LinearRegression, Ridge, Lasso
<span style="color:#DCDCAA">from</span> sklearn.ensemble <span style="color:#DCDCAA">import</span> RandomForestRegressor
<span style="color:#DCDCAA">from</span> sklearn.metrics <span style="color:#DCDCAA">import</span> mean_squared_error, r2_score, mean_absolute_error
<span style="color:#DCDCAA">import</span> pandas <span style="color:#DCDCAA">as</span> pd
<span style="color:#DCDCAA">import</span> numpy <span style="color:#DCDCAA">as</span> np

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üè† Project 2: House Price Prediction"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"="*50</span>)

<span style="color:#6A9955"># Create realistic housing dataset</span>
np.random.seed(<span style="color:#B5CEA8">42</span>)
n_houses = <span style="color:#B5CEA8">2000</span>

<span style="color:#6A9955"># Generate features with realistic correlations</span>
square_footage = np.random.normal(<span style="color:#B5CEA8">2000</span>, <span style="color:#B5CEA8">500</span>, n_houses).clip(<span style="color:#B5CEA8">800</span>, <span style="color:#B5CEA8">5000</span>)
bedrooms = np.random.poisson(<span style="color:#B5CEA8">3</span>, n_houses).clip(<span style="color:#B5CEA8">1</span>, <span style="color:#B5CEA8">6</span>)
bathrooms = bedrooms + np.random.normal(<span style="color:#B5CEA8">0</span>, <span style="color:#B5CEA8">0.5</span>, n_houses).clip(-<span style="color:#B5CEA8">1</span>, <span style="color:#B5CEA8">3</span>)
age = np.random.exponential(<span style="color:#B5CEA8">15</span>, n_houses).clip(<span style="color:#B5CEA8">0</span>, <span style="color:#B5CEA8">100</span>)
garage_size = np.random.choice([<span style="color:#B5CEA8">0</span>, <span style="color:#B5CEA8">1</span>, <span style="color:#B5CEA8">2</span>, <span style="color:#B5CEA8">3</span>], n_houses, p=[<span style="color:#B5CEA8">0.1</span>, <span style="color:#B5CEA8">0.3</span>, <span style="color:#B5CEA8">0.5</span>, <span style="color:#B5CEA8">0.1</span>])
neighborhood_quality = np.random.choice([<span style="color:#B5CEA8">1</span>, <span style="color:#B5CEA8">2</span>, <span style="color:#B5CEA8">3</span>, <span style="color:#B5CEA8">4</span>, <span style="color:#B5CEA8">5</span>], n_houses, p=[<span style="color:#B5CEA8">0.1</span>, <span style="color:#B5CEA8">0.2</span>, <span style="color:#B5CEA8">0.4</span>, <span style="color:#B5CEA8">0.2</span>, <span style="color:#B5CEA8">0.1</span>])

<span style="color:#6A9955"># Create price with realistic relationships</span>
base_price = (
    square_footage * <span style="color:#B5CEA8">120</span> +  <span style="color:#6A9955"># $120 per sq ft</span>
    bedrooms * <span style="color:#B5CEA8">15000</span> +      <span style="color:#6A9955"># $15k per bedroom</span>
    bathrooms * <span style="color:#B5CEA8">8000</span> +      <span style="color:#6A9955"># $8k per bathroom</span>
    garage_size * <span style="color:#B5CEA8">12000</span> +   <span style="color:#6A9955"># $12k per garage space</span>
    neighborhood_quality * <span style="color:#B5CEA8">25000</span>  <span style="color:#6A9955"># $25k per quality level</span>
)

<span style="color:#6A9955"># Add age effect (depreciation)</span>
age_effect = np.where(age < <span style="color:#B5CEA8">10</span>, <span style="color:#B5CEA8">1.1</span>, <span style="color:#B5CEA8">1.0</span> - (age - <span style="color:#B5CEA8">10</span>) * <span style="color:#B5CEA8">0.005</span>)
base_price *= age_effect

<span style="color:#6A9955"># Add noise and market factors</span>
market_factor = np.random.normal(<span style="color:#B5CEA8">1.0</span>, <span style="color:#B5CEA8">0.1</span>, n_houses).clip(<span style="color:#B5CEA8">0.7</span>, <span style="color:#B5CEA8">1.3</span>)
price = base_price * market_factor + np.random.normal(<span style="color:#B5CEA8">0</span>, <span style="color:#B5CEA8">10000</span>, n_houses)
price = price.clip(<span style="color:#B5CEA8">50000</span>, <span style="color:#B5CEA8">1000000</span>)

<span style="color:#6A9955"># Create DataFrame</span>
df = pd.DataFrame({
    <span style="color:#CE9178">'square_footage'</span>: square_footage,
    <span style="color:#CE9178">'bedrooms'</span>: bedrooms,
    <span style="color:#CE9178">'bathrooms'</span>: bathrooms,
    <span style="color:#CE9178">'age'</span>: age,
    <span style="color:#CE9178">'garage_size'</span>: garage_size,
    <span style="color:#CE9178">'neighborhood_quality'</span>: neighborhood_quality,
    <span style="color:#CE9178">'price'</span>: price
})

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Dataset: {df.shape[0]} houses, {df.shape[1]-1} features"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nDataset overview:"</span>)
<span style="color:#DCDCAA">print</span>(df.describe().round(<span style="color:#B5CEA8">2</span>))

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nPrice distribution:"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Mean: ${df['price'].mean():,.2f}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Median: ${df['price'].median():,.2f}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Range: ${df['price'].min():,.2f} - ${df['price'].max():,.2f}"</span>)

<span style="color:#6A9955"># Feature engineering</span>
df[<span style="color:#CE9178">'price_per_sqft'</span>] = df[<span style="color:#CE9178">'price'</span>] / df[<span style="color:#CE9178">'square_footage'</span>]
df[<span style="color:#CE9178">'total_rooms'</span>] = df[<span style="color:#CE9178">'bedrooms'</span>] + df[<span style="color:#CE9178">'bathrooms'</span>]
df[<span style="color:#CE9178">'bath_bed_ratio'</span>] = df[<span style="color:#CE9178">'bathrooms'</span>] / df[<span style="color:#CE9178">'bedrooms'</span>]
df[<span style="color:#CE9178">'is_new'</span>] = (df[<span style="color:#CE9178">'age'</span>] < <span style="color:#B5CEA8">5</span>).astype(<span style="color:#CE9178">int</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüîß Feature Engineering:"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ price_per_sqft: price / square_footage"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ total_rooms: bedrooms + bathrooms"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ bath_bed_ratio: bathrooms / bedrooms"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"‚úÖ is_new: age < 5 years"</span>)

<span style="color:#6A9955"># Prepare features</span>
feature_cols = [<span style="color:#CE9178">'square_footage'</span>, <span style="color:#CE9178">'bedrooms'</span>, <span style="color:#CE9178">'bathrooms'</span>, <span style="color:#CE9178">'age'</span>, 
                <span style="color:#CE9178">'garage_size'</span>, <span style="color:#CE9178">'neighborhood_quality'</span>, <span style="color:#CE9178">'total_rooms'</span>, 
                <span style="color:#CE9178">'bath_bed_ratio'</span>, <span style="color:#CE9178">'is_new'</span>]
X = df[feature_cols]
y = df[<span style="color:#CE9178">'price'</span>]

<span style="color:#6A9955"># Split data</span>
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=<span style="color:#B5CEA8">0.2</span>, random_state=<span style="color:#B5CEA8">42</span>
)

<span style="color:#6A9955"># Scale features</span>
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

<span style="color:#6A9955"># Model comparison</span>
models = {
    <span style="color:#CE9178">'Linear Regression'</span>: LinearRegression(),
    <span style="color:#CE9178">'Ridge Regression'</span>: Ridge(alpha=<span style="color:#B5CEA8">1.0</span>),
    <span style="color:#CE9178">'Lasso Regression'</span>: Lasso(alpha=<span style="color:#B5CEA8">1.0</span>),
    <span style="color:#CE9178">'Random Forest'</span>: RandomForestRegressor(n_estimators=<span style="color:#B5CEA8">100</span>, random_state=<span style="color:#B5CEA8">42</span>)
}

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nü§ñ Model Comparison:"</span>)
results = {}

<span style="color:#DCDCAA">for</span> name, model <span style="color:#DCDCAA">in</span> models.items():
    <span style="color:#6A9955"># Use scaled data for linear models, original for RF</span>
    X_train_model = X_train_scaled <span style="color:#DCDCAA">if</span> <span style="color:#CE9178">'Regression'</span> <span style="color:#DCDCAA">in</span> name <span style="color:#DCDCAA">else</span> X_train
    X_test_model = X_test_scaled <span style="color:#DCDCAA">if</span> <span style="color:#CE9178">'Regression'</span> <span style="color:#DCDCAA">in</span> name <span style="color:#DCDCAA">else</span> X_test
    
    <span style="color:#6A9955"># Train model</span>
    model.fit(X_train_model, y_train)
    predictions = model.predict(X_test_model)
    
    <span style="color:#6A9955"># Calculate metrics</span>
    mse = mean_squared_error(y_test, predictions)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)
    
    results[name] = {<span style="color:#CE9178">'RMSE'</span>: rmse, <span style="color:#CE9178">'MAE'</span>: mae, <span style="color:#CE9178">'R¬≤'</span>: r2}
    
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\n{name}:"</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  RMSE: ${rmse:,.2f}"</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  MAE:  ${mae:,.2f}"</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  R¬≤:   {r2:.3f}"</span>)

<span style="color:#6A9955"># Best model analysis</span>
best_model = max(results.keys(), key=<span style="color:#DCDCAA">lambda</span> k: results[k][<span style="color:#CE9178">'R¬≤'</span>])
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüèÜ Best Model: {best_model}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  R¬≤ Score: {results[best_model]['R¬≤']:.3f}"</span>)

<span style="color:#6A9955"># Feature importance (for Random Forest)</span>
<span style="color:#DCDCAA">if</span> best_model == <span style="color:#CE9178">'Random Forest'</span>:
    rf_model = RandomForestRegressor(n_estimators=<span style="color:#B5CEA8">100</span>, random_state=<span style="color:#B5CEA8">42</span>)
    rf_model.fit(X_train, y_train)
    
    feature_importance = pd.DataFrame({
        <span style="color:#CE9178">'feature'</span>: feature_cols,
        <span style="color:#CE9178">'importance'</span>: rf_model.feature_importances_
    }).sort_values(<span style="color:#CE9178">'importance'</span>, ascending=<span style="color:#B5CEA8">False</span>)
    
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüìä Feature Importance:"</span>)
    <span style="color:#DCDCAA">for</span> _, row <span style="color:#DCDCAA">in</span> feature_importance.iterrows():
        <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  {row['feature']:20}: {row['importance']:.3f}"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\n‚úÖ Project 2 Complete!"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Skills learned: Regression, feature engineering, model evaluation"</span>)
                    </code></pre>           
                    </div>
                </div>

                <div class="card">
                    <h3>üõçÔ∏è Project 3: Customer Segmentation (Advanced)</h3>
                    <p><strong>Goal:</strong> Segment customers using unsupervised learning</p>
                    <p><strong>Dataset:</strong> Customer transaction and behavior data</p>
                    <p><strong>Skills:</strong> Clustering, dimensionality reduction, business insights</p>

                    <div class="code-block">
                    <pre><code class="language-python">
<span style="color:#6A9955"># Project 3: Customer Segmentation Analysis</span>
<span style="color:#DCDCAA">from</span> sklearn.cluster <span style="color:#DCDCAA">import</span> KMeans
<span style="color:#DCDCAA">from</span> sklearn.preprocessing <span style="color:#DCDCAA">import</span> StandardScaler
<span style="color:#DCDCAA">from</span> sklearn.decomposition <span style="color:#DCDCAA">import</span> PCA
<span style="color:#DCDCAA">from</span> sklearn.metrics <span style="color:#DCDCAA">import</span> silhouette_score
<span style="color:#DCDCAA">import</span> pandas <span style="color:#DCDCAA">as</span> pd
<span style="color:#DCDCAA">import</span> numpy <span style="color:#DCDCAA">as</span> np
<span style="color:#DCDCAA">import</span> matplotlib.pyplot <span style="color:#DCDCAA">as</span> plt

<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"üõçÔ∏è Project 3: Customer Segmentation Analysis"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"="*50</span>)

<span style="color:#6A9955"># Generate realistic customer data</span>
np.random.seed(<span style="color:#B5CEA8">42</span>)
n_customers = <span style="color:#B5CEA8">2000</span>

<span style="color:#6A9955"># Create customer segments with different behaviors</span>
segment_sizes = [<span style="color:#B5CEA8">600</span>, <span style="color:#B5CEA8">800</span>, <span style="color:#B5CEA8">400</span>, <span style="color:#B5CEA8">200</span>]  <span style="color:#6A9955"># VIP, Regular, Budget, Inactive</span>
segments = []

<span style="color:#DCDCAA">for</span> i, size <span style="color:#DCDCAA">in</span> enumerate(segment_sizes):
    <span style="color:#DCDCAA">if</span> i == <span style="color:#B5CEA8">0</span>:  <span style="color:#6A9955"># VIP customers</span>
        annual_spending = np.random.normal(<span style="color:#B5CEA8">5000</span>, <span style="color:#B5CEA8">1000</span>, size).clip(<span style="color:#B5CEA8">3000</span>, <span style="color:#B5CEA8">8000</span>)
        visit_frequency = np.random.normal(<span style="color:#B5CEA8">15</span>, <span style="color:#B5CEA8">3</span>, size).clip(<span style="color:#B5CEA8">10</span>, <span style="color:#B5CEA8">25</span>)
        avg_purchase = np.random.normal(<span style="color:#B5CEA8">200</span>, <span style="color:#B5CEA8">50</span>, size).clip(<span style="color:#B5CEA8">100</span>, <span style="color:#B5CEA8">400</span>)
    <span style="color:#DCDCAA">elif</span> i == <span style="color:#B5CEA8">1</span>:  <span style="color:#6A9955"># Regular customers</span>
        annual_spending = np.random.normal(<span style="color:#B5CEA8">2000</span>, <span style="color:#B5CEA8">500</span>, size).clip(<span style="color:#B5CEA8">1000</span>, <span style="color:#B5CEA8">3500</span>)
        visit_frequency = np.random.normal(<span style="color:#B5CEA8">8</span>, <span style="color:#B5CEA8">2</span>, size).clip(<span style="color:#B5CEA8">4</span>, <span style="color:#B5CEA8">15</span>)
        avg_purchase = np.random.normal(<span style="color:#B5CEA8">100</span>, <span style="color:#B5CEA8">30</span>, size).clip(<span style="color:#B5CEA8">50</span>, <span style="color:#B5CEA8">200</span>)
    <span style="color:#DCDCAA">elif</span> i == <span style="color:#B5CEA8">2</span>:  <span style="color:#6A9955"># Budget customers</span>
        annual_spending = np.random.normal(<span style="color:#B5CEA8">800</span>, <span style="color:#B5CEA8">200</span>, size).clip(<span style="color:#B5CEA8">400</span>, <span style="color:#B5CEA8">1300</span>)
        visit_frequency = np.random.normal(<span style="color:#B5CEA8">12</span>, <span style="color:#B5CEA8">3</span>, size).clip(<span style="color:#B5CEA8">6</span>, <span style="color:#B5CEA8">20</span>)
        avg_purchase = np.random.normal(<span style="color:#B5CEA8">50</span>, <span style="color:#B5CEA8">15</span>, size).clip(<span style="color:#B5CEA8">20</span>, <span style="color:#B5CEA8">100</span>)
    <span style="color:#DCDCAA">else</span>:  <span style="color:#6A9955"># Inactive customers</span>
        annual_spending = np.random.normal(<span style="color:#B5CEA8">300</span>, <span style="color:#B5CEA8">100</span>, size).clip(<span style="color:#B5CEA8">100</span>, <span style="color:#B5CEA8">600</span>)
        visit_frequency = np.random.normal(<span style="color:#B5CEA8">2</span>, <span style="color:#B5CEA8">1</span>, size).clip(<span style="color:#B5CEA8">1</span>, <span style="color:#B5CEA8">5</span>)
        avg_purchase = np.random.normal(<span style="color:#B5CEA8">80</span>, <span style="color:#B5CEA8">20</span>, size).clip(<span style="color:#B5CEA8">40</span>, <span style="color:#B5CEA8">150</span>)
    
    segments.append({
        <span style="color:#CE9178">'annual_spending'</span>: annual_spending,
        <span style="color:#CE9178">'visit_frequency'</span>: visit_frequency,
        <span style="color:#CE9178">'avg_purchase'</span>: avg_purchase,
        <span style="color:#CE9178">'true_segment'</span>: [f<span style="color:#CE9178">'Segment_{i+1}'</span>] * size
    })

<span style="color:#6A9955"># Combine all segments</span>
data = {
    <span style="color:#CE9178">'customer_id'</span>: <span style="color:#B5CEA8">list</span>(<span style="color:#B5CEA8">range</span>(<span style="color:#B5CEA8">1</span>, <span style="color:#B5CEA8">sum</span>(segment_sizes) + <span style="color:#B5CEA8">1</span>)),
    <span style="color:#CE9178">'annual_spending'</span>: np.concatenate([s[<span style="color:#CE9178">'annual_spending'</span>] <span style="color:#DCDCAA">for</span> s <span style="color:#DCDCAA">in</span> segments]),
    <span style="color:#CE9178">'visit_frequency'</span>: np.concatenate([s[<span style="color:#CE9178">'visit_frequency'</span>] <span style="color:#DCDCAA">for</span> s <span style="color:#DCDCAA">in</span> segments]),
    <span style="color:#CE9178">'avg_purchase'</span>: np.concatenate([s[<span style="color:#CE9178">'avg_purchase'</span>] <span style="color:#DCDCAA">for</span> s <span style="color:#DCDCAA">in</span> segments]),
    <span style="color:#CE9178">'true_segment'</span>: np.concatenate([s[<span style="color:#CE9178">'true_segment'</span>] <span style="color:#DCDCAA">for</span> s <span style="color:#DCDCAA">in</span> segments])
}

<span style="color:#6A9955"># Add additional realistic features</span>
data[<span style="color:#CE9178">'days_since_last_purchase'</span>] = np.random.exponential(<span style="color:#B5CEA8">30</span>, <span style="color:#B5CEA8">sum</span>(segment_sizes)).clip(<span style="color:#B5CEA8">1</span>, <span style="color:#B5CEA8">365</span>)
data[<span style="color:#CE9178">'total_purchases'</span>] = data[<span style="color:#CE9178">'annual_spending'</span>] / np.array(data[<span style="color:#CE9178">'avg_purchase'</span>])
data[<span style="color:#CE9178">'customer_lifetime'</span>] = np.random.exponential(<span style="color:#B5CEA8">2</span>, <span style="color:#B5CEA8">sum</span>(segment_sizes)).clip(<span style="color:#B5CEA8">0.5</span>, <span style="color:#B5CEA8">8</span>)  <span style="color:#6A9955"># years</span>
data[<span style="color:#CE9178">'satisfaction_score'</span>] = np.random.normal(<span style="color:#B5CEA8">3.5</span>, <span style="color:#B5CEA8">0.8</span>, <span style="color:#B5CEA8">sum</span>(segment_sizes)).clip(<span style="color:#B5CEA8">1</span>, <span style="color:#B5CEA8">5</span>)

df = pd.DataFrame(data)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Dataset: {df.shape[0]} customers, {df.shape[1]-2} features"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nDataset overview:"</span>)
feature_cols = [<span style="color:#CE9178">'annual_spending'</span>, <span style="color:#CE9178">'visit_frequency'</span>, <span style="color:#CE9178">'avg_purchase'</span>, 
                <span style="color:#CE9178">'days_since_last_purchase'</span>, <span style="color:#CE9178">'total_purchases'</span>, 
                <span style="color:#CE9178">'customer_lifetime'</span>, <span style="color:#CE9178">'satisfaction_score'</span>]
<span style="color:#DCDCAA">print</span>(df[feature_cols].describe().round(<span style="color:#B5CEA8">2</span>))

<span style="color:#6A9955"># Prepare data for clustering</span>
X = df[feature_cols]

<span style="color:#6A9955"># Scale features (important for clustering)</span>
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüîç Finding Optimal Number of Clusters:"</span>)

<span style="color:#6A9955"># Elbow method and silhouette analysis</span>
inertias = []
silhouette_scores = []
k_range = <span style="color:#B5CEA8">range</span>(<span style="color:#B5CEA8">2</span>, <span style="color:#B5CEA8">11</span>)

<span style="color:#DCDCAA">for</span> k <span style="color:#DCDCAA">in</span> k_range:
    kmeans = KMeans(n_clusters=k, random_state=<span style="color:#B5CEA8">42</span>, n_init=<span style="color:#B5CEA8">10</span>)
    cluster_labels = kmeans.fit_predict(X_scaled)
    
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, cluster_labels))
    
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"k={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette_scores[-1]:.3f}"</span>)

<span style="color:#6A9955"># Find optimal k (highest silhouette score)</span>
optimal_k = k_range[np.argmax(silhouette_scores)]
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüéØ Optimal number of clusters: {optimal_k}"</span>)

<span style="color:#6A9955"># Apply K-means with optimal k</span>
kmeans_final = KMeans(n_clusters=optimal_k, random_state=<span style="color:#B5CEA8">42</span>, n_init=<span style="color:#B5CEA8">10</span>)
cluster_labels = kmeans_final.fit_predict(X_scaled)

df[<span style="color:#CE9178">'predicted_cluster'</span>] = cluster_labels

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüìä Cluster Analysis:"</span>)
<span style="color:#DCDCAA">for</span> cluster <span style="color:#DCDCAA">in</span> <span style="color:#B5CEA8">range</span>(optimal_k):
    cluster_data = df[df[<span style="color:#CE9178">'predicted_cluster'</span>] == cluster]
    size = <span style="color:#B5CEA8">len</span>(cluster_data)
    
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nCluster {cluster + 1} ({size} customers, {size/len(df)*100:.1f}%):"</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  Annual Spending: ${cluster_data['annual_spending'].mean():,.2f}"</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  Visit Frequency: {cluster_data['visit_frequency'].mean():.1f} visits/year"</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  Avg Purchase: ${cluster_data['avg_purchase'].mean():.2f}"</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  Customer Lifetime: {cluster_data['customer_lifetime'].mean():.1f} years"</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  Satisfaction: {cluster_data['satisfaction_score'].mean():.2f}/5"</span>)

<span style="color:#6A9955"># Business insights and recommendations</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüéØ Business Insights & Recommendations:"</span>)

<span style="color:#6A9955"># Identify cluster characteristics</span>
<span style="color:#DCDCAA">for</span> cluster <span style="color:#DCDCAA">in</span> <span style="color:#B5CEA8">range</span>(optimal_k):
    cluster_data = df[df[<span style="color:#CE9178">'predicted_cluster'</span>] == cluster]
    avg_spending = cluster_data[<span style="color:#CE9178">'annual_spending'</span>].mean()
    avg_frequency = cluster_data[<span style="color:#CE9178">'visit_frequency'</span>].mean()
    avg_satisfaction = cluster_data[<span style="color:#CE9178">'satisfaction_score'</span>].mean()
    
    <span style="color:#DCDCAA">if</span> avg_spending > <span style="color:#B5CEA8">3000</span> <span style="color:#DCDCAA">and</span> avg_frequency > <span style="color:#B5CEA8">10</span>:
        cluster_type = <span style="color:#CE9178">"üíé VIP Customers"</span>
        recommendation = <span style="color:#CE9178">"Exclusive perks, premium service, loyalty rewards"</span>
    <span style="color:#DCDCAA">elif</span> avg_spending > <span style="color:#B5CEA8">1500</span> <span style="color:#DCDCAA">and</span> avg_frequency > <span style="color:#B5CEA8">6</span>:
        cluster_type = <span style="color:#CE9178">"üéØ Regular Customers"</span>
        recommendation = <span style="color:#CE9178">"Upselling campaigns, personalized offers"</span>
    <span style="color:#DCDCAA">elif</span> avg_frequency > <span style="color:#B5CEA8">8</span> <span style="color:#DCDCAA">and</span> avg_spending < <span style="color:#B5CEA8">1500</span>:
        cluster_type = <span style="color:#CE9178">"üõí Budget-Conscious"</span>
        recommendation = <span style="color:#CE9178">"Discount programs, value bundles"</span>
    <span style="color:#DCDCAA">else</span>:
        cluster_type = <span style="color:#CE9178">"üò¥ At-Risk/Inactive"</span>
        recommendation = <span style="color:#CE9178">"Re-engagement campaigns, win-back offers"</span>
    
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nCluster {cluster + 1}: {cluster_type}"</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  Strategy: {recommendation}"</span>)
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  Revenue potential: ${avg_spending * len(cluster_data):,.2f}"</span>)

<span style="color:#6A9955"># Dimensionality reduction for visualization</span>
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüìà 2D Visualization using PCA:"</span>)
pca = PCA(n_components=<span style="color:#B5CEA8">2</span>)
X_pca = pca.fit_transform(X_scaled)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"PCA Explained Variance Ratio:"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  PC1: {pca.explained_variance_ratio_[0]:.3f}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  PC2: {pca.explained_variance_ratio_[1]:.3f}"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"  Total: {sum(pca.explained_variance_ratio_):.3f}"</span>)

<span style="color:#6A9955"># Calculate business metrics</span>
total_revenue = df[<span style="color:#CE9178">'annual_spending'</span>].sum()
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\nüí∞ Business Impact:"</span>)
<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Total annual revenue: ${total_revenue:,.2f}"</span>)

<span style="color:#DCDCAA">for</span> cluster <span style="color:#DCDCAA">in</span> <span style="color:#B5CEA8">range</span>(optimal_k):
    cluster_data = df[df[<span style="color:#CE9178">'predicted_cluster'</span>] == cluster]
    cluster_revenue = cluster_data[<span style="color:#CE9178">'annual_spending'</span>].sum()
    cluster_share = cluster_revenue / total_revenue
    
    <span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">"Cluster {cluster + 1}: ${cluster_revenue:,.2f} ({cluster_share:.1%})"</span>)

<span style="color:#DCDCAA">print</span>(f<span style="color:#CE9178">\n‚úÖ Project 3 Complete!"</span>)
<span style="color:#DCDCAA">print</span>(<span style="color:#CE9178">"Skills learned: Clustering, PCA, business insights, customer analysis"</span>)
                    </code></pre>       
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üéì</span>Key Takeaways & Next Steps</h2>
            
            <div class="card">
                <h3>üß† Core Concepts Mastered</h3>
                <div class="grid">
                    <div class="highlight-box tip">
                        <h4>‚úÖ Supervised Learning</h4>
                        <ul>
                            <li>Regression vs Classification understanding</li>
                            <li>Training with labeled examples</li>
                            <li>Performance evaluation metrics</li>
                            <li>Real-world business applications</li>
                        </ul>
                    </div>
                    
                    <div class="highlight-box info">
                        <h4>‚úÖ Unsupervised Learning</h4>
                        <ul>
                            <li>Pattern discovery without labels</li>
                            <li>Clustering for customer segmentation</li>
                            <li>Dimensionality reduction techniques</li>
                            <li>Business insight generation</li>
                        </ul>
                    </div>
                    
                    <div class="highlight-box warning">
                        <h4>‚úÖ Reinforcement Learning</h4>
                        <ul>
                            <li>Learning through trial and error</li>
                            <li>Agent-environment interaction</li>
                            <li>Reward-based optimization</li>
                            <li>Advanced applications overview</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="card">
                <h3>üõ†Ô∏è Technical Skills Acquired</h3>
                <div class="grid">
                    <div style="background: #f8f9fa; padding: 20px; border-radius: 8px;">
                        <h4>üêç Python ML Stack</h4>
                        <ul>
                            <li>NumPy for numerical computing</li>
                            <li>Pandas for data manipulation</li>
                            <li>Scikit-learn for ML algorithms</li>
                            <li>Matplotlib/Seaborn for visualization</li>
                        </ul>
                    </div>
                    
                    <div style="background: #e8f5e8; padding: 20px; border-radius: 8px;">
                        <h4>üßπ Data Preprocessing</h4>
                        <ul>
                            <li>Missing value handling strategies</li>
                            <li>Feature scaling and normalization</li>
                            <li>Categorical encoding techniques</li>
                            <li>Feature engineering principles</li>
                        </ul>
                    </div>
                    
                    <div style="background: #fff3e0; padding: 20px; border-radius: 8px;">
                        <h4>üîÑ ML Workflow</h4>
                        <ul>
                            <li>Problem definition and scoping</li>
                            <li>Data exploration and analysis</li>
                            <li>Model selection and training</li>
                            <li>Evaluation and deployment</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="card">
                <h3>üöÄ Your ML Journey Roadmap</h3>
                
                <div class="flowchart">
                    <div class="flow-step" style="background: linear-gradient(135deg, #e8f5e8 0%, #c8e6c9 100%);">
                        <div class="step-number">‚úÖ</div>
                        <div>
                            <h4>Chapter 1: Foundations Complete</h4>
                            <p>You now understand ML types, Python tools, and preprocessing</p>
                        </div>
                    </div>
                    
                    <div class="flow-step" style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);">
                        <div class="step-number">2</div>
                        <div>
                            <h4>Next: Chapter 2 - Deep Dive into Regression</h4>
                            <p>Linear regression, polynomial features, regularization techniques</p>
                        </div>
                    </div>
                    
                    <div class="flow-step" style="background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);">
                        <div class="step-number">3</div>
                        <div>
                            <h4>Chapter 3 - Classification Mastery</h4>
                            <p>Logistic regression, decision trees, ensemble methods</p>
                        </div>
                    </div>
                    
                    <div class="flow-step" style="background: linear-gradient(135deg, #fff8e1 0%, #ffecb3 100%);">
                        <div class="step-number">4</div>
                        <div>
                            <h4>Chapter 4 - Advanced Topics</h4>
                            <p>Neural networks, deep learning, model interpretation</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="interactive-demo">
                <h3>üéØ Practice Recommendations</h3>
                <div class="grid">
                    <div style="background: rgba(255,255,255,0.1); padding: 20px; border-radius: 8px;">
                        <h4>üìö Immediate Practice</h4>
                        <ul>
                            <li>Complete all three hands-on projects</li>
                            <li>Experiment with different datasets from Kaggle</li>
                            <li>Try various preprocessing techniques</li>
                            <li>Compare multiple algorithms on same data</li>
                        </ul>
                    </div>
                    
                    <div style="background: rgba(255,255,255,0.1); padding: 20px; border-radius: 8px;">
                        <h4>üî¨ Extended Learning</h4>
                        <ul>
                            <li>Join ML competitions (Kaggle Learn)</li>
                            <li>Build a portfolio of ML projects</li>
                            <li>Read research papers in your domain</li>
                            <li>Contribute to open-source ML projects</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="highlight-box tip">
                <h4>üí° Final Words of Wisdom</h4>
                <p><strong>Remember:</strong> Machine learning is 80% data preparation and 20% algorithms. Focus on understanding your data, asking the right questions, and iterating based on results. The most sophisticated algorithm won't help if your data quality is poor or you're solving the wrong problem.</p>
                
                <p><strong>Keep Learning:</strong> ML is a rapidly evolving field. Stay curious, keep practicing, and don't be afraid to experiment. Every expert was once a beginner who never gave up!</p>
            </div>

            <div style="text-align: center; margin: 40px 0;">
                <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 15px;">
                    <h2>üéâ Congratulations!</h2>
                    <p style="font-size: 1.2em; margin: 20px 0;">You've completed Chapter 1 and built a solid foundation in Machine Learning!</p>
                    <p>You're now ready to tackle real-world ML problems with confidence.</p>
                    
                    <div style="display: flex; justify-content: space-between; margin-top: 30px;">
                        <!-- Left-aligned button -->
                        <button style="background: white; color: #667eea; border: none; padding: 15px 30px; border-radius: 8px; font-size: 1.1em; font-weight: bold; margin: 10px; cursor: pointer;">
                            <a href="#top" class="azbn-btn azbn-secondary" style="text-decoration: none; color: inherit;">  üîÑ Review This Chapter</a>
                        </button>

                        <!-- Right-aligned button -->
                        <button style="background: white; color: #667eea; border: none; padding: 15px 30px; border-radius: 8px; font-size: 1.1em; font-weight: bold; margin: 10px; cursor: pointer;">
                            <a href="./chapter2.html"class="azbn-btn azbn-secondary" style="text-decoration: none; color: inherit;">üìö Continue to Chapter 2: Regression</a>
                        </button>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        function showTab(tabName) {
            // Hide all tab contents
            const contents = document.querySelectorAll('.tab-content');
            contents.forEach(content => content.classList.remove('active'));
            
            // Remove active class from all tabs
            const tabs = document.querySelectorAll('.tab');
            tabs.forEach(tab => tab.classList.remove('active'));
            
            // Show selected tab content
            document.getElementById(tabName).classList.add('active');
            
            // Add active class to clicked tab
            event.target.classList.add('active');
        }

        // Add some interactive feedback
        document.addEventListener('DOMContentLoaded', function() {
            const cards = document.querySelectorAll('.card');
            cards.forEach(card => {
                card.addEventListener('mouseenter', function() {
                    this.style.transform = 'translateY(-5px)';
                    this.style.boxShadow = '0 10px 25px rgba(0,0,0,0.15)';
                });
                
                card.addEventListener('mouseleave', function() {
                    this.style.transform = 'translateY(0)';
                    this.style.boxShadow = '0 4px 6px rgba(0,0,0,0.1)';
                });
            });
            
            // Animate progress bar
            const progressFill = document.querySelector('.progress-fill');
            if (progressFill) {
                progressFill.style.width = '0%';
                setTimeout(() => {
                    progressFill.style.width = '100%';
                }, 500);
            }
        });
    </script>
</body>
</html>
                