<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Feature Engineering - ML Tutorial</title>
    
    <!-- SEO Meta Tags -->
    <meta name="description" content="Data preprocessing and feature selection techniques">
    <meta name="keywords" content="machine learning, artificial intelligence, data science, python, portfolio">
    <meta name="author" content="Alireza Barzin Zanganeh">
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost/tutorials/feature-engineering">
    <meta property="og:title" content="Feature Engineering - ML Tutorial">
    <meta property="og:description" content="Data preprocessing and feature selection techniques">
    
    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="http://localhost/tutorials/feature-engineering">
    <meta property="twitter:title" content="Feature Engineering - ML Tutorial">
    <meta property="twitter:description" content="Data preprocessing and feature selection techniques">
    
    <!-- CSS -->
    <link href="/static/css/main.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    
<style>
.tutorial-content {
    max-width: 1000px;
    margin: 0 auto;
}

.tutorial-header {
    background: var(--gradient-primary);
    color: white;
    padding: 3rem 0;
    text-align: center;
    margin-bottom: 2rem;
}

.tutorial-meta {
    display: flex;
    justify-content: center;
    gap: 1rem;
    margin-top: 1rem;
    flex-wrap: wrap;
}

.tutorial-body {
    background: white;
    padding: 2rem;
    border-radius: var(--border-radius-lg);
    box-shadow: var(--shadow-lg);
    margin-bottom: 2rem;
}

.tutorial-section {
    margin-bottom: 2rem;
}

.tutorial-section h2 {
    color: var(--primary-color);
    border-bottom: 2px solid var(--accent-color);
    padding-bottom: 0.5rem;
    margin-bottom: 1rem;
}

.tutorial-section h3 {
    color: var(--secondary-color);
    margin-top: 2rem;
    margin-bottom: 1rem;
}

.code-example {
    background: var(--bg-dark);
    color: #E2E8F0;
    padding: 1.5rem;
    border-radius: var(--border-radius-md);
    margin: 1rem 0;
    font-family: var(--font-family-mono);
    font-size: 0.9rem;
    overflow-x: auto;
    position: relative;
    white-space: pre-wrap;
    word-wrap: break-word;
}

.code-example::before {
    content: 'Python';
    position: absolute;
    top: 0.5rem;
    right: 1rem;
    background: var(--accent-color);
    color: white;
    padding: 0.25rem 0.5rem;
    border-radius: var(--border-radius-sm);
    font-size: 0.7rem;
    font-weight: 600;
}

.formula-box {
    background: linear-gradient(135deg, #f0f8ff 0%, #e6f3ff 100%);
    border-left: 4px solid var(--accent-color);
    padding: 1rem;
    margin: 1rem 0;
    border-radius: var(--border-radius-md);
    font-style: italic;
    text-align: center;
}

.example-box {
    background: var(--bg-secondary);
    border: 1px solid var(--border-color);
    border-radius: var(--border-radius-md);
    padding: 1.5rem;
    margin: 1rem 0;
}

.navigation-box {
    background: var(--bg-secondary);
    padding: 1.5rem;
    border-radius: var(--border-radius-lg);
    text-align: center;
    margin-top: 2rem;
}

.interactive-demo {
    background: white;
    border: 2px solid var(--accent-color);
    border-radius: var(--border-radius-lg);
    padding: 1.5rem;
    margin: 2rem 0;
}

ul.feature-list {
    list-style: none;
    padding: 0;
}

ul.feature-list li {
    padding: 0.5rem 0;
    border-bottom: 1px solid var(--border-color);
}

ul.feature-list li:before {
    content: '‚úì';
    color: var(--success-color);
    font-weight: bold;
    margin-right: 0.5rem;
}

@media (max-width: 768px) {
    .tutorial-body {
        padding: 1rem;
    }
    
    .tutorial-meta {
        flex-direction: column;
        align-items: center;
    }
}
</style>

</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-logo">
                <a href="/" class="logo-link">
                    <img src="/static/images/logo.png" alt="Logo" class="nav-logo-img">
                    <span class="nav-logo-text">Alireza Barzin Zanganeh</span>
                </a>
            </div>
            <div class="nav-menu" id="nav-menu">
                <a href="/" class="nav-link">Home</a>
                <a href="/#about" class="nav-link">About</a>
                <a href="/tutorials" class="nav-link">Tutorials</a>
                <a href="/projects" class="nav-link">Projects</a>
                <a href="/#contact" class="nav-link">Contact</a>
            </div>
            <div class="nav-toggle" id="mobile-menu">
                <span class="bar"></span>
                <span class="bar"></span>
                <span class="bar"></span>
            </div>
        </div>
    </nav>

    <!-- Main Content -->
    <main class="main-content">
        
<div class="tutorial-content">
    <!-- Tutorial Header -->
    <div class="tutorial-header">
        <div class="container">
            <h1>Feature Engineering</h1>
            <p>Data preprocessing and feature selection techniques</p>
            <div class="tutorial-meta">
                <span class="badge badge-warning">Intermediate</span>
                <span class="badge badge-secondary">22 min read</span>
                
                <span class="badge badge-primary">preprocessing</span>
                
                <span class="badge badge-primary">feature-selection</span>
                
                <span class="badge badge-primary">intermediate</span>
                
            </div>
        </div>
    </div>

    <!-- Tutorial Content -->
    <div class="container">
        <div class="tutorial-body">
            
<div class="tutorial-section">
    <h2>Introduction to Feature Engineering</h2>
    <p>
        Feature engineering is the process of selecting, modifying, or creating features from raw data 
        to improve machine learning model performance. It's often considered the most important aspect 
        of machine learning, as good features can make simple models outperform complex ones with poor features.
    </p>
    
    <div class="highlight-box">
        <h3>üéØ Why Feature Engineering Matters</h3>
        <ul>
            <li><strong>Model Performance:</strong> Good features dramatically improve accuracy</li>
            <li><strong>Interpretability:</strong> Meaningful features make models more explainable</li>
            <li><strong>Efficiency:</strong> Relevant features reduce computational complexity</li>
            <li><strong>Generalization:</strong> Well-engineered features improve model robustness</li>
            <li><strong>Domain Knowledge:</strong> Incorporates business understanding into models</li>
        </ul>
    </div>
</div>

<div class="tutorial-section">
    <h2>Types of Feature Engineering</h2>
    
    <div class="feature-types-grid">
        <div class="feature-type">
            <h3>üîÑ Feature Transformation</h3>
            <ul>
                <li>Scaling and normalization</li>
                <li>Logarithmic transformations</li>
                <li>Box-Cox transformations</li>
                <li>Polynomial features</li>
            </ul>
        </div>
        
        <div class="feature-type">
            <h3>üé≠ Feature Encoding</h3>
            <ul>
                <li>One-hot encoding</li>
                <li>Label encoding</li>
                <li>Target encoding</li>
                <li>Binary encoding</li>
            </ul>
        </div>
        
        <div class="feature-type">
            <h3>‚ö° Feature Creation</h3>
            <ul>
                <li>Interaction features</li>
                <li>Aggregation features</li>
                <li>Time-based features</li>
                <li>Domain-specific features</li>
            </ul>
        </div>
        
        <div class="feature-type">
            <h3>üéØ Feature Selection</h3>
            <ul>
                <li>Filter methods</li>
                <li>Wrapper methods</li>
                <li>Embedded methods</li>
                <li>Dimensionality reduction</li>
            </ul>
        </div>
    </div>
</div>

<div class="tutorial-section">
    <h2>Data Preprocessing Techniques</h2>
    
    <h3>Handling Missing Values</h3>
    <div class="code-example">import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# Load sample data
df = pd.DataFrame({
    'age': [25, np.nan, 35, 40, np.nan, 30],
    'income': [50000, 60000, np.nan, 80000, 70000, np.nan],
    'education': ['Bachelor', np.nan, 'Master', 'PhD', 'Bachelor', 'Master']
})

# 1. Simple Imputation
# Mean/Median/Mode imputation
numerical_imputer = SimpleImputer(strategy='median')
df['age_imputed'] = numerical_imputer.fit_transform(df[['age']])

categorical_imputer = SimpleImputer(strategy='most_frequent')
df['education_imputed'] = categorical_imputer.fit_transform(df[['education']])

# 2. KNN Imputation
knn_imputer = KNNImputer(n_neighbors=3)
df_numerical = df[['age', 'income']].copy()
df_knn_imputed = pd.DataFrame(
    knn_imputer.fit_transform(df_numerical),
    columns=['age_knn', 'income_knn']
)

# 3. Iterative Imputation (MICE)
iterative_imputer = IterativeImputer(random_state=42)
df_iterative = pd.DataFrame(
    iterative_imputer.fit_transform(df_numerical),
    columns=['age_iterative', 'income_iterative']
)

print("Original Data:")
print(df)
print("\nImputed Data Summary:")
print("KNN Imputation:", df_knn_imputed.head())
print("Iterative Imputation:", df_iterative.head())</div>

    <h3>Feature Scaling and Normalization</h3>
    <div class="code-example">from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.preprocessing import Normalizer, QuantileTransformer
import matplotlib.pyplot as plt

# Generate sample data
np.random.seed(42)
data = np.random.exponential(2, (1000, 1))
df_scale = pd.DataFrame({'original': data.flatten()})

# 1. Standardization (Z-score normalization)
scaler_standard = StandardScaler()
df_scale['standardized'] = scaler_standard.fit_transform(data)

# 2. Min-Max Scaling
scaler_minmax = MinMaxScaler()
df_scale['minmax'] = scaler_minmax.fit_transform(data)

# 3. Robust Scaling (median and IQR)
scaler_robust = RobustScaler()
df_scale['robust'] = scaler_robust.fit_transform(data)

# 4. Quantile Transformation
quantile_transformer = QuantileTransformer(output_distribution='normal')
df_scale['quantile'] = quantile_transformer.fit_transform(data)

# 5. Unit Vector Scaling
normalizer = Normalizer()
df_scale['normalized'] = normalizer.fit_transform(data.reshape(1, -1)).flatten()

# Visualize distributions
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
columns = ['original', 'standardized', 'minmax', 'robust', 'quantile', 'normalized']

for i, col in enumerate(columns):
    row, col_idx = i // 3, i % 3
    if i < len(columns):
        axes[row, col_idx].hist(df_scale[col], bins=50, alpha=0.7)
        axes[row, col_idx].set_title(f'{col.title()} Distribution')
        axes[row, col_idx].set_xlabel('Value')
        axes[row, col_idx].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

print("Scaling Statistics:")
print(df_scale.describe())</div>
</div>

<div class="tutorial-section">
    <h2>Categorical Feature Encoding</h2>
    
    <div class="code-example">from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.preprocessing import OrdinalEncoder, TargetEncoder
import category_encoders as ce

# Sample categorical data
df_cat = pd.DataFrame({
    'color': ['red', 'blue', 'green', 'red', 'blue', 'green', 'yellow'],
    'size': ['small', 'medium', 'large', 'medium', 'small', 'large', 'medium'],
    'grade': ['A', 'B', 'C', 'A', 'B', 'C', 'A'],
    'target': [1, 0, 1, 1, 0, 1, 0]
})

# 1. Label Encoding (ordinal)
label_encoder = LabelEncoder()
df_cat['color_label'] = label_encoder.fit_transform(df_cat['color'])

# 2. One-Hot Encoding
onehot_encoder = OneHotEncoder(sparse_output=False, drop='first')
color_onehot = onehot_encoder.fit_transform(df_cat[['color']])
color_columns = [f'color_{cat}' for cat in onehot_encoder.categories_[0][1:]]
df_onehot = pd.DataFrame(color_onehot, columns=color_columns)

# 3. Ordinal Encoding (with custom order)
size_order = ['small', 'medium', 'large']
ordinal_encoder = OrdinalEncoder(categories=[size_order])
df_cat['size_ordinal'] = ordinal_encoder.fit_transform(df_cat[['size']])

# 4. Target Encoding
target_encoder = TargetEncoder()
df_cat['color_target'] = target_encoder.fit_transform(df_cat[['color']], df_cat['target'])

# 5. Binary Encoding
binary_encoder = ce.BinaryEncoder(cols=['color'])
df_binary = binary_encoder.fit_transform(df_cat)

# 6. Frequency Encoding
frequency_map = df_cat['color'].value_counts().to_dict()
df_cat['color_frequency'] = df_cat['color'].map(frequency_map)

print("Original Data:")
print(df_cat[['color', 'size', 'grade', 'target']])
print("\nEncoded Features:")
print("Label Encoded:", df_cat['color_label'].tolist())
print("Ordinal Encoded:", df_cat['size_ordinal'].tolist())
print("Target Encoded:", df_cat['color_target'].tolist())
print("Frequency Encoded:", df_cat['color_frequency'].tolist())
print("\nOne-Hot Encoded:")
print(df_onehot)</div>
</div>

<div class="tutorial-section">
    <h2>Feature Creation Techniques</h2>
    
    <h3>Polynomial Features</h3>
    <div class="code-example">from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Generate sample data
np.random.seed(42)
X = np.random.uniform(-2, 2, (100, 2))
y = X[:, 0]**2 + X[:, 1]**2 + 0.1 * np.random.normal(0, 1, 100)

# Create polynomial features
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)

print("Original features shape:", X.shape)
print("Polynomial features shape:", X_poly.shape)
print("Feature names:", poly_features.get_feature_names_out(['x1', 'x2']))

# Compare linear vs polynomial regression
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
X_poly_train, X_poly_test = train_test_split(X_poly, test_size=0.3, random_state=42)

# Linear model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)
y_pred_linear = linear_model.predict(X_test)

# Polynomial model
poly_model = LinearRegression()
poly_model.fit(X_poly_train, y_train)
y_pred_poly = poly_model.predict(X_poly_test)

print(f"\nLinear Model R¬≤: {r2_score(y_test, y_pred_linear):.4f}")
print(f"Polynomial Model R¬≤: {r2_score(y_test, y_pred_poly):.4f}")</div>

    <h3>Time-Based Features</h3>
    <div class="code-example"># Time series feature engineering
import datetime

# Create sample time series data
dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')
sales = np.random.normal(100, 20, len(dates)) + \
        10 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25) + \
        5 * np.sin(2 * np.pi * np.arange(len(dates)) / 7)

df_time = pd.DataFrame({'date': dates, 'sales': sales})
df_time['date'] = pd.to_datetime(df_time['date'])

# Extract time-based features
df_time['year'] = df_time['date'].dt.year
df_time['month'] = df_time['date'].dt.month
df_time['day'] = df_time['date'].dt.day
df_time['day_of_week'] = df_time['date'].dt.dayofweek
df_time['day_of_year'] = df_time['date'].dt.dayofyear
df_time['week_of_year'] = df_time['date'].dt.isocalendar().week
df_time['quarter'] = df_time['date'].dt.quarter
df_time['is_weekend'] = (df_time['day_of_week'] >= 5).astype(int)
df_time['is_month_start'] = df_time['date'].dt.is_month_start.astype(int)
df_time['is_month_end'] = df_time['date'].dt.is_month_end.astype(int)

# Cyclical encoding for periodic features
df_time['month_sin'] = np.sin(2 * np.pi * df_time['month'] / 12)
df_time['month_cos'] = np.cos(2 * np.pi * df_time['month'] / 12)
df_time['day_of_week_sin'] = np.sin(2 * np.pi * df_time['day_of_week'] / 7)
df_time['day_of_week_cos'] = np.cos(2 * np.pi * df_time['day_of_week'] / 7)

# Lag features
df_time['sales_lag_1'] = df_time['sales'].shift(1)
df_time['sales_lag_7'] = df_time['sales'].shift(7)
df_time['sales_lag_30'] = df_time['sales'].shift(30)

# Rolling window features
df_time['sales_ma_7'] = df_time['sales'].rolling(window=7).mean()
df_time['sales_ma_30'] = df_time['sales'].rolling(window=30).mean()
df_time['sales_std_7'] = df_time['sales'].rolling(window=7).std()
df_time['sales_min_7'] = df_time['sales'].rolling(window=7).min()
df_time['sales_max_7'] = df_time['sales'].rolling(window=7).max()

print("Time-based features created:")
print(df_time.columns.tolist())
print("\nSample data:")
print(df_time.head(10))</div>

    <h3>Interaction Features</h3>
    <div class="code-example"># Create interaction features
from itertools import combinations

# Sample data
df_interact = pd.DataFrame({
    'age': [25, 30, 35, 40, 45],
    'income': [50000, 60000, 70000, 80000, 90000],
    'education_years': [16, 18, 20, 22, 24],
    'experience': [3, 8, 12, 18, 22]
})

# Mathematical interactions
df_interact['age_income'] = df_interact['age'] * df_interact['income']
df_interact['income_per_age'] = df_interact['income'] / df_interact['age']
df_interact['income_education_ratio'] = df_interact['income'] / df_interact['education_years']

# Domain-specific interactions
df_interact['income_per_experience'] = df_interact['income'] / (df_interact['experience'] + 1)
df_interact['education_premium'] = df_interact['education_years'] * df_interact['income']

# Automatic interaction generation
numerical_cols = ['age', 'income', 'education_years', 'experience']
for col1, col2 in combinations(numerical_cols, 2):
    # Multiplication
    df_interact[f'{col1}_{col2}_mult'] = df_interact[col1] * df_interact[col2]
    # Addition
    df_interact[f'{col1}_{col2}_add'] = df_interact[col1] + df_interact[col2]
    # Ratio (avoiding division by zero)
    df_interact[f'{col1}_{col2}_ratio'] = df_interact[col1] / (df_interact[col2] + 1e-8)

print("Interaction features created:")
print([col for col in df_interact.columns if '_' in col])</div>
</div>

<div class="tutorial-section">
    <h2>Feature Selection Methods</h2>
    
    <h3>Filter Methods</h3>
    <div class="code-example">from sklearn.feature_selection import SelectKBest, f_classif, chi2, mutual_info_classif
from sklearn.feature_selection import VarianceThreshold
from sklearn.datasets import make_classification
import scipy.stats as stats

# Generate sample classification data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, n_clusters_per_class=1, random_state=42)
feature_names = [f'feature_{i}' for i in range(X.shape[1])]
df_select = pd.DataFrame(X, columns=feature_names)
df_select['target'] = y

# 1. Variance Threshold
variance_selector = VarianceThreshold(threshold=0.1)
X_variance = variance_selector.fit_transform(X)
selected_features_variance = [feature_names[i] for i in variance_selector.get_support(indices=True)]

# 2. Univariate Statistical Tests
# For classification: f_classif, chi2, mutual_info_classif
f_selector = SelectKBest(score_func=f_classif, k=10)
X_f_test = f_selector.fit_transform(X, y)
f_scores = f_selector.scores_
selected_features_f = [feature_names[i] for i in f_selector.get_support(indices=True)]

# 3. Mutual Information
mi_selector = SelectKBest(score_func=mutual_info_classif, k=10)
X_mi = mi_selector.fit_transform(X, y)
mi_scores = mi_selector.scores_
selected_features_mi = [feature_names[i] for i in mi_selector.get_support(indices=True)]

# 4. Correlation Analysis
correlation_matrix = df_select.corr()
target_correlation = correlation_matrix['target'].abs().sort_values(ascending=False)

print("Feature Selection Results:")
print(f"Variance Threshold: {len(selected_features_variance)} features")
print(f"F-test: {len(selected_features_f)} features") 
print(f"Mutual Information: {len(selected_features_mi)} features")
print(f"\nTop 5 features by correlation with target:")
print(target_correlation.head(6)[1:])  # Exclude target itself</div>

    <h3>Wrapper Methods</h3>
    <div class="code-example">from sklearn.feature_selection import RFE, RFECV
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold

# Recursive Feature Elimination
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# RFE with fixed number of features
rfe = RFE(estimator=rf_model, n_features_to_select=10)
X_rfe = rfe.fit_transform(X, y)
selected_features_rfe = [feature_names[i] for i in rfe.get_support(indices=True)]

# RFE with Cross-Validation
rfecv = RFECV(estimator=rf_model, step=1, cv=StratifiedKFold(n_splits=5), 
              scoring='accuracy', n_jobs=-1)
X_rfecv = rfecv.fit_transform(X, y)
selected_features_rfecv = [feature_names[i] for i in rfecv.get_support(indices=True)]

print(f"RFE selected features: {len(selected_features_rfe)}")
print(f"RFE-CV selected features: {len(selected_features_rfecv)}")
print(f"Optimal number of features: {rfecv.n_features_}")

# Plot RFE-CV results
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), 
         rfecv.cv_results_['mean_test_score'], marker='o')
plt.xlabel('Number of Features')
plt.ylabel('Cross-Validation Score')
plt.title('RFE-CV Feature Selection')
plt.grid(True)
plt.show()</div>

    <h3>Embedded Methods</h3>
    <div class="code-example">from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LassoCV
from sklearn.feature_selection import SelectFromModel

# 1. Tree-based Feature Importance
rf_importance = RandomForestClassifier(n_estimators=100, random_state=42)
rf_importance.fit(X, y)

# Get feature importance
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'importance': rf_importance.feature_importances_
}).sort_values('importance', ascending=False)

# Select features based on importance threshold
importance_selector = SelectFromModel(rf_importance, threshold='median')
X_importance = importance_selector.fit_transform(X, y)
selected_features_importance = [feature_names[i] for i in importance_selector.get_support(indices=True)]

# 2. L1 Regularization (Lasso)
lasso = LassoCV(cv=5, random_state=42)
lasso.fit(X, y)

# Select non-zero coefficients
lasso_selector = SelectFromModel(lasso, prefit=True)
X_lasso = lasso_selector.transform(X)
selected_features_lasso = [feature_names[i] for i in lasso_selector.get_support(indices=True)]

print("Embedded Method Results:")
print(f"Random Forest Importance: {len(selected_features_importance)} features")
print(f"Lasso Regularization: {len(selected_features_lasso)} features")

print(f"\nTop 10 Most Important Features:")
print(feature_importance.head(10))

# Visualize feature importance
plt.figure(figsize=(12, 8))
plt.barh(range(len(feature_importance)), feature_importance['importance'])
plt.yticks(range(len(feature_importance)), feature_importance['feature'])
plt.xlabel('Feature Importance')
plt.title('Random Forest Feature Importance')
plt.tight_layout()
plt.show()</div>
</div>

<div class="tutorial-section">
    <h2>Advanced Feature Engineering</h2>
    
    <h3>Dimensionality Reduction</h3>
    <div class="code-example">from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# Principal Component Analysis
pca = PCA(n_components=0.95)  # Keep 95% of variance
X_pca = pca.fit_transform(X)

print(f"Original features: {X.shape[1]}")
print(f"PCA features: {X_pca.shape[1]}")
print(f"Explained variance ratio: {pca.explained_variance_ratio_[:5]}")

# Linear Discriminant Analysis (supervised)
lda = LDA(n_components=1)  # n_classes - 1 for binary classification
X_lda = lda.fit_transform(X, y)

# t-SNE for visualization
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

# Visualize dimensionality reduction
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# PCA
axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
axes[0].set_title('PCA')
axes[0].set_xlabel('First Principal Component')
axes[0].set_ylabel('Second Principal Component')

# LDA
axes[1].hist([X_lda[y==0, 0], X_lda[y==1, 0]], bins=30, alpha=0.7, label=['Class 0', 'Class 1'])
axes[1].set_title('LDA')
axes[1].set_xlabel('Linear Discriminant')
axes[1].legend()

# t-SNE
axes[2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')
axes[2].set_title('t-SNE')
axes[2].set_xlabel('t-SNE 1')
axes[2].set_ylabel('t-SNE 2')

plt.tight_layout()
plt.show()</div>

    <h3>Automated Feature Engineering</h3>
    <div class="code-example"># Automated feature engineering with Featuretools
# Note: Install with: pip install featuretools

try:
    import featuretools as ft
    
    # Create sample relational data
    customers = pd.DataFrame({
        'customer_id': [1, 2, 3, 4, 5],
        'age': [25, 30, 35, 40, 45],
        'join_date': pd.to_datetime(['2020-01-01', '2020-02-15', '2020-03-10', '2020-04-05', '2020-05-20'])
    })
    
    transactions = pd.DataFrame({
        'transaction_id': range(1, 21),
        'customer_id': np.random.choice([1, 2, 3, 4, 5], 20),
        'amount': np.random.normal(50, 15, 20),
        'timestamp': pd.date_range('2020-06-01', periods=20, freq='D')
    })
    
    # Create entity set
    es = ft.EntitySet(id='customer_data')
    
    # Add entities
    es = es.add_dataframe(dataframe_name='customers', dataframe=customers, index='customer_id')
    es = es.add_dataframe(dataframe_name='transactions', dataframe=transactions, 
                         index='transaction_id', time_index='timestamp')
    
    # Add relationship
    es = es.add_relationship('customers', 'customer_id', 'transactions', 'customer_id')
    
    # Generate features automatically
    feature_matrix, feature_defs = ft.dfs(entityset=es, target_dataframe_name='customers',
                                         max_depth=2, verbose=True)
    
    print("Automated Feature Engineering Results:")
    print(f"Generated {len(feature_defs)} features")
    print(f"Feature matrix shape: {feature_matrix.shape}")
    print("\nSample features:")
    print(feature_matrix.head())
    
except ImportError:
    print("Featuretools not installed. Install with: pip install featuretools")
    
    # Manual aggregation features example
    customer_features = transactions.groupby('customer_id').agg({
        'amount': ['count', 'sum', 'mean', 'std', 'min', 'max'],
        'timestamp': ['min', 'max']
    }).round(2)
    
    customer_features.columns = ['_'.join(col).strip() for col in customer_features.columns]
    print("\nManual Aggregation Features:")
    print(customer_features)</div>
</div>

<div class="tutorial-section">
    <h2>Feature Engineering Pipeline</h2>
    
    <div class="code-example">from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import cross_val_score

# Create comprehensive preprocessing pipeline
def create_feature_pipeline():
    # Define column types
    numeric_features = ['age', 'income', 'experience']
    categorical_features = ['education', 'location']
    
    # Numeric pipeline
    numeric_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    # Categorical pipeline
    categorical_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))
    ])
    
    # Combine pipelines
    preprocessor = ColumnTransformer([
        ('num', numeric_pipeline, numeric_features),
        ('cat', categorical_pipeline, categorical_features)
    ])
    
    return preprocessor

# Example usage
sample_data = pd.DataFrame({
    'age': [25, np.nan, 35, 40, 30],
    'income': [50000, 60000, np.nan, 80000, 70000],
    'experience': [3, 8, 12, np.nan, 10],
    'education': ['Bachelor', 'Master', 'PhD', 'Bachelor', np.nan],
    'location': ['NYC', 'SF', 'LA', 'NYC', 'SF'],
    'target': [1, 0, 1, 1, 0]
})

# Create and fit pipeline
preprocessor = create_feature_pipeline()
X = sample_data.drop('target', axis=1)
y = sample_data['target']

X_processed = preprocessor.fit_transform(X)
print(f"Original shape: {X.shape}")
print(f"Processed shape: {X_processed.shape}")

# Feature names after preprocessing
feature_names = (
    ['age', 'income', 'experience'] +  # numeric features
    list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(['education', 'location']))
)
print(f"\nProcessed features: {feature_names}")</div>
</div>

<div class="tutorial-section">
    <h2>Best Practices</h2>
    
    <div class="best-practices-grid">
        <div class="practice">
            <h4>üéØ Domain Knowledge</h4>
            <ul>
                <li>Understand the business problem deeply</li>
                <li>Consult domain experts for feature ideas</li>
                <li>Create meaningful, interpretable features</li>
                <li>Consider real-world constraints and data availability</li>
            </ul>
        </div>
        
        <div class="practice">
            <h4>üîç Data Understanding</h4>
            <ul>
                <li>Perform thorough exploratory data analysis</li>
                <li>Understand data distributions and patterns</li>
                <li>Identify missing values and outliers</li>
                <li>Check for data leakage</li>
            </ul>
        </div>
        
        <div class="practice">
            <h4>‚ö° Iterative Process</h4>
            <ul>
                <li>Start simple, then add complexity</li>
                <li>Validate features with cross-validation</li>
                <li>Monitor feature importance and stability</li>
                <li>Remove redundant or harmful features</li>
            </ul>
        </div>
        
        <div class="practice">
            <h4>üõ°Ô∏è Avoid Pitfalls</h4>
            <ul>
                <li>Prevent data leakage from future information</li>
                <li>Handle time-based splits properly</li>
                <li>Be careful with target encoding</li>
                <li>Validate on out-of-sample data</li>
            </ul>
        </div>
    </div>
</div>

<div class="tutorial-section">
    <h2>Real-World Applications</h2>
    
    <div class="applications-grid">
        <div class="application">
            <h4>üí∞ Financial Services</h4>
            <p>Credit scoring with payment history aggregations, debt-to-income ratios, and spending pattern features</p>
        </div>
        
        <div class="application">
            <h4>üõí E-commerce</h4>
            <p>Customer lifetime value with purchase frequency, recency, monetary features, and behavioral patterns</p>
        </div>
        
        <div class="application">
            <h4>üè• Healthcare</h4>
            <p>Risk prediction with vital sign trends, medication interactions, and demographic risk factors</p>
        </div>
        
        <div class="application">
            <h4>üöó Transportation</h4>
            <p>Demand forecasting with weather data, event calendars, traffic patterns, and seasonal features</p>
        </div>
        
        <div class="application">
            <h4>üì± Tech Platforms</h4>
            <p>User engagement with session features, content interactions, and social network metrics</p>
        </div>
        
        <div class="application">
            <h4>üè≠ Manufacturing</h4>
            <p>Predictive maintenance with sensor aggregations, equipment age, and operational features</p>
        </div>
    </div>
</div>

<div class="tutorial-section">
    <h2>üöÄ Next Steps</h2>
    <div class="example-box">
        <h3>Master Feature Engineering</h3>
        <ul>
            <li><strong>Practice:</strong> Work with real datasets from different domains</li>
            <li><strong>Automation:</strong> Learn tools like Featuretools and AutoML platforms</li>
            <li><strong>Deep Learning:</strong> Explore feature learning and representation learning</li>
            <li><strong>MLOps:</strong> Build robust feature engineering pipelines for production</li>
        </ul>
        
        <div style="margin-top: 1.5rem;">
            <a href="/tutorials" class="btn btn-primary" style="margin-right: 1rem;">All Tutorials</a>
            <a href="/projects" class="btn btn-secondary">View Projects</a>
        </div>
    </div>
</div>

        </div>

        <!-- Navigation -->
        <div class="navigation-box">
            <a href="/tutorials" class="btn btn-secondary" style="margin-right: 1rem;">
                <i class="fas fa-arrow-left"></i> All Tutorials
            </a>
            <a href="/" class="btn btn-primary">
                <i class="fas fa-home"></i> Home
            </a>
        </div>
    </div>
</div>

    </main>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h3>Alireza Barzin Zanganeh</h3>
                    <p>Machine Learning Engineer</p>
                </div>
                <div class="footer-section">
                    <h4>Quick Links</h4>
                    <ul>
                        <li><a href="/tutorials">Tutorials</a></li>
                        <li><a href="/projects">Projects</a></li>
                        <li><a href="/#about">About</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h4>Connect</h4>
                    <div class="social-links">
                        <a href="#" class="social-link"><i class="fab fa-github"></i></a>
                        <a href="#" class="social-link"><i class="fab fa-linkedin"></i></a>
                        <a href="#" class="social-link"><i class="fab fa-twitter"></i></a>
                    </div>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 Alireza Barzin Zanganeh. Built with Flask and Python.</p>
            </div>
        </div>
    </footer>

    <!-- JavaScript -->
    <script src="/static/js/main.js"></script>
    
<script>
// Add smooth scrolling for table of contents
document.querySelectorAll('a[href^="#"]').forEach(anchor => {
    anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const target = document.querySelector(this.getAttribute('href'));
        if (target) {
            target.scrollIntoView({
                behavior: 'smooth',
                block: 'start'
            });
        }
    });
});

// Highlight code blocks
document.querySelectorAll('.code-example').forEach(block => {
    // Add copy button
    const copyBtn = document.createElement('button');
    copyBtn.textContent = 'Copy';
    copyBtn.className = 'btn btn-secondary';
    copyBtn.style.position = 'absolute';
    copyBtn.style.top = '0.5rem';
    copyBtn.style.left = '1rem';
    copyBtn.style.fontSize = '0.7rem';
    copyBtn.style.padding = '0.25rem 0.5rem';
    
    copyBtn.addEventListener('click', () => {
        navigator.clipboard.writeText(block.textContent);
        copyBtn.textContent = 'Copied!';
        setTimeout(() => copyBtn.textContent = 'Copy', 2000);
    });
    
    block.style.position = 'relative';
    block.appendChild(copyBtn);
});
</script>

</body>
</html>